# NewsBank æŠ“å–å™¨ - å¿«é€Ÿå¼€å§‹æŒ‡å—

## ğŸ“‹ ç›®å½•

1. [å®‰è£…](#å®‰è£…)
2. [é…ç½®](#é…ç½®)
3. [åŸºç¡€ä½¿ç”¨](#åŸºç¡€ä½¿ç”¨)
4. [é«˜çº§ç‰¹æ€§](#é«˜çº§ç‰¹æ€§)
5. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

---

## å®‰è£…

### 1. å®‰è£…ä¾èµ–

```bash
# å®‰è£… Python åŒ…
pip install -r requirements.txt

# å®‰è£… Playwright æµè§ˆå™¨é©±åŠ¨
playwright install chromium
```

### 2. éªŒè¯å®‰è£…

```bash
python -c "import playwright; print(f'Playwright {playwright.__version__} installed')"
python -c "import aiosqlite; print('aiosqlite installed')"
python -c "from pydantic_settings import BaseSettings; print('pydantic-settings installed')"
```

---

## é…ç½®

### 1. åˆ›å»º .env æ–‡ä»¶

```bash
# å¤åˆ¶ç¤ºä¾‹é…ç½®
cp .env.example .env

# ç¼–è¾‘ .env æ–‡ä»¶æ ¹æ®éœ€è¦ä¿®æ”¹å‚æ•°
```

### 2. é…ç½®å‚æ•°è¯´æ˜

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `SCRAPER_HEADLESS` | æ— å¤´æ¨¡å¼ï¼ˆä¸æ˜¾ç¤ºæµè§ˆå™¨çª—å£ï¼‰ | `true` |
| `SCRAPER_MAX_CONCURRENT_PAGES` | æœ€å¤§å¹¶å‘é¡µé¢æ•° | `5` |
| `SCRAPER_PAGE_TIMEOUT` | é¡µé¢åŠ è½½è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ | `30000` |
| `SCRAPER_DATABASE_PATH` | SQLite æ•°æ®åº“æ–‡ä»¶è·¯å¾„ | `newsbank.db` |
| `SCRAPER_MAX_RETRIES` | æœ€å¤§é‡è¯•æ¬¡æ•° | `3` |
| `SCRAPER_LOG_LEVEL` | æ—¥å¿—çº§åˆ« | `INFO` |

### 3. ç¯å¢ƒå˜é‡ä¼˜å…ˆçº§

é…ç½®åŠ è½½é¡ºåºï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰ï¼š
1. ç¯å¢ƒå˜é‡ï¼ˆ`SCRAPER_*` å‰ç¼€ï¼‰
2. `.env` æ–‡ä»¶
3. ä»£ç ä¸­çš„é»˜è®¤å€¼

---

## åŸºç¡€ä½¿ç”¨

### 1. ç®€å•çš„æŠ“å–ç¤ºä¾‹

```python
import asyncio
from advanced_scraper_example import NewsBankScraper, ScraperSettings

async def main():
    # åŠ è½½é…ç½®
    settings = ScraperSettings()
    
    # åˆ›å»ºæŠ“å–å™¨
    scraper = NewsBankScraper(settings)
    await scraper.initialize()
    
    # æŠ“å– URL
    urls = [
        "https://example.com/news/1",
        "https://example.com/news/2",
    ]
    
    result = await scraper.scrape_and_store(urls)
    print(f"Result: {result}")

if __name__ == "__main__":
    asyncio.run(main())
```

### 2. ä»æ–‡ä»¶è¯»å– URL

```python
import asyncio
from advanced_scraper_example import NewsBankScraper, ScraperSettings

async def main():
    settings = ScraperSettings()
    scraper = NewsBankScraper(settings)
    await scraper.initialize()
    
    # ä»æ–‡ä»¶è¯»å– URL
    with open("urls.txt", "r") as f:
        urls = [line.strip() for line in f if line.strip()]
    
    result = await scraper.scrape_and_store(urls)
    print(f"Scraped {result['successful']} articles")

if __name__ == "__main__":
    asyncio.run(main())
```

---

## é«˜çº§ç‰¹æ€§

### 1. è‡ªå®šä¹‰é‡è¯•ç­–ç•¥

```python
from tenacity import retry, stop_after_attempt, wait_exponential
from playwright.async_api import async_playwright

@retry(
    stop=stop_after_attempt(5),
    wait=wait_exponential(multiplier=2, max=30)
)
async def scrape_with_custom_retry(url: str):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        
        try:
            await page.goto(url, wait_until="networkidle", timeout=30000)
            return await page.content()
        finally:
            await page.close()
            await browser.close()
```

### 2. æ•°æ®åº“æŸ¥è¯¢

```python
import asyncio
from advanced_scraper_example import ConcurrentSQLiteManager

async def main():
    db = ConcurrentSQLiteManager("newsbank.db")
    
    # è·å–å•ä¸ªæ–‡ç« 
    article = await db.get_article(1)
    print(f"Article: {article}")
    
    # è·å–æ‰€æœ‰æ–‡ç« 
    async with db.get_connection() as conn:
        conn.row_factory = __import__('aiosqlite').Row
        cursor = await conn.execute("SELECT * FROM articles LIMIT 10")
        rows = await cursor.fetchall()
        for row in rows:
            print(dict(row))

if __name__ == "__main__":
    asyncio.run(main())
```

### 3. å¹¶å‘æ§åˆ¶

```python
import asyncio
from advanced_scraper_example import PlaywrightConcurrentScraper

async def main():
    # åˆ›å»ºæŠ“å–å™¨ï¼Œæœ€å¤š 10 ä¸ªå¹¶å‘é¡µé¢
    scraper = PlaywrightConcurrentScraper(
        max_pages=10,
        headless=True,
        timeout=30000
    )
    
    urls = [f"https://example.com/page/{i}" for i in range(100)]
    results = await scraper.scrape_multiple_pages(urls)
    
    successful = sum(1 for r in results if r['status'] == 'success')
    print(f"Scraped {successful}/{len(urls)} pages")

if __name__ == "__main__":
    asyncio.run(main())
```

### 4. é”™è¯¯å¤„ç†å’Œæ—¥å¿—

```python
import logging
import asyncio
from advanced_scraper_example import NewsBankScraper, ScraperSettings

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper.log'),
        logging.StreamHandler()
    ]
)

async def main():
    try:
        settings = ScraperSettings()
        scraper = NewsBankScraper(settings)
        await scraper.initialize()
        
        urls = ["https://example.com/news/1"]
        result = await scraper.scrape_and_store(urls)
        
    except Exception as e:
        logging.error(f"Scraping failed: {e}", exc_info=True)

if __name__ == "__main__":
    asyncio.run(main())
```

---

## æ•…éšœæ’é™¤

### é—®é¢˜ 1: Playwright æµè§ˆå™¨é©±åŠ¨æœªå®‰è£…

**ç—‡çŠ¶**: `PlaywrightError: Executable doesn't exist`

**è§£å†³æ–¹æ¡ˆ**:
```bash
playwright install chromium
# æˆ–å®‰è£…æ‰€æœ‰æµè§ˆå™¨
playwright install
```

### é—®é¢˜ 2: æ•°æ®åº“è¢«é”å®š

**ç—‡çŠ¶**: `sqlite3.OperationalError: database is locked`

**è§£å†³æ–¹æ¡ˆ**:
- ç¡®ä¿å¯ç”¨äº† WAL æ¨¡å¼ï¼ˆä»£ç å·²åŒ…å«ï¼‰
- å‡å°‘å¹¶å‘è¿æ¥æ•°
- æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–è¿›ç¨‹è®¿é—®æ•°æ®åº“

```python
# åœ¨ ScraperSettings ä¸­è°ƒæ•´
SCRAPER_MAX_DB_CONNECTIONS=3  # å‡å°‘å¹¶å‘è¿æ¥
```

### é—®é¢˜ 3: é¡µé¢åŠ è½½è¶…æ—¶

**ç—‡çŠ¶**: `TimeoutError: Timeout 30000ms exceeded`

**è§£å†³æ–¹æ¡ˆ**:
```python
# å¢åŠ è¶…æ—¶æ—¶é—´
settings = ScraperSettings()
settings.page_timeout = 60000  # 60 ç§’

# æˆ–åœ¨ .env ä¸­è®¾ç½®
SCRAPER_PAGE_TIMEOUT=60000
```

### é—®é¢˜ 4: å†…å­˜å ç”¨è¿‡é«˜

**ç—‡çŠ¶**: æŠ“å–è¿‡ç¨‹ä¸­å†…å­˜æŒç»­å¢é•¿

**è§£å†³æ–¹æ¡ˆ**:
```python
# å‡å°‘å¹¶å‘é¡µé¢æ•°
SCRAPER_MAX_CONCURRENT_PAGES=3

# å®šæœŸæ¸…ç†æµè§ˆå™¨ç¼“å­˜
async def scrape_with_cleanup(urls):
    for batch in chunks(urls, 10):  # æ¯ 10 ä¸ª URL é‡å¯æµè§ˆå™¨
        results = await scraper.scrape_multiple_pages(batch)
        # å¤„ç†ç»“æœ
```

### é—®é¢˜ 5: ç½‘ç«™åçˆ¬è™«

**ç—‡çŠ¶**: 403 Forbidden æˆ–è¢«é‡å®šå‘

**è§£å†³æ–¹æ¡ˆ**:
```python
# ä½¿ç”¨ä»£ç†
SCRAPER_USE_PROXY=true
SCRAPER_PROXY_URL=http://proxy.example.com:8080

# æ·»åŠ å»¶è¿Ÿ
import asyncio
for url in urls:
    await scraper.scrape_and_store([url])
    await asyncio.sleep(2)  # æ¯ä¸ªè¯·æ±‚é—´éš” 2 ç§’
```

---

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. æ•°æ®åº“ä¼˜åŒ–

```python
# å¯ç”¨ WAL æ¨¡å¼ï¼ˆå·²åœ¨ä»£ç ä¸­å¯ç”¨ï¼‰
PRAGMA journal_mode=WAL;

# è°ƒæ•´ç¼“å­˜å¤§å°
PRAGMA cache_size=10000;

# ä½¿ç”¨æ‰¹é‡æ’å…¥è€Œä¸æ˜¯é€æ¡æ’å…¥
await db.batch_insert_articles(articles)
```

### 2. Playwright ä¼˜åŒ–

```python
# ç¦ç”¨å›¾ç‰‡åŠ è½½åŠ å¿«é€Ÿåº¦
page = await browser.new_page()
await page.route("**/*.{png,jpg,jpeg,gif,svg}", lambda route: route.abort())

# ä½¿ç”¨ CDP æ¨¡å¼ï¼ˆæ›´å¿«ï¼‰
browser = await p.chromium.connect_over_cdp(remote_debugging_url)
```

### 3. å¹¶å‘ä¼˜åŒ–

```python
# æ ¹æ® CPU æ ¸å¿ƒæ•°è°ƒæ•´å¹¶å‘æ•°
import multiprocessing
optimal_pages = multiprocessing.cpu_count() * 2
```

---

## ç›‘æ§å’Œæ—¥å¿—

### æŸ¥çœ‹æ—¥å¿—

```bash
# å®æ—¶æŸ¥çœ‹æ—¥å¿—
tail -f scraper.log

# æŸ¥çœ‹é”™è¯¯æ—¥å¿—
grep ERROR scraper.log

# ç»Ÿè®¡æŠ“å–ç»“æœ
grep "Batch inserted" scraper.log
```

### æ•°æ®åº“ç»Ÿè®¡

```python
import asyncio
from advanced_scraper_example import ConcurrentSQLiteManager

async def stats():
    db = ConcurrentSQLiteManager("newsbank.db")
    
    async with db.get_connection() as conn:
        # æ€»æ–‡ç« æ•°
        cursor = await conn.execute("SELECT COUNT(*) FROM articles")
        total = (await cursor.fetchone())[0]
        
        # æŒ‰æ¥æºç»Ÿè®¡
        cursor = await conn.execute(
            "SELECT source, COUNT(*) FROM articles GROUP BY source"
        )
        sources = await cursor.fetchall()
        
        print(f"Total articles: {total}")
        for source, count in sources:
            print(f"  {source}: {count}")

asyncio.run(stats())
```

---

## ä¸‹ä¸€æ­¥

- æŸ¥çœ‹ `PLAYWRIGHT_PATTERNS.md` äº†è§£æ›´å¤šé«˜çº§æ¨¡å¼
- æŸ¥çœ‹ `advanced_scraper_example.py` çš„å®Œæ•´æºä»£ç 
- æ ¹æ®éœ€è¦è‡ªå®šä¹‰æŠ“å–é€»è¾‘
