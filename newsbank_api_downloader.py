# -*- coding: utf-8 -*-
"""
NewsBank API ä¸‹è½½å™¨
é€šè¿‡ç›´æ¥è°ƒç”¨NewsBankçš„APIè·å–æ–‡ç« å†…å®¹

åŠŸèƒ½ï¼š
1. ç”¨æˆ·è¾“å…¥NewsBankæœç´¢ç»“æœé¡µURL
2. è„šæœ¬è®¿é—®é¡µé¢å¹¶è·å–å¿…è¦çš„å‚æ•°
3. ç›´æ¥è°ƒç”¨APIè·å–å¤šç¯‡æ–‡ç« å®Œæ•´å†…å®¹
4. ä»å“åº”ä¸­è§£ææ–‡ç« åˆ—è¡¨å’Œå†…å®¹
5. ä¿å­˜æ–‡ç« ä¸ºæ–‡æœ¬æ–‡ä»¶

ä½¿ç”¨æ–¹æ³•ï¼š
    python newsbank_api_downloader.py "https://infoweb-newsbank-com.ezproxy.sl.nsw.gov.au/apps/news/results?..."
    
    python newsbank_api_downloader.py "URL" --max-pages 3
    
    python newsbank_api_downloader.py "URL" --download-all

ä½œè€…: AI Assistant
æ—¥æœŸ: 2026-02-15
"""

import asyncio
import argparse
import json
import re
import time
import os
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Dict, Any, Tuple
from urllib.parse import urlparse, parse_qs, urljoin, quote, unquote
from dataclasses import dataclass, asdict

from playwright.async_api import async_playwright, Page, BrowserContext

# å°è¯•å¯¼å…¥ openaiï¼ˆç”¨äºLLMè°ƒç”¨ï¼‰
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("[è­¦å‘Š] openai åŒ…æœªå®‰è£…ï¼ŒLLMç­›é€‰åŠŸèƒ½ä¸å¯ç”¨ã€‚è¯·è¿è¡Œ: pip install openai")

# å°è¯•åŠ è½½ .env æ–‡ä»¶
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass


@dataclass
class ArticleInfo:
    """æ–‡ç« ä¿¡æ¯æ•°æ®ç±»"""
    title: str
    date: str
    source: str
    author: str
    preview: str
    url: str
    page_num: int
    article_id: Optional[str] = None
    word_count: int = 0
    full_text: str = ""  # å®Œæ•´æ–‡ç« å†…å®¹
    
    def to_dict(self) -> dict:
        return asdict(self)


@dataclass
class RequestRecord:
    """å•æ¬¡è¯·æ±‚è®°å½•"""
    timestamp: float  # Unixæ—¶é—´æˆ³
    url: str
    method: str
    status_code: int
    response_time: float  # å“åº”æ—¶é—´ï¼ˆç§’ï¼‰
    success: bool
    error: Optional[str] = None
    request_type: str = "api"  # api, page, download


class TrafficLogger:
    """
    NewsBank æµé‡è®°å½•å™¨
    
    åŠŸèƒ½ï¼š
    1. è®°å½•æ¯æ¬¡API/é¡µé¢è¯·æ±‚
    2. ç»Ÿè®¡è¯·æ±‚é¢‘ç‡
    3. æ£€æµ‹é™æµé£é™©
    4. è¾“å‡ºæµé‡æŠ¥å‘Š
    """
    
    # é™æµé˜ˆå€¼é…ç½®
    MAX_REQUESTS_PER_MINUTE = 30  # æ¯åˆ†é’Ÿæœ€å¤§è¯·æ±‚æ•°
    MAX_REQUESTS_PER_SECOND = 2   # æ¯ç§’æœ€å¤§è¯·æ±‚æ•°
    RATE_WARNING_THRESHOLD = 0.8  # è¾¾åˆ°80%é˜ˆå€¼æ—¶è­¦å‘Š
    
    def __init__(self, output_dir: str = "logs"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.records: List[RequestRecord] = []
        self.session_start = time.time()
        self.last_request_time = 0
        self.min_request_interval = 0.5  # æœ€å°è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
        
        # é™æµè­¦å‘ŠçŠ¶æ€
        self.rate_limit_warned = False
        self.blocked_warned = False
    
    def record_request(self,
                       url: str,
                       method: str = "GET",
                       status_code: int = 200,
                       response_time: float = 0,
                       success: bool = True,
                       error: Optional[str] = None,
                       request_type: str = "api"):
        """è®°å½•ä¸€æ¬¡è¯·æ±‚"""
        record = RequestRecord(
            timestamp=time.time(),
            url=url,
            method=method,
            status_code=status_code,
            response_time=response_time,
            success=success,
            error=error,
            request_type=request_type
        )
        self.records.append(record)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è­¦å‘Š
        self._check_rate_limit()
        
        # æ›´æ–°æœ€åè¯·æ±‚æ—¶é—´
        self.last_request_time = record.timestamp
        
        return record
    
    def _check_rate_limit(self):
        """æ£€æŸ¥æ˜¯å¦æ¥è¿‘é™æµé˜ˆå€¼"""
        now = time.time()
        
        # æ£€æŸ¥æœ€è¿‘1åˆ†é’Ÿçš„è¯·æ±‚æ•°
        recent_records = [r for r in self.records if now - r.timestamp < 60]
        requests_per_minute = len(recent_records)
        
        # æ£€æŸ¥æœ€è¿‘1ç§’çš„è¯·æ±‚æ•°
        very_recent = [r for r in self.records if now - r.timestamp < 1]
        requests_per_second = len(very_recent)
        
        # æ¯åˆ†é’Ÿé™æµè­¦å‘Š
        if requests_per_minute >= self.MAX_REQUESTS_PER_MINUTE * self.RATE_WARNING_THRESHOLD:
            if not self.rate_limit_warned:
                print(f"\n[âš ï¸  æµé‡è­¦å‘Š] æœ€è¿‘1åˆ†é’Ÿè¯·æ±‚æ•°: {requests_per_minute}")
                print(f"  [å»ºè®®] å»ºè®®é™ä½è¯·æ±‚é¢‘ç‡ï¼Œå¢åŠ å»¶è¿Ÿ")
                self.rate_limit_warned = True
        
        # æ¯ç§’é™æµè­¦å‘Š
        if requests_per_second >= self.MAX_REQUESTS_PER_SECOND * self.RATE_WARNING_THRESHOLD:
            print(f"\n[âš ï¸  æµé‡è­¦å‘Š] æœ€è¿‘1ç§’è¯·æ±‚æ•°: {requests_per_second}")
            print(f"  [å»ºè®®] è¯·æ±‚è¿‡å¿«ï¼Œè¯·å¢åŠ å»¶è¿Ÿ")
        
        # æ£€æŸ¥æ˜¯å¦è¢«é˜»æ­¢ï¼ˆ429çŠ¶æ€ç ï¼‰
        recent_429 = [r for r in recent_records if r.status_code == 429]
        if recent_429 and not self.blocked_warned:
            print(f"\n[ğŸš« é™æµè­¦å‘Š] æ£€æµ‹åˆ° {len(recent_429)} æ¬¡ 429 çŠ¶æ€ç ï¼ˆè¯·æ±‚è¿‡äºé¢‘ç¹ï¼‰")
            print(f"  [å»ºè®®] è¯·ç­‰å¾…å‡ åˆ†é’Ÿåé‡è¯•ï¼Œæˆ–å¢åŠ è¯·æ±‚å»¶è¿Ÿ")
            self.blocked_warned = True
    
    def should_wait(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦ç­‰å¾…ï¼ˆåŸºäºè¯·æ±‚é¢‘ç‡ï¼‰"""
        now = time.time()
        
        # æ£€æŸ¥æœ€è¿‘1ç§’çš„è¯·æ±‚æ•°
        very_recent = [r for r in self.records if now - r.timestamp < 1]
        if len(very_recent) >= self.MAX_REQUESTS_PER_SECOND:
            return True
        
        # æ£€æŸ¥æœ€è¿‘1åˆ†é’Ÿçš„è¯·æ±‚æ•°
        recent = [r for r in self.records if now - r.timestamp < 60]
        if len(recent) >= self.MAX_REQUESTS_PER_MINUTE * 0.9:
            return True
        
        return False
    
    async def wait_if_needed(self):
        """å¦‚æœè¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œç­‰å¾…åå†ç»§ç»­"""
        while self.should_wait():
            wait_time = 0.5
            print(f"  [ç­‰å¾…] è¯·æ±‚é¢‘ç‡è¿‡é«˜ï¼Œç­‰å¾… {wait_time} ç§’...")
            await asyncio.sleep(wait_time)
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æµé‡ç»Ÿè®¡"""
        if not self.records:
            return {
                "total_requests": 0,
                "successful_requests": 0,
                "failed_requests": 0,
                "api_requests": 0,
                "page_requests": 0,
                "requests_per_minute": 0,
                "average_response_time": 0,
                "session_duration": 0,
                "status_codes": {},
                "rate_limit_warnings": False,
                "blocked_warnings": False
            }
        
        now = time.time()
        session_duration = now - self.session_start
        
        # æŒ‰ç±»å‹ç»Ÿè®¡
        api_requests = [r for r in self.records if r.request_type == "api"]
        page_requests = [r for r in self.records if r.request_type == "page"]
        
        # æˆåŠŸ/å¤±è´¥
        successful = [r for r in self.records if r.success]
        failed = [r for r in self.records if not r.success]
        
        # å“åº”æ—¶é—´
        response_times = [r.response_time for r in self.records if r.response_time > 0]
        avg_response_time = sum(response_times) / len(response_times) if response_times else 0
        
        # æ¯åˆ†é’Ÿè¯·æ±‚æ•°
        requests_per_minute = len(self.records) / (session_duration / 60) if session_duration > 0 else 0
        
        # çŠ¶æ€ç ç»Ÿè®¡
        status_codes = {}
        for r in self.records:
            status_codes[r.status_code] = status_codes.get(r.status_code, 0) + 1
        
        return {
            "total_requests": len(self.records),
            "successful_requests": len(successful),
            "failed_requests": len(failed),
            "api_requests": len(api_requests),
            "page_requests": len(page_requests),
            "requests_per_minute": round(requests_per_minute, 2),
            "average_response_time": round(avg_response_time, 3),
            "session_duration": round(session_duration, 1),
            "status_codes": status_codes,
            "rate_limit_warnings": self.rate_limit_warned,
            "blocked_warnings": self.blocked_warned
        }
    
    def print_stats(self):
        """æ‰“å°æµé‡ç»Ÿè®¡æŠ¥å‘Š"""
        stats = self.get_stats()
        
        print("\n" + "=" * 60)
        print("ğŸ“Š NewsBank æµé‡æŠ¥å‘Š")
        print("=" * 60)
        print(f"  ä¼šè¯æ—¶é•¿: {stats['session_duration']} ç§’")
        print(f"  æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
        print(f"    - APIè¯·æ±‚: {stats['api_requests']}")
        print(f"    - é¡µé¢è¯·æ±‚: {stats['page_requests']}")
        print(f"  æˆåŠŸ/å¤±è´¥: {stats['successful_requests']} / {stats['failed_requests']}")
        print(f"  å¹³å‡è¯·æ±‚é¢‘ç‡: {stats['requests_per_minute']} è¯·æ±‚/åˆ†é’Ÿ")
        print(f"  å¹³å‡å“åº”æ—¶é—´: {stats['average_response_time']} ç§’")
        
        if stats['status_codes']:
            print(f"  çŠ¶æ€ç åˆ†å¸ƒ:")
            for code, count in sorted(stats['status_codes'].items()):
                print(f"    {code}: {count}")
        
        if stats['rate_limit_warnings']:
            print(f"  âš ï¸  æ›¾è§¦å‘é™æµè­¦å‘Š")
        if stats['blocked_warnings']:
            print(f"  ğŸš« æ›¾è¢«é˜»æ­¢è®¿é—®")
        
        print("=" * 60)
    
    def save_log(self, filename: str = None):
        """ä¿å­˜æµé‡æ—¥å¿—åˆ°æ–‡ä»¶"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"traffic_log_{timestamp}.json"
        
        filepath = self.output_dir / filename
        
        stats = self.get_stats()
        
        log_data = {
            "session_start": datetime.fromtimestamp(self.session_start).isoformat(),
            "session_end": datetime.now().isoformat(),
            "stats": stats,
            "records": [
                {
                    "timestamp": datetime.fromtimestamp(r.timestamp).isoformat(),
                    "url": r.url[:100] + "..." if len(r.url) > 100 else r.url,
                    "method": r.method,
                    "status_code": r.status_code,
                    "response_time": r.response_time,
                    "success": r.success,
                    "error": r.error,
                    "request_type": r.request_type
                }
                for r in self.records
            ]
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, ensure_ascii=False, indent=2)
        
        print(f"\n[ğŸ’¾ æµé‡æ—¥å¿—å·²ä¿å­˜]: {filepath}")
        return filepath


class NewsBankAPIDownloader:
    """NewsBank API ä¸‹è½½å™¨ - ç›´æ¥è°ƒç”¨APIè·å–æ–‡ç« """
    
    def __init__(self,
                 headless: bool = False,
                 max_pages: int = 10,
                 output_dir: str = "articles_api",
                 request_delay: float = 2.0):
        self.headless = headless
        self.max_pages = max_pages
        self.request_delay = request_delay  # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰ï¼Œé˜²æ­¢è¢«å°
        
        self.cookie_file = Path("cookies/newsbank_auth.json")
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.cookie_file.parent.mkdir(parents=True, exist_ok=True)
        
        # æµé‡è®°å½•å™¨
        self.traffic_logger = TrafficLogger(output_dir=str(self.output_dir))
        
        # ç»Ÿè®¡
        self.stats = {
            "total_pages": 0,
            "total_articles": 0,
            "downloaded": 0,
            "skipped": 0,
            "errors": []
        }
        
        self.articles: List[ArticleInfo] = []
        self.api_endpoint = "https://infoweb-newsbank-com.ezproxy.sl.nsw.gov.au/apps/news/nb-multidocs/get"
    
    async def _safe_delay(self, seconds: float = 2.0):
        """å®‰å…¨çš„å»¶è¿Ÿï¼Œå¸¦æœ‰éšæœºæ³¢åŠ¨ä»¥æ¨¡æ‹Ÿäººç±»è¡Œä¸º"""
        import random
        delay = seconds if seconds else self.request_delay
        # æ·»åŠ éšæœºæ³¢åŠ¨ (Â±20%)ï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º
        jitter = delay * 0.2 * (random.random() * 2 - 1)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é¢å¤–ç­‰å¾…
        await self.traffic_logger.wait_if_needed()
        
        await asyncio.sleep(delay + jitter)
    
    def _record_page_access(self, url: str, success: bool = True, status_code: int = 200, error: str = None):
        """è®°å½•é¡µé¢è®¿é—®"""
        self.traffic_logger.record_request(
            url=url,
            method="GET",
            status_code=status_code,
            success=success,
            error=error,
            request_type="page"
        )
    
    def _record_api_request(self, url: str, method: str = "POST", status_code: int = 200, 
                           response_time: float = 0, success: bool = True, error: str = None):
        """è®°å½•APIè¯·æ±‚"""
        self.traffic_logger.record_request(
            url=url,
            method=method,
            status_code=status_code,
            response_time=response_time,
            success=success,
            error=error,
            request_type="api"
        )
    
    async def check_login(self, context: BrowserContext) -> bool:
        """æ£€æŸ¥ç™»å½•çŠ¶æ€"""
        print("\n[æ£€æŸ¥ç™»å½•çŠ¶æ€]")
        print("-" * 40)
        
        if not self.cookie_file.exists():
            print("[ä¿¡æ¯] æœªæ‰¾åˆ°ç™»å½•Cookie")
            return False
        
        try:
            test_page = await context.new_page()
            await test_page.goto(
                "https://infoweb-newsbank-com.ezproxy.sl.nsw.gov.au/apps/news/browse-multi?p=AWGLNB",
                wait_until="networkidle", timeout=30000
            )
            
            current_url = test_page.url
            await test_page.close()
            
            if "infoweb-newsbank" in current_url and "login" not in current_url:
                print("[æˆåŠŸ] Cookieæœ‰æ•ˆï¼Œå·²ç™»å½•")
                return True
            else:
                print("[ä¿¡æ¯] Cookieå·²è¿‡æœŸï¼Œéœ€è¦é‡æ–°ç™»å½•")
                return False
                
        except Exception as e:
            print(f"[è­¦å‘Š] æ£€æŸ¥ç™»å½•çŠ¶æ€æ—¶å‡ºé”™: {e}")
            return False
    
    async def do_login(self, page: Page) -> bool:
        """æ‰§è¡Œç™»å½•"""
        print("\n[ç™»å½•]")
        print("-" * 40)
        print("è¯·åœ¨æµè§ˆå™¨çª—å£ä¸­å®Œæˆç™»å½•...")
        print("ç™»å½•æˆåŠŸåå°†è‡ªåŠ¨ç»§ç»­")
        
        try:
            await page.goto(
                "https://eresources.sl.nsw.gov.au/newsbank-including-access-australia",
                wait_until="networkidle", timeout=60000
            )
            
            start_time = asyncio.get_event_loop().time()
            while (asyncio.get_event_loop().time() - start_time) < 180:
                if "infoweb-newsbank-com.ezproxy" in page.url and "login" not in page.url:
                    print("[æˆåŠŸ] ç™»å½•æˆåŠŸï¼")
                    return True
                await asyncio.sleep(2)
            
            print("[é”™è¯¯] ç™»å½•è¶…æ—¶ï¼ˆ3åˆ†é’Ÿï¼‰")
            return False
            
        except Exception as e:
            print(f"[é”™è¯¯] ç™»å½•å¤±è´¥: {e}")
            return False
    
    def _build_search_url(self, 
                         keyword: str,
                         maxresults: int = 200,
                         source: str = "Australian Financial Review Collection",
                         year_from: Optional[int] = None,
                         year_to: Optional[int] = None,
                         first_page_maxresults: int = 60) -> str:
        """
        æ ¹æ®å…³é”®å­—æ„å»ºNewsBankæœç´¢URL
        
        å‚æ•°:
            keyword: æœç´¢å…³é”®å­—
            maxresults: æ¯é¡µç»“æœæ•°ï¼ˆåç»­é¡µé»˜è®¤20ï¼‰
            source: æ•°æ®æºåç§°
            year_from: èµ·å§‹å¹´ä»½
            year_to: ç»“æŸå¹´ä»½
            first_page_maxresults: ç¬¬ä¸€é¡µç»“æœæ•°ï¼ˆé»˜è®¤60ï¼‰
        
        è¿”å›:
            å®Œæ•´çš„æœç´¢URL
        """
        # åŸºç¡€URL
        base_url = "https://infoweb-newsbank-com.ezproxy.sl.nsw.gov.au/apps/news/results"
        
        # å‚æ•°æ„å»º - ä¸ç”¨æˆ·æä¾›çš„URLä¸€è‡´
        # sort=_rank_:D è¡¨ç¤ºæŒ‰ç›¸å…³æ€§æ’åº
        params = {
            "p": "AWGLNB",
            "hide_duplicates": "2",
            "fld-base-0": "alltext",
            "sort": "_rank_:D",  # æŒ‰ç›¸å…³æ€§æ’åº
            "maxresults": str(first_page_maxresults),  # ç¬¬ä¸€é¡µä½¿ç”¨60
            "val-base-0": keyword,
        }
        
        # æ„å»ºtå‚æ•°ï¼ˆæ•°æ®æºï¼‰
        source_encoded = quote(source)
        params["t"] = f"favorite:AFRWAFRN!{source_encoded}"
        
        # æ„å»ºå¹´ä»½è¿‡æ»¤
        if year_from or year_to:
            if year_from and year_to:
                year_filter = f"year:{year_from}%21{year_to}"
            elif year_from:
                year_filter = f"year:{year_from}"
            else:
                year_filter = f"year:{year_to}"
            params["t"] = f"{params['t']}/{year_filter}"
        
        # æ„å»ºURL - ä½¿ç”¨ä¸å‚è€ƒURLä¸€è‡´çš„æ ¼å¼ï¼ˆä½†ä¿ç•™YMD_date:Dæ’åºï¼‰
        query_string = f"p=AWGLNB&hide_duplicates=2&fld-base-0=alltext&sort=YMD_date:D&maxresults={params['maxresults']}&val-base-0={quote(params['val-base-0'])}&t={quote(params['t'])}"
        
        return f"{base_url}?{query_string}"
    
    def _extract_total_results(self, html_content: str) -> int:
        """ä»é¡µé¢HTMLä¸­æå–æ€»ç»“æœæ•°"""
        # åŒ¹é…æ ¼å¼ï¼š<div class="search-hits__meta--total_hits">1,006 Results</div>
        pattern = r'search-hits__meta--total_hits[^>]*>[\s]*([\d,]+)\s*Results'
        match = re.search(pattern, html_content)
        if match:
            total_str = match.group(1).replace(',', '')
            try:
                return int(total_str)
            except:
                pass
        return 0
    
    def _build_page_url(self, base_url: str, page_num: int, first_page_maxresults: int = 60, subsequent_maxresults: int = 20) -> str:
        """
        æ„å»ºåˆ†é¡µURL
        
        æ ¹æ®ç¿»é¡µ.txtçš„è§„å¾‹ï¼š
        - ç¬¬1é¡µï¼šoffset=0, maxresults=60
        - ç¬¬2é¡µåŠä»¥åï¼šoffset=63, maxresults=40, page=é¡µç -1
        """
        if page_num == 1:
            # ç¬¬ä¸€é¡µä¿æŒåŸæ ·
            return base_url
        else:
            # ç¬¬2é¡µåŠä»¥åï¼šæ·»åŠ  offset=63, maxresults=20, page=é¡µç -1
            parsed = urlparse(base_url)
            query_params = parse_qs(parsed.query)
            
            # è®¾ç½®åç»­é¡µçš„å‚æ•°
            query_params['offset'] = ['63']
            query_params['maxresults'] = [str(subsequent_maxresults)]
            query_params['page'] = [str(page_num - 1)]  # pageå‚æ•°ä»0å¼€å§‹
            query_params['hide_duplicates'] = ['0']  # åç»­é¡µå…³é—­hide_duplicates
            
            # é‡æ–°æ„å»ºURL
            new_query = "&".join([f"{k}={quote(v[0])}" for k, v in query_params.items()])
            return f"{parsed.scheme}://{parsed.netloc}{parsed.path}?{new_query}"
    
    def _is_search_keyword(self, input_text: str) -> bool:
        """åˆ¤æ–­è¾“å…¥æ˜¯æœç´¢å…³é”®å­—è¿˜æ˜¯URL"""
        # å¦‚æœåŒ…å«ç©ºæ ¼ä¸”ä¸æ˜¯å®Œæ•´URLï¼Œè®¤ä¸ºæ˜¯å…³é”®å­—
        if ' ' in input_text and not input_text.startswith('http'):
            return True
        # å¦‚æœä¸åŒ…å«apps/news/resultsï¼Œè®¤ä¸ºæ˜¯å…³é”®å­—
        if 'apps/news/results' not in input_text:
            return True
        return False
    
    def _extract_preview_from_html(self, html_content: str) -> List[Dict]:
        """ä»HTMLä¸­æå–æ–‡ç« çš„docrefå’Œpreview"""
        articles = []
        
        # åŒ¹é…æ–‡ç« å— - ä½¿ç”¨æ›´å®½çš„èŒƒå›´ï¼šsearch-hits__hit__inner
        preview_pattern = r'<div class="search-hits__hit__inner"[^>]*>.*?preview-first-paragraph.*?</div>\s*</div>\s*</div>'
        article_matches = re.findall(preview_pattern, html_content, re.DOTALL)
        
        print(f"  [è°ƒè¯•] ä»HTMLä¸­æ‰¾åˆ° {len(article_matches)} ä¸ªåŒ…å«previewçš„æ–‡ç« å—")
        
        for idx, article_html in enumerate(article_matches):
            # æå–docref - ä»hrefä¸­æå–
            doc_id = ''
            
            # æ–¹æ³•1: ä»hrefä¸­æå– docref=news/xxx
            docref_match = re.search(r'docref=(news/[^&"]+)', article_html)
            if docref_match:
                doc_id = docref_match.group(1)
            
            # æ–¹æ³•2: å¦‚æœæ–¹æ³•1æ²¡æœ‰ï¼Œå°è¯•data-doc-id
            if not doc_id:
                docref_match = re.search(r'data-doc-id="([^"]+)"', article_html)
                if docref_match:
                    doc_id = 'news/' + docref_match.group(1)
            
            if not doc_id:
                print(f"  [è°ƒè¯•] æ–‡ç« {idx+1} æœªèƒ½æå–åˆ°docref")
                continue
            
            # æå–preview - æ­£ç¡®åŒ¹é…<div class="preview-first-paragraph">åˆ°</div>ä¹‹é—´çš„å†…å®¹
            preview = ""
            preview_match = re.search(
                r'preview-first-paragraph[^>]*>(.*?)</div>',
                article_html, re.DOTALL
            )
            if preview_match:
                preview = preview_match.group(1).strip()
                # æ¸…ç†HTMLæ ‡ç­¾ï¼Œè·å–çº¯æ–‡æœ¬
                preview = re.sub(r'<[^>]+>', '', preview)
                preview = preview.strip()[:200]
                print(f"  [è°ƒè¯•] æ–‡ç« {idx+1} docref={doc_id}, preview={preview[:30]}...")
            
            if doc_id:
                articles.append({
                    'docref': doc_id,
                    'preview': preview
                })
        
        print(f"  [è°ƒè¯•] æå–åˆ° {len(articles)} ä¸ªpreviewæ•°æ®")
        return articles
    
    def _extract_article_ids_from_page(self, html_content: str) -> List[str]:
        """ä»é¡µé¢HTMLä¸­æå–æ–‡ç« IDåˆ—è¡¨"""
        article_ids = []
        
        # æ–¹æ³•1: åŒ¹é… doc= å‚æ•° (åœ¨hrefä¸­)
        pattern1 = r'href="[^"]*doc=([^&"\s]+)'
        matches1 = re.findall(pattern1, html_content)
        
        # æ–¹æ³•2: åŒ¹é… data-doc-id å±æ€§
        pattern2 = r'data-doc-id="([^"]+)"'
        matches2 = re.findall(pattern2, html_content)
        
        # æ–¹æ³•3: åŒ¹é… doc= åœ¨ä»»ä½•ä½ç½®
        pattern3 = r'doc=([^&"\s]+)'
        matches3 = re.findall(pattern3, html_content)
        
        all_matches = matches1 + matches2 + matches3
        
        for match in all_matches:
            # è§£ç URLç¼–ç 
            doc_id = unquote(match)
            # è¿‡æ»¤æ‰ä¸€äº›å¸¸è§çš„éæ–‡ç« IDå€¼
            if doc_id and doc_id not in article_ids and len(doc_id) > 5:
                article_ids.append(doc_id)
        
        return article_ids
    
    async def _extract_selected_articles_metadata(self, page: Page) -> List[Dict]:
        """
        ä»é¡µé¢æå–é€‰ä¸­æ–‡ç« çš„å…ƒæ•°æ®
        
        è¿”å›æ ¼å¼:
        [{"docref":"news/xxx","cache_type":"AWGLNB","size":xxx,"pbi":"xxx","title":"xxx","product":"AWGLNB"}, ...]
        
        è¿™ä¸ªæ ¼å¼æ˜¯ nb-cache-doc/js/set API æ‰€éœ€çš„
        """
        print("  [æå–] æå–é€‰ä¸­æ–‡ç« çš„å…ƒæ•°æ®...")
        
        try:
            # ä»é¡µé¢æå–é€‰ä¸­å¤é€‰æ¡†çš„æ–‡ç« ä¿¡æ¯
            metadata = await page.evaluate("""() => {
                const selectedArticles = [];
                
                // æŸ¥æ‰¾æ‰€æœ‰é€‰ä¸­çš„æ–‡ç« å¤é€‰æ¡†
                const checkboxes = document.querySelectorAll('article.search-hits__hit input[type="checkbox"]:checked');
                
                console.log('Found ' + checkboxes.length + ' checked checkboxes');
                
                checkboxes.forEach(checkbox => {
                    // æ‰¾åˆ°å¯¹åº”çš„æ–‡ç« å…ƒç´ 
                    const article = checkbox.closest('article.search-hits__hit');
                    if (!article) return;
                    
                    // æ‰“å°articleçš„datasetç”¨äºè°ƒè¯•
                    console.log('Article dataset:', JSON.stringify(article.dataset));
                    
                    // æå–æ–‡ç« ä¿¡æ¯ - ä»dataå±æ€§æå–
                    const docref = article.dataset.docId || '';
                    const pbi = article.dataset.pbi || article.dataset.pbI || '';
                    const cacheType = article.dataset.cacheType || article.dataset.cacheType || 'AWGLNB';
                    const product = article.dataset.product || 'AWGLNB';
                    const size = article.dataset.size || article.dataset.docSize || '0';
                    
                    // æ–¹æ³•2: ä»hrefä¸­æå–docref
                    const link = article.querySelector('h3.search-hits__hit__title a');
                    const href = link ? link.href : '';
                    const docMatch = href.match(/doc=([^&]+)/);
                    const docIdFromHref = docMatch ? 'news/' + docMatch[1] : '';
                    
                    // æå–æ ‡é¢˜
                    const title = link ? link.textContent.trim() : '';
                    
                    // ä½¿ç”¨ä»hrefä¸­æå–çš„docrefï¼ˆæ›´å¯é ï¼‰
                    const finalDocref = docIdFromHref || docref;
                    
                    if (finalDocref) {
                        selectedArticles.push({
                            docref: finalDocref,
                            cache_type: cacheType,
                            size: parseInt(size) || 0,
                            pbi: pbi,
                            title: title,
                            product: product
                        });
                    }
                });
                
                // å¦‚æœæ²¡æœ‰é€‰ä¸­ä»»ä½•æ–‡ç« ï¼Œè¿”å›æ‰€æœ‰æ–‡ç« ï¼ˆæœªé€‰ä¸­çš„ï¼‰
                if (selectedArticles.length === 0) {
                    const allArticles = document.querySelectorAll('article.search-hits__hit');
                    console.log('No checked articles, trying all ' + allArticles.length + ' articles');
                    
                    allArticles.forEach(article => {
                        const link = article.querySelector('h3.search-hits__hit__title a');
                        const href = link ? link.href : '';
                        const docMatch = href.match(/doc=([^&]+)/);
                        const docIdFromHref = docMatch ? 'news/' + docMatch[1] : '';
                        const title = link ? link.textContent.trim() : '';
                        
                        // ä»å„ç§å¯èƒ½çš„dataå±æ€§ä¸­æå–
                        const pbi = article.dataset.pbi || article.dataset.pbI || 
                                   article.dataset.bpi || '';
                        const cacheType = article.dataset.cacheType || 'AWGLNB';
                        const product = article.dataset.product || 'AWGLNB';
                        const size = article.dataset.size || article.dataset.docSize || '0';
                        
                        if (docIdFromHref) {
                            selectedArticles.push({
                                docref: docIdFromHref,
                                cache_type: cacheType,
                                size: parseInt(size) || 0,
                                pbi: pbi,
                                title: title,
                                product: product
                            });
                        }
                    });
                }
                
                return selectedArticles;
            }""")
            
            print(f"  [æå–] æ‰¾åˆ° {len(metadata)} ç¯‡é€‰ä¸­æ–‡ç« ")
            
            # æ‰“å°å‰å‡ ç¯‡ç”¨äºè°ƒè¯•
            for i, art in enumerate(metadata[:3]):
                print(f"    [{i+1}] title={art.get('title', 'N/A')[:30]}...")
                print(f"        docref={art.get('docref', 'N/A')}, size={art.get('size', 0)}, pbi={art.get('pbi', 'N/A')[:20]}...")
            
            return metadata
            
        except Exception as e:
            print(f"  [é”™è¯¯] æå–é€‰ä¸­æ–‡ç« å…ƒæ•°æ®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    async def _capture_selected_articles_payload(self, page: Page) -> Optional[str]:
        """
        é€šè¿‡ç›‘å¬ç½‘ç»œè¯·æ±‚æ•è·å®é™…å‘é€çš„payload
        
        åœ¨é€‰ä¸­æ–‡ç« åï¼Œç½‘é¡µä¼šå‘é€ä¸€ä¸ªPOSTè¯·æ±‚åˆ° nb-cache-doc/js/set
        è¿™ä¸ªæ–¹æ³•æ•è·è¯¥è¯·æ±‚çš„bodyï¼Œå³å®é™…ä½¿ç”¨çš„payload
        """
        print("  [æ•è·] ç›‘å¬ç½‘ç»œè¯·æ±‚ä»¥æ•è·å®é™…payload...")
        
        captured_payloads = []
        
        async def handle_request(request):
            # ç›‘å¬ nb-cache-doc/js/set æˆ– nb-multidocs/get è¯·æ±‚
            url = request.url
            if "nb-cache-doc" in url or "nb-multidocs" in url:
                try:
                    # è·å–è¯·æ±‚çš„postæ•°æ®
                    post_data = request.post_data
                    if post_data:
                        if isinstance(post_data, bytes):
                            post_data = post_data.decode('utf-8')
                        
                        # åªæ•è·åŒ…å«docs=çš„è¯·æ±‚ï¼ˆè¿™æ˜¯è®¾ç½®é€‰ä¸­æ–‡ç«  çš„è¯·æ±‚ï¼‰
                        if 'docs=' in post_data:
                            captured_payloads.append({
                                'url': url,
                                'post_data': post_data,
                                'timestamp': time.time(),
                                'length': len(post_data)
                            })
                            print(f"  [æ•è·] æ•è·åˆ°è¯·æ±‚: {request.method} {url.split('/')[-1]}")
                            print(f"  [æ•è·] Payloadé•¿åº¦: {len(post_data)} å­—ç¬¦")
                            print(f"  [æ•è·] Payloadé¢„è§ˆ: {post_data[:200]}...")
                except Exception as e:
                    pass  # é™é»˜å¤„ç†ï¼Œé¿å…è¿‡å¤šè¾“å‡º
        
        # è®¾ç½®ç›‘å¬å™¨
        page.on("request", handle_request)
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œæ”¶é›†æ‰€æœ‰è¯·æ±‚
        print("  [ç­‰å¾…] ç­‰å¾…æ•è·è¯·æ±‚...")
        for i in range(4):
            await asyncio.sleep(1)
            if captured_payloads:
                print(f"  [ä¿¡æ¯] å·²æ•è· {len(captured_payloads)} ä¸ªç›¸å…³è¯·æ±‚")
                # æ‰“å°æ¯ä¸ªæ•è·çš„payloadé•¿åº¦
                for idx, p in enumerate(captured_payloads):
                    print(f"    [{idx+1}] é•¿åº¦: {p['length']} å­—ç¬¦")
        
        if captured_payloads:
            # ä½¿ç”¨æœ€åä¸€ä¸ªåŒ…å«docs=çš„è¯·æ±‚ï¼ˆé€šå¸¸æ˜¯æœ€æ–°çš„ï¼‰
            # å› ä¸ºé€‰ä¸­æ“ä½œä¼šè§¦å‘å¤šæ¬¡è¯·æ±‚
            last_payload = captured_payloads[-1]['post_data']
            payload_length = len(last_payload)
            print(f"  [æˆåŠŸ] ä½¿ç”¨æœ€åä¸€ä¸ªè¯·æ±‚çš„payload ({payload_length} å­—ç¬¦)")
            
            # æ£€æŸ¥payloadæ˜¯å¦å¯èƒ½è¢«æˆªæ–­
            if payload_length > 5000:
                print(f"  [è­¦å‘Š] Payloadè¾ƒé•¿({payload_length}å­—ç¬¦)ï¼Œæ£€æŸ¥å®Œæ•´æ€§...")
                # æ£€æŸ¥æ˜¯å¦æœ‰æœªé—­åˆçš„æ‹¬å·
                bracket_count = last_payload.count('[') - last_payload.count(']')
                brace_count = last_payload.count('{') - last_payload.count('}')
                print(f"  [æ£€æŸ¥] æ‹¬å·å¹³è¡¡: [ {bracket_count}, {{ {brace_count}")
                if bracket_count != 0 or brace_count != 0:
                    print(f"  [è­¦å‘Š] Payloadå¯èƒ½è¢«æˆªæ–­ï¼")
            
            return last_payload
        else:
            print(f"  [è­¦å‘Š] æœªæ•è·åˆ°åŒ…å«docs=çš„payload")
            return None
    
    def _parse_captured_payload(self, payload_str: str) -> Optional[List[Dict]]:
        """
        è§£ææ•è·çš„payloadå­—ç¬¦ä¸²ï¼Œæå–æ–‡ç« å…ƒæ•°æ®åˆ—è¡¨
        
        è¿”å›æ ¼å¼:
        [{"docref":"news/xxx","cache_type":"AWGLNB","size":xxx,"pbi":"xxx","title":"xxx","product":"AWGLNB"}, ...]
        """
        try:
            # payloadå¯èƒ½æ˜¯URLç¼–ç çš„ï¼Œå¯èƒ½éœ€è¦å¤šæ¬¡è§£ç 
            decoded = payload_str
            
            # å°è¯•å¤šæ¬¡è§£ç ï¼Œç›´åˆ°ä¸å†æ˜¯URLç¼–ç æ ¼å¼
            max_iterations = 3
            for i in range(max_iterations):
                try:
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«URLç¼–ç ï¼ˆ%XXæ ¼å¼ï¼‰
                    if '%' in decoded:
                        decoded = unquote(decoded)
                        print(f"  [è§£ç ] ç¬¬{i+1}æ¬¡è§£ç å: {decoded[:200]}...")
                    else:
                        break
                except Exception:
                    break
            
            # æ‰“å°å®Œæ•´çš„è§£ç åçš„payloadç”¨äºè°ƒè¯•
            print(f"  [è°ƒè¯•] å®Œæ•´payload: {decoded[:500]}...")
            print(f"  [è°ƒè¯•] Payloadæ€»é•¿åº¦: {len(decoded)} å­—ç¬¦")
            
            # æå– docs= åé¢çš„JSONæ•°ç»„
            if 'docs=' in decoded:
                json_part = decoded.split('docs=')[1]
                
                # å¯èƒ½è¿˜åŒ…å«å…¶ä»–å‚æ•°ï¼Œç”¨ & åˆ†éš”
                # ä½†è¦æ³¨æ„JSONå†…éƒ¨å¯èƒ½æœ‰ &amp; (HTMLå®ä½“) è§£ç åçš„ &
                # éœ€è¦æ™ºèƒ½æ‰¾åˆ°çœŸæ­£çš„å‚æ•°åˆ†éš”ä½ç½®
                if '&' in json_part:
                    # å°è¯•æ‰¾åˆ°JSONæ•°ç»„çš„ç»“æŸä½ç½®ï¼ˆæœ€åä¸€ä¸ª]ï¼‰
                    # é€šè¿‡è®¡ç®—æ‹¬å·çš„å¹³è¡¡æ¥ç¡®å®š
                    last_bracket = json_part.rfind(']')
                    if last_bracket > 0:
                        # æ£€æŸ¥åé¢æ˜¯å¦è¿˜æœ‰å…¶ä»–å‚æ•°
                        remaining = json_part[last_bracket+1:]
                        if remaining and '&' in remaining:
                            # ç¡®å®æœ‰å…¶ä»–å‚æ•°ï¼Œåªå–åˆ°]ä¸ºæ­¢
                            json_part = json_part[:last_bracket+1]
                            print(f"  [è°ƒè¯•] æ™ºèƒ½æˆªæ–­åˆ°æœ€åä¸€ä¸ª]ï¼Œä½ç½®: {last_bracket}")
                        else:
                            # åé¢æ²¡æœ‰&ï¼Œè¯´æ˜æ²¡æœ‰å…¶ä»–å‚æ•°äº†
                            json_part = json_part[:last_bracket+1]
                            print(f"  [è°ƒè¯•] æ— åç»­å‚æ•°ï¼Œä½¿ç”¨å®Œæ•´JSONï¼Œä½ç½®: {last_bracket}")
                    else:
                        # æ²¡æœ‰æ‰¾åˆ°]ï¼Œç”¨åŸæ¥çš„æ–¹å¼
                        print(f"  [è°ƒè¯•] æœªæ‰¾åˆ°]ï¼Œä½¿ç”¨åŸæ–¹å¼")
                        json_part = json_part.split('&')[0]
                
                # æ¸…ç†å¯èƒ½çš„ç©ºç™½å­—ç¬¦
                json_part = json_part.strip()
                
                print(f"  [è°ƒè¯•] JSONéƒ¨åˆ†é•¿åº¦: {len(json_part)} å­—ç¬¦")
                print(f"  [è°ƒè¯•] JSONéƒ¨åˆ†å‰200å­—ç¬¦: {json_part[:200]}...")
                
                # æ£€æŸ¥JSONæ˜¯å¦è¢«æˆªæ–­ - æ›´è¯¦ç»†çš„æ£€æŸ¥
                bracket_diff = json_part.count('[') - json_part.count(']')
                brace_diff = json_part.count('{') - json_part.count('}')
                print(f"  [è°ƒè¯•] æ‹¬å·å¹³è¡¡: [ {bracket_diff}, {{ {brace_diff}")
                
                if bracket_diff > 0 or brace_diff > 0:
                    print(f"  [è­¦å‘Š] JSONæ•°ç»„å¯èƒ½æœªæ­£ç¡®å…³é—­ï¼Œå°è¯•ä¿®å¤...")
                    # å°è¯•è¡¥å…¨ç¼ºå¤±çš„ ]
                    if bracket_diff > 0:
                        json_part = json_part + ']' * bracket_diff
                    if brace_diff > 0:
                        json_part = json_part + '}' * brace_diff
                
                # è§£æJSON
                try:
                    articles = json.loads(json_part)
                    print(f"  [è§£æ] ä»payloadä¸­æå–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
                    
                    # æ‰“å°æ–‡ç« æ ‡é¢˜ç”¨äºè°ƒè¯•
                    for i, art in enumerate(articles[:5]):
                        print(f"    [{i+1}] {art.get('title', 'N/A')[:40]}")
                        print(f"        docref: {art.get('docref', 'N/A')}")
                    
                    return articles
                except json.JSONDecodeError as json_err:
                    print(f"  [è§£æ] JSONè§£æå¤±è´¥: {json_err}")
                    # å°è¯•æ›´è¯¦ç»†çš„é”™è¯¯å®šä½
                    print(f"  [è°ƒè¯•] é”™è¯¯ä½ç½®: char {json_err.pos}, line {json_err.lineno}, col {json_err.colno}")
                    # æ‰“å°é”™è¯¯ä½ç½®å‰åçš„å­—ç¬¦
                    if json_err.pos and json_err.pos < len(json_part):
                        start = max(0, json_err.pos - 50)
                        end = min(len(json_part), json_err.pos + 50)
                        print(f"  [è°ƒè¯•] é”™è¯¯ä½ç½®é™„è¿‘: ...{json_part[start:end]}...")
                    
                    # å¦‚æœJSONè¢«æˆªæ–­ï¼Œå°è¯•åªè§£æå·²æœ‰çš„éƒ¨åˆ†
                    if json_err.pos:
                        truncated_json = json_part[:json_err.pos]
                        print(f"  [å¤‡é€‰] å°è¯•è§£ææˆªæ–­çš„JSONéƒ¨åˆ†...")
                        try:
                            # å°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„å¯¹è±¡
                            # é€šè¿‡æ‰¾åˆ°æœ€åä¸€ä¸ª } æ¥åˆ¤æ–­
                            last_brace = truncated_json.rfind('}')
                            if last_brace > 0:
                                truncated_json = truncated_json[:last_brace+1]
                                truncated_json = '[' + truncated_json + ']'
                                articles = json.loads(truncated_json)
                                print(f"  [è§£æ] ä»æˆªæ–­payloadä¸­æå–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
                                return articles
                        except:
                            pass
                    raise
            
            elif decoded.startswith('['):
                # å°è¯•ç›´æ¥è§£æï¼ˆæ²¡æœ‰docs=å‰ç¼€ï¼‰
                articles = json.loads(decoded)
                return articles
            else:
                print(f"  [è­¦å‘Š] payloadä¸­æœªæ‰¾åˆ°docs=å‚æ•°")
                print(f"  [è°ƒè¯•] payloadå†…å®¹: {decoded[:500]}...")
                return None
                
        except Exception as e:
            print(f"  [è§£æ] è§£æpayloadå¤±è´¥: {e}")
            # æ‰“å°åŸå§‹payloadç”¨äºè°ƒè¯•
            print(f"  [è°ƒè¯•] åŸå§‹payload: {payload_str[:300]}...")
            print(f"  [è°ƒè¯•] åŸå§‹payloadé•¿åº¦: {len(payload_str)} å­—ç¬¦")
            return None
    
    async def _save_article_metadata_to_json(self, article_metadata: List[Dict], keyword: str) -> Path:
        """
        å°†æ–‡ç« å…ƒæ•°æ®ä¿å­˜åˆ°JSONæ–‡ä»¶
        
        Args:
            article_metadata: æ–‡ç« å…ƒæ•°æ®åˆ—è¡¨
            keyword: æœç´¢å…³é”®å­—
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        # ç­›é€‰å¤„ç†ï¼šåªä¿ç•™ docref ä»¥ "news/" å¼€å¤´çš„è®°å½•ï¼ˆè¿‡æ»¤æ‰ image/ ç­‰éæ–‡ç« è®°å½•ï¼‰
        filtered_metadata = [
            art for art in article_metadata 
            if art.get('docref', '').startswith('news/')
        ]
        
        if len(filtered_metadata) < len(article_metadata):
            removed_count = len(article_metadata) - len(filtered_metadata)
            print(f"  [ç­›é€‰] è¿‡æ»¤æ‰ {removed_count} æ¡é news/ å¼€å¤´çš„è®°å½•")
            print(f"  [ç­›é€‰] ä¿ç•™ {len(filtered_metadata)} æ¡è®°å½•")
        
        # å¦‚æœç­›é€‰åæ²¡æœ‰æœ‰æ•ˆè®°å½•ï¼Œå‘å‡ºè­¦å‘Š
        if not filtered_metadata:
            print(f"  [è­¦å‘Š] ç­›é€‰åæ²¡æœ‰æœ‰æ•ˆè®°å½•ï¼Œè¯·æ£€æŸ¥ docref æ ¼å¼")
            filtered_metadata = article_metadata  # ä¿ç•™åŸå§‹æ•°æ®ä»¥ä¾¿è°ƒè¯•
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_keyword = re.sub(r'[^\w\s-]', '', keyword).replace(' ', '_')[:30]
        filename = f"article_{safe_keyword}_{timestamp}.json"
        filepath = self.output_dir / filename
        
        # å‡†å¤‡ä¿å­˜çš„æ•°æ®
        save_data = {
            "search_keyword": keyword,
            "extracted_at": datetime.now().isoformat(),
            "total_articles": len(filtered_metadata),
            "original_count": len(article_metadata),  # è®°å½•åŸå§‹æ•°é‡
            "articles": filtered_metadata
        }
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        print(f"  [ä¿å­˜] å…ƒæ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
        return filepath
    
    def _init_llm_client(self, api_key: str = None, model: str = None):
        """
        åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
        
        Args:
            api_key: APIå¯†é’¥ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–
            model: æ¨¡å‹åç§°
            
        Returns:
            openaiå®¢æˆ·ç«¯ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        if not OPENAI_AVAILABLE:
            print("[é”™è¯¯] openai åŒ…æœªå®‰è£…")
            return None
        
        # è·å– API Key
        if not api_key:
            api_key = os.getenv("NVIDIA_API_KEY") or os.getenv("OPENAI_API_KEY")
        
        if not api_key:
            print("[é”™è¯¯] æœªè®¾ç½® API Keyï¼Œè¯·è®¾ç½® NVIDIA_API_KEY æˆ– OPENAI_API_KEY ç¯å¢ƒå˜é‡")
            return None
        
        # æ£€æµ‹ provider
        if api_key.startswith("nvapi-"):
            provider = "nvidia"
            base_url = "https://integrate.api.nvidia.com/v1"
            default_model = "z-ai/glm4.7"  # æ¨èä¸­æ–‡ç†è§£å¥½çš„æ¨¡å‹
        else:
            provider = "openai"
            base_url = None
            default_model = "gpt-3.5-turbo"
        
        # ä¼˜å…ˆçº§ï¼šå‘½ä»¤è¡Œå‚æ•° > ç¯å¢ƒå˜é‡ > é»˜è®¤å€¼
        if not model:
            # å°è¯•ä» LLM_MODEL ç¯å¢ƒå˜é‡è¯»å–
            model = os.getenv("LLM_MODEL")
        
        if not model:
            model = default_model
        
        print(f"[LLM] Provider: {provider}")
        print(f"[LLM] Model: {model}")
        
        try:
            client = openai.OpenAI(
                api_key=api_key,
                base_url=base_url
            )
            return client, model
        except Exception as e:
            print(f"[é”™è¯¯] åˆå§‹åŒ– LLM å®¢æˆ·ç«¯å¤±è´¥: {e}")
            return None
    
    def _build_relevance_prompt(self, keyword: str, articles: List[Dict]) -> str:
        """
        æ„å»ºç›¸å…³æ€§åˆ¤æ–­çš„ prompt
        
        Args:
            keyword: æœç´¢å…³é”®å­—
            articles: æ–‡ç« åˆ—è¡¨ï¼ŒåŒ…å« title å’Œ preview
            
        Returns:
            prompt å­—ç¬¦ä¸²
        """
        # è§£ç URLç¼–ç çš„æ ‡é¢˜å’Œé¢„è§ˆ
        decoded_articles = []
        for art in articles:
            title = art.get('title', '')
            preview = art.get('preview', '')[:200] if art.get('preview') else ''  # é™åˆ¶previewé•¿åº¦
            
            # URLè§£ç 
            title = title.replace('+', ' ')
            try:
                title = unquote(title)
            except:
                pass
            
            if preview:
                preview = preview.replace('+', ' ')
                try:
                    preview = unquote(preview)
                except:
                    pass
            
            decoded_articles.append({
                'title': title[:100],  # é™åˆ¶æ ‡é¢˜é•¿åº¦
                'preview': preview[:200] if preview else ''
            })
        
        # æ„å»ºprompt
        articles_text = "\n".join([
            f"{i+1}. æ ‡é¢˜: {a['title'][:80]}\n   é¢„è§ˆ: {a['preview'][:150] if a['preview'] else '(æ— é¢„è§ˆ)'}"
            for i, a in enumerate(decoded_articles)
        ])
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ–‡ç« ç›¸å…³æ€§åˆ¤æ–­ä¸“å®¶ã€‚è¯·åˆ¤æ–­ä»¥ä¸‹æ–‡ç« æ˜¯å¦ä¸æœç´¢å…³é”®å­—"{keyword}"ç›¸å…³ã€‚

åˆ¤æ–­æ ‡å‡†ï¼š
- ç›´æ¥æåˆ°å…³é”®å­—æˆ–å…³é”®å­—çš„å˜ä½“ï¼ˆå¦‚å…¬å¸åï¼Œå“ç‰Œåã€ç¼©å†™ï¼‰
- è®¨è®ºä¸å…³é”®å­—ç›¸å…³çš„äº‹ä»¶ã€äº§å“ã€æœåŠ¡
- ä¸å…³é”®å­—æ‰€åœ¨è¡Œä¸šæˆ–é¢†åŸŸç›´æ¥ç›¸å…³
- ä»…ä»…æ˜¯é€šç”¨æ–°é—»ä½†æ²¡æœ‰å®è´¨æ€§æåˆ°å…³é”®å­—ï¼Œä¸ç®—ç›¸å…³

æ–‡ç« åˆ—è¡¨ï¼š
{articles_text}

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼š
{{
    "results": [
        {{"index": 1, "relevant": true/false, "reason": "ç®€çŸ­åŸå› "}},
        ...
    ]
}}

åªè¿”å›JSONï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ã€‚"""
        
        return prompt
    
    async def _filter_articles_by_llm(self, 
                                       json_file: Path, 
                                       api_key: str = None, 
                                       model: str = None,
                                       threshold: float = 0.5,
                                       batch_size: int = 10) -> Optional[Path]:
        """
        ä½¿ç”¨ LLM ç­›é€‰ç›¸å…³æ–‡ç« 
        
        Args:
            json_file: è¾“å…¥çš„ JSON æ–‡ä»¶è·¯å¾„
            api_key: API å¯†é’¥
            model: æ¨¡å‹åç§°
            threshold: ç›¸å…³æ€§é˜ˆå€¼ (0-1)
            batch_size: æ¯æ‰¹æ¬¡å¤„ç†çš„æ–‡ç« æ•°
            
        Returns:
            ç­›é€‰åçš„ JSON æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å› None
        """
        print("\n" + "=" * 60)
        print("[LLM æ™ºèƒ½ç­›é€‰]")
        print("=" * 60)
        
        if not OPENAI_AVAILABLE:
            print("[é”™è¯¯] openai åŒ…æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨ LLM ç­›é€‰")
            return None
        
        # åŠ è½½ JSON æ–‡ä»¶
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except Exception as e:
            print(f"[é”™è¯¯] åŠ è½½ JSON æ–‡ä»¶å¤±è´¥: {e}")
            return None
        
        keyword = data.get('search_keyword', '')
        articles = data.get('articles', [])
        
        if not articles:
            print("[è­¦å‘Š] æ–‡ç« åˆ—è¡¨ä¸ºç©º")
            return None
        
        print(f"[LLM] æœç´¢å…³é”®å­—: {keyword}")
        print(f"[LLM] æ–‡ç« æ€»æ•°: {len(articles)}")
        print(f"[LLM] ç›¸å…³æ€§é˜ˆå€¼: {threshold}")
        
        # åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
        llm_result = self._init_llm_client(api_key, model)
        if not llm_result:
            return None
        
        client, model = llm_result
        
        # æå–æ‰€æœ‰æ ‡é¢˜
        titles = [art.get('title', '') for art in articles]
        
        # æ‰¹é‡å¤„ç†
        relevant_articles = []
        total_batches = (len(articles) + batch_size - 1) // batch_size
        
        print(f"[LLM] å¼€å§‹ç­›é€‰ï¼Œå…± {total_batches} æ‰¹æ¬¡...")
        
        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, len(articles))
            batch_titles = titles[start_idx:end_idx]
            batch_articles = articles[start_idx:end_idx]
            
            print(f"[LLM] å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} ({start_idx+1}-{end_idx})...")
            
            # æ„å»º prompt
            prompt = self._build_relevance_prompt(keyword, batch_titles)
            
            try:
                response = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ–‡ç« ç›¸å…³æ€§åˆ¤æ–­ä¸“å®¶ï¼Œæ“…é•¿åˆ†ææ–‡ç« æ ‡é¢˜ä¸æœç´¢ä¸»é¢˜çš„ç›¸å…³æ€§ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.1,
                    max_tokens=2000
                )
                
                result_text = response.choices[0].message.content.strip()
                
                # è§£æ JSON ç»“æœ
                try:
                    # å°è¯•æå– JSON
                    if '```json' in result_text:
                        result_text = result_text.split('```json')[1].split('```')[0]
                    elif '```' in result_text:
                        result_text = result_text.split('```')[1].split('```')[0]
                    
                    result_json = json.loads(result_text)
                    results = result_json.get('results', [])
                    
                    # å¤„ç†ç»“æœ
                    for i, result in enumerate(results):
                        if result.get('relevant', False):
                            relevant_articles.append(batch_articles[i])
                    
                    # æ‰“å°æ‰¹æ¬¡ç»“æœ
                    relevant_count = sum(1 for r in results if r.get('relevant', False))
                    print(f"    æ‰¹æ¬¡ {batch_idx + 1}: {relevant_count}/{len(batch_articles)} ç›¸å…³æ–‡ç« ")
                    
                except json.JSONDecodeError as json_err:
                    print(f"    [è­¦å‘Š] è§£æ LLM å“åº”å¤±è´¥: {json_err}")
                    # å¦‚æœè§£æå¤±è´¥ï¼Œé»˜è®¤ä¿ç•™æ‰€æœ‰æ–‡ç« 
                    relevant_articles.extend(batch_articles)
                    
            except Exception as api_err:
                print(f"    [é”™è¯¯] API è°ƒç”¨å¤±è´¥: {api_err}")
                # API å¤±è´¥æ—¶é»˜è®¤ä¿ç•™æ‰€æœ‰æ–‡ç« 
                relevant_articles.extend(batch_articles)
            
            # é¿å…è¿‡å¿«è¯·æ±‚
            if batch_idx < total_batches - 1:
                await asyncio.sleep(1)
        
        # ç»Ÿè®¡ç»“æœ
        filtered_count = len(relevant_articles)
        removed_count = len(articles) - filtered_count
        
        print(f"\n[LLM ç­›é€‰ç»“æœ]")
        print(f"  åŸå§‹æ–‡ç« : {len(articles)}")
        print(f"  ç›¸å…³æ–‡ç« : {filtered_count}")
        print(f"  è¿‡æ»¤æ‰: {removed_count}")
        print(f"  ç­›é€‰æ¯”ä¾‹: {filtered_count/len(articles)*100:.1f}%")
        
        if not relevant_articles:
            print("[è­¦å‘Š] æ²¡æœ‰ç›¸å…³æ–‡ç« è¢«ä¿ç•™")
            return None
        
        # ä¿å­˜ç­›é€‰åçš„ç»“æœ
        output_data = {
            "search_keyword": keyword,
            "extracted_at": data.get('extracted_at', datetime.now().isoformat()),
            "llm_filtered_at": datetime.now().isoformat(),
            "total_articles": filtered_count,
            "original_count": len(articles),
            "filter_threshold": threshold,
            "filter_model": model,
            "articles": relevant_articles
        }
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_keyword = re.sub(r'[^\w\s-]', '', keyword).replace(' ', '_')[:30]
        output_filename = f"article_{safe_keyword}_filtered_{timestamp}.json"
        output_filepath = self.output_dir / output_filename
        
        with open(output_filepath, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\n[ä¿å­˜] ç­›é€‰åçš„ç»“æœå·²ä¿å­˜åˆ°: {output_filepath}")
        return output_filepath
    
    async def _filter_single_page_with_llm(self,
                                            article_metadata: List[Dict],
                                            keyword: str,
                                            llm_client,
                                            llm_model: str,
                                            threshold: float = 0.5) -> List[Dict]:
        """
        å¯¹å•é¡µæ–‡ç« è¿›è¡Œ LLM ç­›é€‰
        
        Args:
            article_metadata: å•é¡µæ–‡ç« å…ƒæ•°æ®åˆ—è¡¨
            keyword: æœç´¢å…³é”®å­—
            llm_client: å·²åˆå§‹åŒ–çš„ LLM å®¢æˆ·ç«¯
            llm_model: æ¨¡å‹åç§°
            threshold: ç›¸å…³æ€§é˜ˆå€¼
            
        Returns:
            ç­›é€‰åçš„æ–‡ç« åˆ—è¡¨
        """
        if not article_metadata:
            return []
        
        # è§£ç æ ‡é¢˜ç”¨äºæ˜¾ç¤º
        decoded_titles = []
        for title in article_metadata:
            t = title.get('title', '')
            t = t.replace('+', ' ')
            try:
                t = unquote(t)
            except:
                pass
            decoded_titles.append(t[:60] + "..." if len(t) > 60 else t)
        
        print(f"    [LLM] å¾…ç­›é€‰æ–‡ç« æ ‡é¢˜:")
        for i, t in enumerate(decoded_titles[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"      [{i+1}] {t}")
        if len(decoded_titles) > 5:
            print(f"      ... è¿˜æœ‰ {len(decoded_titles) - 5} ç¯‡")
        
        # æ„å»º prompt - ä¼ å…¥å®Œæ•´æ–‡ç« åˆ—è¡¨
        prompt = self._build_relevance_prompt(keyword, article_metadata)
        
        print(f"    [LLM] å‘é€è¯·æ±‚åˆ°æ¨¡å‹: {llm_model}...")
        
        try:
            start_time = time.time()
            response = llm_client.chat.completions.create(
                model=llm_model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ–‡ç« ç›¸å…³æ€§åˆ¤æ–­ä¸“å®¶ï¼Œæ“…é•¿åˆ†ææ–‡ç« æ ‡é¢˜ä¸æœç´¢ä¸»é¢˜çš„ç›¸å…³æ€§ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=2000
            )
            elapsed = time.time() - start_time
            
            result_text = response.choices[0].message.content.strip()
            print(f"    [LLM] æ¨¡å‹å“åº”æ—¶é—´: {elapsed:.2f}ç§’")
            print(f"    [LLM] å“åº”é•¿åº¦: {len(result_text)} å­—ç¬¦")
            
            # è§£æ JSON ç»“æœ
            try:
                # å°è¯•æå– JSON
                if '```json' in result_text:
                    result_text = result_text.split('```json')[1].split('```')[0]
                elif '```' in result_text:
                    result_text = result_text.split('```')[1].split('```')[0]
                
                result_json = json.loads(result_text)
                results = result_json.get('results', [])
                
                # æ˜¾ç¤ºæ¯ä¸ªç»“æœçš„åˆ¤æ–­
                print(f"    [LLM] æ¨¡å‹åˆ¤æ–­ç»“æœ:")
                relevant_articles = []
                for i, result in enumerate(results):
                    if i < len(article_metadata):
                        title_short = decoded_titles[i][:40] if i < len(decoded_titles) else "Unknown"
                        is_relevant = result.get('relevant', False)
                        reason = result.get('reason', '')[:30]
                        status = "âœ“ ç›¸å…³" if is_relevant else "âœ— ä¸ç›¸å…³"
                        print(f"      [{i+1}] {status} - {title_short}")
                        if is_relevant:
                            relevant_articles.append(article_metadata[i])
                
                return relevant_articles
                
            except json.JSONDecodeError as e:
                # è§£æå¤±è´¥æ—¶è¿”å›æ‰€æœ‰æ–‡ç« 
                print(f"    [è­¦å‘Š] è§£æLLMå“åº”å¤±è´¥: {e}")
                print(f"    [è­¦å‘Š] å“åº”å†…å®¹: {result_text[:200]}...")
                return article_metadata
                
        except Exception as e:
            print(f"    [é”™è¯¯] LLM è°ƒç”¨å¤±è´¥: {e}")
            # API å¤±è´¥æ—¶è¿”å›æ‰€æœ‰æ–‡ç« 
            return article_metadata
    
    async def _prompt_user_to_select_articles(self, article_metadata: List[Dict], max_download: int = 20) -> List[Dict]:
        """
        è®©ç”¨æˆ·é€‰æ‹©è¦ä¸‹è½½çš„æ–‡ç« 
        
        Args:
            article_metadata: æ–‡ç« å…ƒæ•°æ®åˆ—è¡¨
            max_download: å»ºè®®çš„æœ€å¤§ä¸‹è½½æ•°é‡ï¼ˆæœåŠ¡å™¨é™åˆ¶ï¼‰
            
        Returns:
            ç”¨æˆ·é€‰æ‹©çš„æ–‡ç« å…ƒæ•°æ®åˆ—è¡¨
        """
        print("\n" + "=" * 60)
        print("[æ–‡ç« é€‰æ‹©]")
        print("=" * 60)
        
        total = len(article_metadata)
        print(f"\nå…±æå–åˆ° {total} ç¯‡æ–‡ç« ã€‚")
        print(f"\nâš ï¸  è­¦å‘Š: å»ºè®®å•æ¬¡ä¸‹è½½ä¸è¶…è¿‡ {max_download} ç¯‡ï¼Œé˜²æ­¢æœåŠ¡å™¨é™åˆ¶ã€‚")
        
        # æ˜¾ç¤ºæ–‡ç« åˆ—è¡¨ï¼ˆå¸¦ç¼–å·ï¼‰
        print("\næ–‡ç« åˆ—è¡¨:")
        print("-" * 60)
        
        for i, art in enumerate(article_metadata, 1):
            title = art.get('title', 'N/A')[:60]
            size = art.get('size', 0)
            docref = art.get('docref', 'N/A')
            print(f"  {i:3}. [{size:>6} bytes] {title}")
        
        print("-" * 60)
        
        # è®©ç”¨æˆ·è¾“å…¥é€‰æ‹©
        print(f"\nè¯·é€‰æ‹©è¦ä¸‹è½½çš„æ–‡ç« :")
        print("  - è¾“å…¥æ•°å­—ç¼–å· (ä¾‹å¦‚: 1,5,10)")
        print("  - è¾“å…¥èŒƒå›´ (ä¾‹å¦‚: 1-10)")
        print("  - è¾“å…¥ 'all' ä¸‹è½½å…¨éƒ¨ (âš ï¸ å¯èƒ½è§¦å‘æœåŠ¡å™¨é™åˆ¶)")
        print("  - è¾“å…¥ 'first N' ä¸‹è½½å‰Nç¯‡ (ä¾‹å¦‚: first 10)")
        print("  - è¾“å…¥ 'last N' ä¸‹è½½åNç¯‡")
        print("  - è¾“å…¥ 'cancel' å–æ¶ˆä¸‹è½½")
        
        while True:
            user_input = input("\nè¯·è¾“å…¥é€‰æ‹©: ").strip().lower()
            
            if user_input == 'cancel':
                print("  å·²å–æ¶ˆä¸‹è½½")
                return []
            
            if user_input == 'all':
                selected = article_metadata
                print(f"\n  âš ï¸  é€‰æ‹©äº†å…¨éƒ¨ {len(selected)} ç¯‡æ–‡ç« ")
                if len(selected) > max_download:
                    confirm = input(f"  è¶…è¿‡å»ºè®®æ•°é‡ ({max_download})ï¼Œæ˜¯å¦ç»§ç»­? (y/n): ").strip().lower()
                    if confirm != 'y':
                        continue
                return selected
            
            # å¤„ç† "first N" æˆ– "last N"
            if user_input.startswith('first '):
                try:
                    n = int(user_input.split()[1])
                    selected = article_metadata[:n]
                    print(f"\n  å·²é€‰æ‹©å‰ {len(selected)} ç¯‡æ–‡ç« ")
                    return selected
                except:
                    print("  è¾“å…¥æ ¼å¼é”™è¯¯")
                    continue
            
            if user_input.startswith('last '):
                try:
                    n = int(user_input.split()[1])
                    selected = article_metadata[-n:]
                    print(f"\n  å·²é€‰æ‹©å {len(selected)} ç¯‡æ–‡ç« ")
                    return selected
                except:
                    print("  è¾“å…¥æ ¼å¼é”™è¯¯")
                    continue
            
            # å¤„ç†èŒƒå›´ (ä¾‹å¦‚: 1-10)
            if '-' in user_input:
                try:
                    parts = user_input.split('-')
                    start = int(parts[0].strip())
                    end = int(parts[1].strip())
                    if 1 <= start <= end <= total:
                        selected = article_metadata[start-1:end]
                        print(f"\n  å·²é€‰æ‹©ç¬¬ {start} åˆ° {end} ç¯‡ï¼Œå…± {len(selected)} ç¯‡")
                        return selected
                    else:
                        print(f"  èŒƒå›´æ— æ•ˆ (1-{total})")
                except:
                    print("  è¾“å…¥æ ¼å¼é”™è¯¯")
                    continue
            
            # å¤„ç†å•ä¸ªæˆ–å¤šä¸ªæ•°å­— (ä¾‹å¦‚: 1,5,10)
            if ',' in user_input or user_input.replace(',', '').replace(' ', '').isdigit():
                try:
                    # è§£æè¾“å…¥
                    nums = []
                    for part in user_input.replace(',', ' ').split():
                        if part.isdigit():
                            nums.append(int(part))
                    
                    if nums:
                        # éªŒè¯èŒƒå›´
                        valid = all(1 <= n <= total for n in nums)
                        if valid:
                            selected = [article_metadata[n-1] for n in nums]
                            print(f"\n  å·²é€‰æ‹© {len(selected)} ç¯‡æ–‡ç« : {nums}")
                            return selected
                        else:
                            print(f"  æ•°å­—èŒƒå›´æ— æ•ˆ (1-{total})")
                    else:
                        print("  è¾“å…¥æ ¼å¼é”™è¯¯")
                except:
                    print("  è¾“å…¥æ ¼å¼é”™è¯¯")
                    continue
            
            print("  è¾“å…¥æ— æ•ˆï¼Œè¯·é‡è¯•")
    
    def _load_article_metadata_from_json(self, json_path: Path) -> Optional[List[Dict]]:
        """
        ä»JSONæ–‡ä»¶åŠ è½½æ–‡ç« å…ƒæ•°æ®
        
        Args:
            json_path: JSONæ–‡ä»¶è·¯å¾„
            
        Returns:
            æ–‡ç« å…ƒæ•°æ®åˆ—è¡¨ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            articles = data.get('articles', [])
            print(f"  [åŠ è½½] ä» {json_path.name} åŠ è½½äº† {len(articles)} ç¯‡æ–‡ç« å…ƒæ•°æ®")
            return articles
            
        except Exception as e:
            print(f"  [é”™è¯¯] åŠ è½½JSONå¤±è´¥: {e}")
            return None
    
    def _build_download_payload(self, 
                                page_num: int = 1, 
                                article_ids: Optional[List[str]] = None,
                                instance_id: Optional[str] = None,
                                p_param: str = "AWGLNB") -> Dict[str, str]:
        """
        æ„å»ºä¸‹è½½APIçš„payload
        
        åŸºäºnewsbank-api-download.txtä¸­çš„å‚æ•°ç»“æ„
        """
        # åŸºç¡€payload
        # maxresults=20 å¯ä»¥ä¸€æ¬¡è·å–æœ€å¤š20ç¯‡æ–‡ç« çš„å®Œæ•´å†…å®¹ï¼Œä¸èƒ½è®¾ç½®å¾—å¤ªå¤§ï¼Œé˜²æ­¢é™æµ
        payload = {
            "page": str(page_num),
            "load_pager": "true",
            "p": p_param,
            "action": "download",
            "label": "Multidocs Display pane",
            "maxresults": "20",  # æ”¹ä¸º20ï¼Œç³»ç»Ÿçš„é¢„è®¾å€¼ã€‚è¿™ä¸ªæ•°å­—æ”¾çš„è¿‡å¤§å¯èƒ½è§¦å‘æœåŠ¡å™¨é™åˆ¶
            "pdf_enabled": "true",
            "pdf_label": "Download PDF",
            "pdf_path": "multidocs",
            "pdf_filename": "NewsBank Multiple Articles.pdf",
            "zustat_category_override": "co_sc_pdf_download"
        }
        
        # æ·»åŠ instance_idï¼ˆå¦‚æœæœ‰ï¼‰
        if instance_id:
            payload["instance_id"] = instance_id
        
        # æ„å»ºpdf_paramsï¼ˆåŒ…å«æ–‡ç« IDï¼‰
        pdf_params_parts = [
            "action=pdf",
            "format=pdf",
            "pdf_enabled=false",
            "load_pager=false",
            "maxresults=100"
        ]
        
        # å¦‚æœæœ‰æ–‡ç« IDï¼Œæ·»åŠ åˆ°pdf_params
        if article_ids:
            # æ„å»ºæ–‡æ¡£åˆ—è¡¨å‚æ•°
            docs_param = "&".join([f"doc={quote(doc_id)}" for doc_id in article_ids[:100]])
            pdf_params_parts.append(docs_param)
        
        payload["pdf_params"] = "&".join(pdf_params_parts)
        
        return payload
    
    async def select_all_articles(self, page: Page) -> bool:
        """
        é€‰ä¸­é¡µé¢ä¸Šçš„æ‰€æœ‰æ–‡ç« 
        
        NewsBanké€šå¸¸æœ‰å¤é€‰æ¡†æˆ–"Select All"æŒ‰é’®æ¥é€‰æ‹©æ–‡ç« 
        
        Returns:
            æ˜¯å¦æˆåŠŸé€‰æ‹©
        """
        print("\n[é€‰æ‹©æ‰€æœ‰æ–‡ç« ]")
        print("-" * 40)
        
        try:
            # æ–¹æ³•1: æŸ¥æ‰¾"Select All"æˆ–"å…¨é€‰"æŒ‰é’®/é“¾æ¥
            # æ ¹æ®å®é™…HTMLç»“æ„ï¼š
            # <input class="search-hits__select-all form-checkbox" title="Select all articles on this page." id="search-hits__select-all" type="checkbox" value="1" aria-label="Select All Articles">
            select_all_selectors = [
                '#search-hits__select-all',  # ç²¾ç¡®IDé€‰æ‹©å™¨ï¼ˆæœ€å¯é ï¼‰
                'input.search-hits__select-all',  # classé€‰æ‹©å™¨
                'input[title*="Select all articles"]',  # titleå±æ€§
                'input[aria-label="Select All Articles"]',  # aria-label
            ]
            
            for selector in select_all_selectors:
                try:
                    print(f"  [å°è¯•] æŸ¥æ‰¾å…¨é€‰å¤é€‰æ¡†: {selector}")
                    
                    # ç­‰å¾…å…ƒç´ å‡ºç°
                    try:
                        await page.wait_for_selector(selector, timeout=5000)
                    except:
                        print(f"    ç­‰å¾…è¶…æ—¶ï¼Œå°è¯•ç›´æ¥æŸ¥æ‰¾")
                    
                    # æŸ¥æ‰¾å…ƒç´ 
                    select_all_elem = await page.query_selector(selector)
                    
                    if select_all_elem:
                        print(f"    æ‰¾åˆ°å…ƒç´ ï¼Œå‡†å¤‡æ»šåŠ¨åˆ°è§†å›¾...")
                        
                        # å¼ºåˆ¶æ»šåŠ¨åˆ°é¡µé¢é¡¶éƒ¨ï¼Œç¡®ä¿å¤é€‰æ¡†å¯è§
                        await page.evaluate("window.scrollTo(0, 0)")
                        await asyncio.sleep(0.5)
                        
                        # æ»šåŠ¨åˆ°å…ƒç´ 
                        await select_all_elem.scroll_into_view_if_needed()
                        await asyncio.sleep(0.5)
                        
                        # æ£€æŸ¥å…ƒç´ çŠ¶æ€
                        is_visible = await select_all_elem.is_visible()
                        box = await select_all_elem.bounding_box()
                        print(f"    å…ƒç´ çŠ¶æ€ - visible: {is_visible}, box: {box}")
                        
                        if is_visible and box:
                            # è·å–å½“å‰é€‰ä¸­çŠ¶æ€
                            is_checked = await select_all_elem.is_checked()
                            print(f"    å½“å‰é€‰ä¸­çŠ¶æ€: {is_checked}")
                            
                            # å¼ºåˆ¶è§¦å‘ç½‘ç»œè¯·æ±‚ï¼šæ— è®ºæ˜¯å¦é€‰ä¸­ï¼Œéƒ½è¦ç‚¹å‡»
                            # å¦‚æœå·²é€‰ä¸­ï¼Œå…ˆå–æ¶ˆå†é€‰ä¸­ï¼›å¦‚æœæœªé€‰ä¸­ï¼Œç›´æ¥é€‰ä¸­
                            if is_checked:
                                print(f"    [å¼ºåˆ¶] å¤é€‰æ¡†å·²é€‰ä¸­ï¼Œå…ˆå–æ¶ˆå†é€‰ä¸­ä»¥è§¦å‘ç½‘ç»œè¯·æ±‚...")
                                # å–æ¶ˆé€‰ä¸­
                                try:
                                    await select_all_elem.click(force=True)
                                    print(f"    [å–æ¶ˆ] å·²å–æ¶ˆé€‰ä¸­")
                                    await asyncio.sleep(0.5)
                                except Exception as e:
                                    print(f"    [è­¦å‘Š] å–æ¶ˆå¤±è´¥: {e}")
                            
                            print(f"    å‡†å¤‡ç‚¹å‡»å¤é€‰æ¡†...")
                            
                            # æ–¹æ³•1: ä½¿ç”¨é¼ æ ‡æ¨¡æ‹Ÿç‚¹å‡»ï¼ˆæ›´çœŸå®ï¼‰
                            click_success = False
                            try:
                                box = await select_all_elem.bounding_box()
                                if box:
                                    # è®¡ç®—å…ƒç´ ä¸­å¿ƒç‚¹
                                    x = box['x'] + box['width'] / 2
                                    y = box['y'] + box['height'] / 2
                                    print(f"    ä½¿ç”¨é¼ æ ‡ç‚¹å‡»åæ ‡: ({x}, {y})")
                                    await page.mouse.click(x, y)
                                    print(f"    [æˆåŠŸ] ä½¿ç”¨ mouse.click() ç‚¹å‡»")
                                    click_success = True
                            except Exception as mouse_err:
                                print(f"    [è­¦å‘Š] é¼ æ ‡ç‚¹å‡»å¤±è´¥: {mouse_err}")
                            
                            # æ–¹æ³•2: ç›´æ¥ç‚¹å‡»
                            if not click_success:
                                try:
                                    await select_all_elem.click(force=True)
                                    print(f"    [æˆåŠŸ] ä½¿ç”¨ click(force=True) ç‚¹å‡»")
                                    click_success = True
                                except Exception as click_err:
                                    print(f"    [è­¦å‘Š] ç›´æ¥ç‚¹å‡»å¤±è´¥: {click_err}")
                            
                            # æ–¹æ³•3: JavaScriptç‚¹å‡»
                            if not click_success:
                                try:
                                    await page.evaluate(f"""
                                        () => {{
                                            const cb = document.querySelector('{selector}');
                                            if (cb) {{
                                                cb.click();
                                                return 'clicked';
                                            }}
                                            return 'not found';
                                        }}
                                    """)
                                    print(f"    [æˆåŠŸ] ä½¿ç”¨é¡µé¢çº§ JavaScript ç‚¹å‡»")
                                    click_success = True
                                except Exception as js_err:
                                    print(f"    [è­¦å‘Š] JavaScriptç‚¹å‡»ä¹Ÿå¤±è´¥: {js_err}")
                            
                            # ç­‰å¾…UIæ›´æ–°
                            await asyncio.sleep(2)
                            
                            # éªŒè¯æ˜¯å¦é€‰ä¸­
                            is_checked_after = await select_all_elem.is_checked()
                            print(f"    ç‚¹å‡»åé€‰ä¸­çŠ¶æ€: {is_checked_after}")
                            
                            # ä¹Ÿæ£€æŸ¥é¡µé¢ä¸Šçš„é€‰ä¸­è®¡æ•°
                            selection_text = await page.evaluate("""
                                () => {
                                    const el = document.querySelector('.search-hits__selections--feedback');
                                    return el ? el.textContent : 'not found';
                                }
                            """)
                            print(f"    é¡µé¢é€‰ä¸­è®¡æ•°: {selection_text}")
                            
                            if is_checked_after or '100' in selection_text:
                                print(f"  [æˆåŠŸ] å…¨é€‰å¤é€‰æ¡†å·²é€‰ä¸­")
                                return True
                            else:
                                print(f"  [è­¦å‘Š] ç‚¹å‡»åä»æœªé€‰ä¸­ï¼Œå°è¯•å¼ºåˆ¶è®¾ç½®å±æ€§")
                                # å¼ºåˆ¶è®¾ç½®checkedå±æ€§å¹¶è§¦å‘äº‹ä»¶
                                await page.evaluate("""
                                    () => {
                                        const cb = document.getElementById('search-hits__select-all');
                                        if (cb) {
                                            cb.checked = true;
                                            cb.dispatchEvent(new Event('change', { bubbles: true }));
                                            cb.dispatchEvent(new Event('click', { bubbles: true }));
                                            
                                            // åŒæ—¶å‹¾é€‰æ‰€æœ‰æ–‡ç« å¤é€‰æ¡†
                                            const articleCheckboxes = document.querySelectorAll('article.search-hits__hit input[type="checkbox"]');
                                            articleCheckboxes.forEach(box => {
                                                if (!box.checked) {
                                                    box.checked = true;
                                                    box.dispatchEvent(new Event('change', { bubbles: true }));
                                                }
                                            });
                                            return `checked ${articleCheckboxes.length} boxes`;
                                        }
                                        return 'checkbox not found';
                                    }
                                """)
                                await asyncio.sleep(1)
                                return True
                    else:
                        print(f"    æœªæ‰¾åˆ°å…ƒç´ ")
                        
                except Exception as e:
                    print(f"    é€‰æ‹©å™¨å¤±è´¥: {str(e)[:100]}")
                    continue
            
            # æ–¹æ³•2: æŸ¥æ‰¾å¹¶ç‚¹å‡»æ¯ä¸ªæ–‡ç« çš„å¤é€‰æ¡†
            print("  [ä¿¡æ¯] æœªæ‰¾åˆ°å…¨é€‰æŒ‰é’®ï¼Œå°è¯•é€ä¸ªé€‰æ‹©æ–‡ç« ...")
            
            checkbox_selectors = [
                'article.search-hits__hit input[type="checkbox"]',
                '.search-hits__hit input[type="checkbox"]',
                'input[class*="select"]',
            ]
            
            total_checked = 0
            for selector in checkbox_selectors:
                try:
                    checkboxes = await page.query_selector_all(selector)
                    if checkboxes:
                        print(f"  [æ‰¾åˆ°] {len(checkboxes)} ä¸ªå¤é€‰æ¡† ({selector})")
                        
                        for i, checkbox in enumerate(checkboxes):
                            try:
                                is_visible = await checkbox.is_visible()
                                if is_visible:
                                    is_checked = await checkbox.is_checked()
                                    if not is_checked:
                                        await checkbox.scroll_into_view_if_needed()
                                        await checkbox.click()
                                        total_checked += 1
                                        
                                        # æ¯10ä¸ªå»¶è¿Ÿä¸€ä¸‹
                                        if total_checked % 10 == 0:
                                            print(f"    å·²é€‰ä¸­ {total_checked} ä¸ª...")
                                            await asyncio.sleep(0.5)
                            except Exception as e:
                                continue
                        
                        if total_checked > 0:
                            print(f"  [æˆåŠŸ] é€‰ä¸­äº† {total_checked} ç¯‡æ–‡ç« ")
                            await asyncio.sleep(1)
                            return True
                        
                except Exception as e:
                    continue
            
            # æ–¹æ³•3: å¼ºåˆ¶ä½¿ç”¨JavaScripté€‰æ‹©
            print("  [ä¿¡æ¯] å°è¯•å¼ºåˆ¶ä½¿ç”¨JavaScripté€‰æ‹©æ‰€æœ‰æ–‡ç« ...")
            try:
                result = await page.evaluate("""() => {
                    // æ‰¾åˆ°å…¨é€‰å¤é€‰æ¡†
                    const selectAllCheckbox = document.querySelector('#search-hits__select-all');
                    if (selectAllCheckbox && !selectAllCheckbox.checked) {
                        selectAllCheckbox.checked = true;
                        selectAllCheckbox.dispatchEvent(new Event('change', { bubbles: true }));
                        selectAllCheckbox.dispatchEvent(new Event('click', { bubbles: true }));
                        return { method: 'select_all_checkbox', checked: 1 };
                    }
                    
                    // å¤‡é€‰ï¼šå‹¾é€‰æ‰€æœ‰æ–‡ç« å¤é€‰æ¡†
                    const checkboxes = document.querySelectorAll('article.search-hits__hit input[type="checkbox"], .search-hits__hit input[type="checkbox"]');
                    let checked = 0;
                    checkboxes.forEach(cb => {
                        if (!cb.checked && cb.offsetParent !== null) {
                            cb.checked = true;
                            cb.dispatchEvent(new Event('change', { bubbles: true }));
                            checked++;
                        }
                    });
                    return { method: 'individual_checkboxes', checked: checked };
                }""")
                
                if result and result.checked > 0:
                    print(f"  [æˆåŠŸ] é€šè¿‡JSé€‰ä¸­äº† {result.checked} ä¸ªå¤é€‰æ¡† (æ–¹æ³•: {result.method})")
                    await asyncio.sleep(1)
                    return True
                else:
                    print("  [è­¦å‘Š] JSé€‰æ‹©æœªæ‰¾åˆ°å¯é€‰æ‹©çš„å¤é€‰æ¡†")
                    
            except Exception as e:
                print(f"  [è­¦å‘Š] JSé€‰æ‹©å¤±è´¥: {e}")
            
            print("  [è­¦å‘Š] æœªèƒ½é€‰æ‹©æ–‡ç« ï¼Œä¸‹è½½æŒ‰é’®å¯èƒ½ä¸ä¼šè¢«æ¿€æ´»")
            return False
            
        except Exception as e:
            print(f"  [é”™è¯¯] é€‰æ‹©æ–‡ç« æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    async def click_download_button_and_get_response(self, page: Page) -> Optional[Dict]:
        """
        ç‚¹å‡»é¡µé¢ä¸Šçš„Download PDFæŒ‰é’®å¹¶æ•è·å“åº”
        
        æµç¨‹ï¼š
        1. å…ˆé€‰æ‹©æ‰€æœ‰æ–‡ç« 
        2. ç‚¹å‡»Download PDFæŒ‰é’®
        3. æ•è·APIå“åº”
        
        Returns:
            APIå“åº”æ•°æ®
        """
        print("\n[æ¨¡æ‹Ÿç‚¹å‡»ä¸‹è½½æŒ‰é’®]")
        print("-" * 40)
        
        # ç¬¬1æ­¥ï¼šå…ˆé€‰æ‹©æ‰€æœ‰æ–‡ç« ï¼ˆå…³é”®æ­¥éª¤ï¼ï¼‰
        select_success = await self.select_all_articles(page)
        if not select_success:
            print("  [è­¦å‘Š] é€‰æ‹©æ–‡ç« å¤±è´¥ï¼Œä¸‹è½½æŒ‰é’®å¯èƒ½æ— æ³•ç‚¹å‡»")
        
        # ç­‰å¾…ä¸€ä¸‹ç¡®ä¿UIæ›´æ–°
        await asyncio.sleep(2)
        
        # è°ƒè¯•ï¼šæˆªå›¾æŸ¥çœ‹å½“å‰é¡µé¢çŠ¶æ€
        debug_screenshot = self.output_dir / "debug_before_click.png"
        try:
            await page.screenshot(path=str(debug_screenshot), full_page=True)
            print(f"  [è°ƒè¯•] å·²ä¿å­˜é¡µé¢æˆªå›¾: {debug_screenshot}")
        except Exception as e:
            print(f"  [è°ƒè¯•] æˆªå›¾å¤±è´¥: {e}")
        
        try:
            # è®¾ç½®å“åº”ç›‘å¬ - ä½¿ç”¨åˆ—è¡¨æ¥å­˜å‚¨æ•è·çš„æ•°æ®
            captured_responses = []
            captured_requests = []
            
            async def handle_response(response):
                url = response.url
                if "nb-multidocs/get" in url:
                    print(f"  [æ•è·] å“åº”: {response.request.method} {url[:60]}... status={response.status}")
                    if response.request.method == "POST":
                        try:
                            body = await response.body()
                            captured_responses.append({
                                "url": url,
                                "status": response.status,
                                "headers": dict(response.headers),
                                "body": body.decode('utf-8', errors='replace')
                            })
                            print(f"  [æˆåŠŸ] å·²æ•è·POSTå“åº”ï¼Œbodyé•¿åº¦: {len(body)} bytes")
                        except Exception as e:
                            print(f"  [è­¦å‘Š] è¯»å–å“åº”å¤±è´¥: {e}")
            
            async def handle_request(request):
                if "nb-multidocs/get" in request.url:
                    print(f"  [æ•è·] è¯·æ±‚: {request.method} {request.url[:60]}...")
                    captured_requests.append(request.url)
            
            # ç›‘å¬å“åº”å’Œè¯·æ±‚
            page.on("response", lambda response: asyncio.create_task(handle_response(response)))
            page.on("request", lambda request: asyncio.create_task(handle_request(request)))
            print("  [è°ƒè¯•] å·²è®¾ç½®è¯·æ±‚/å“åº”ç›‘å¬å™¨")
            
            # æŸ¥æ‰¾å¹¶ç‚¹å‡»Download PDFæŒ‰é’®
            # NewsBankç‰¹å®šé€‰æ‹©å™¨ - åŸºäºå®é™…é¡µé¢ç»“æ„
            download_selectors = [
                # NewsBankç‰¹å®šçš„å·¥å…·æ æŒ‰é’®
                '[data-testid="download-button"]',
                '[data-testid="download-pdf"]',
                '.toolbar-download',
                '.toolbar-download-button',
                '.action-download',
                '.multidoc-download',
                
                # é€šç”¨æ–‡æœ¬åŒ¹é…
                'button:has-text("Download PDF")',
                'button:has-text("Download")',
                'a:has-text("Download PDF")',
                'a:has-text("Download")',
                
                # ç±»ååŒ¹é…
                '[class*="download"]:visible',
                '[class*="toolbar"] [class*="download"]',
                '[class*="action"] [class*="download"]',
                
                # SVGå›¾æ ‡é™„è¿‘çš„æŒ‰é’®
                'button:has(svg)',
                'a:has(svg)',
                
                # æŒ‰é’®å’Œé“¾æ¥çš„ä¸€èˆ¬åŒ¹é…
                'button[title*="download" i]',
                'a[title*="download" i]',
                'button[aria-label*="download" i]',
                'a[aria-label*="download" i]',
            ]
            
            download_button = None
            found_selector = None
            
            print("  [è°ƒè¯•] å¼€å§‹æŸ¥æ‰¾ä¸‹è½½æŒ‰é’®...")
            for selector in download_selectors:
                try:
                    print(f"    å°è¯•é€‰æ‹©å™¨: {selector}")
                    # å…ˆæ£€æŸ¥å…ƒç´ æ˜¯å¦å­˜åœ¨ï¼ˆä¸ç­‰å¾…ï¼‰
                    button = await page.query_selector(selector)
                    if button:
                        # æ»šåŠ¨åˆ°å…ƒç´ ä½ç½®
                        await button.scroll_into_view_if_needed()
                        await asyncio.sleep(0.5)  # ç­‰å¾…æ»šåŠ¨å®Œæˆ
                        
                        is_visible = await button.is_visible()
                        is_enabled = await button.is_enabled()
                        box = await button.bounding_box()
                        print(f"    æ‰¾åˆ°å…ƒç´  - visible: {is_visible}, enabled: {is_enabled}, box: {box}")
                        if is_visible:
                            download_button = button
                            found_selector = selector
                            print(f"  [æ‰¾åˆ°] ä¸‹è½½æŒ‰é’®: {selector}")
                            break
                except Exception as e:
                    print(f"    é€‰æ‹©å™¨å¤±è´¥: {str(e)[:80]}")
                    continue
            
            if not download_button:
                print("  [è°ƒè¯•] æ‰€æœ‰é€‰æ‹©å™¨éƒ½æœªæ‰¾åˆ°ï¼Œå°è¯•æœç´¢é¡µé¢æ‰€æœ‰æŒ‰é’®...")
                # è°ƒè¯•ï¼šè·å–é¡µé¢æ‰€æœ‰æŒ‰é’®çš„æ–‡æœ¬å’Œä½ç½®ä¿¡æ¯
                buttons_info = await page.evaluate("""() => {
                    const buttons = document.querySelectorAll('button, a[role="button"], input[type="submit"], a.btn, a[class*="button"]');
                    return Array.from(buttons).map(b => {
                        const rect = b.getBoundingClientRect();
                        return {
                            tag: b.tagName,
                            text: b.textContent?.trim()?.substring(0, 80) || '',
                            class: b.className,
                            id: b.id,
                            type: b.type,
                            visible: b.offsetParent !== null && rect.width > 0 && rect.height > 0,
                            href: b.href || '',
                            rect: {top: rect.top, left: rect.left, width: rect.width, height: rect.height}
                        };
                    }).filter(b => b.visible);  // åªè¿”å›å¯è§çš„æŒ‰é’®
                }""")
                print(f"  [è°ƒè¯•] é¡µé¢ä¸Šçš„å¯è§æŒ‰é’® ({len(buttons_info)}ä¸ª):")
                for btn in buttons_info[:15]:  # æ˜¾ç¤ºå‰15ä¸ª
                    class_str = btn['class'][:40] if btn['class'] else 'none'
                    print(f"    - {btn['tag']}: '{btn['text']}' (class={class_str}, id={btn['id'] or 'none'}, href={btn['href'][:50] if btn['href'] else 'none'})")
                
                # ä¹Ÿæ£€æŸ¥æ˜¯å¦æœ‰åŒ…å«ç‰¹å®šå…³é”®è¯çš„æ–‡æœ¬
                print("  [è°ƒè¯•] æœç´¢åŒ…å« 'Download' æˆ– 'PDF' çš„å…ƒç´ ...")
                download_elements = await page.evaluate("""() => {
                    const allElements = document.querySelectorAll('*');
                    const results = [];
                    for (const el of allElements) {
                        const text = el.textContent?.toLowerCase() || '';
                        if ((text.includes('download') || text.includes('pdf')) && el.offsetParent !== null) {
                            const rect = el.getBoundingClientRect();
                            if (rect.width > 50 && rect.height > 20) {
                                results.push({
                                    tag: el.tagName,
                                    text: el.textContent?.trim()?.substring(0, 60) || '',
                                    class: el.className,
                                    id: el.id
                                });
                            }
                        }
                    }
                    return results.slice(0, 10);
                }""")
                if download_elements.length > 0:
                    print(f"  [è°ƒè¯•] æ‰¾åˆ° {download_elements.length} ä¸ªå¯èƒ½ç›¸å…³çš„å…ƒç´ :")
                    for el in download_elements:
                        print(f"    - {el.tag}: '{el.text}'")
                
                print("  [è­¦å‘Š] æœªæ‰¾åˆ°ä¸‹è½½æŒ‰é’®ï¼Œå°è¯•ç›´æ¥è°ƒç”¨API")
                return None
            
            # ç‚¹å‡»æŒ‰é’®å‰å†æ¬¡æˆªå›¾
            debug_screenshot2 = self.output_dir / "debug_before_click2.png"
            try:
                await page.screenshot(path=str(debug_screenshot2), full_page=True)
                print(f"  [è°ƒè¯•] å·²ä¿å­˜ç‚¹å‡»å‰æˆªå›¾: {debug_screenshot2}")
            except:
                pass
            
            # ç‚¹å‡»æŒ‰é’®
            print(f"  [æ“ä½œ] ç‚¹å‡»ä¸‹è½½æŒ‰é’® (é€‰æ‹©å™¨: {found_selector})...")
            
            # å°è¯•å¤šç§ç‚¹å‡»æ–¹å¼ï¼Œä»æœ€çœŸå®çš„é¼ æ ‡ç‚¹å‡»å¼€å§‹
            click_success = False
            
            # æ–¹æ³•1: é¼ æ ‡æ¨¡æ‹Ÿç‚¹å‡»ï¼ˆæœ€çœŸå®ï¼‰
            try:
                box = await download_button.bounding_box()
                if box:
                    x = box['x'] + box['width'] / 2
                    y = box['y'] + box['height'] / 2
                    print(f"  [å°è¯•] ä½¿ç”¨é¼ æ ‡ç‚¹å‡»åæ ‡: ({x}, {y})")
                    await page.mouse.click(x, y)
                    print("  [æˆåŠŸ] ä½¿ç”¨ mouse.click() ç‚¹å‡»")
                    click_success = True
            except Exception as mouse_err:
                print(f"  [è­¦å‘Š] é¼ æ ‡ç‚¹å‡»å¤±è´¥: {mouse_err}")
            
            # æ–¹æ³•2: æ ‡å‡†ç‚¹å‡»
            if not click_success:
                try:
                    await download_button.click(force=True)
                    print("  [æˆåŠŸ] ä½¿ç”¨ click(force=True) ç‚¹å‡»")
                    click_success = True
                except Exception as click_err:
                    print(f"  [è­¦å‘Š] æ ‡å‡†ç‚¹å‡»å¤±è´¥: {click_err}")
            
            # æ–¹æ³•3: JavaScriptç‚¹å‡»
            if not click_success:
                try:
                    await download_button.evaluate("element => element.click()")
                    print("  [æˆåŠŸ] ä½¿ç”¨JavaScriptç‚¹å‡»")
                    click_success = True
                except Exception as js_err:
                    print(f"  [é”™è¯¯] JavaScriptç‚¹å‡»ä¹Ÿå¤±è´¥: {js_err}")
                    return None
            
            # ç­‰å¾…å“åº” - å¢åŠ ç­‰å¾…æ—¶é—´
            print("  [ç­‰å¾…] ç­‰å¾…APIå“åº”...")
            response_data = None
            for i in range(10):  # æœ€å¤šç­‰å¾…10ç§’
                await asyncio.sleep(1)
                if captured_responses:
                    response_data = captured_responses[-1]  # ä½¿ç”¨æœ€åä¸€ä¸ªå“åº”
                    print(f"  [æˆåŠŸ] æ•è·åˆ°å“åº” (çŠ¶æ€: {response_data['status']})")
                    break
                if captured_requests and i >= 3:
                    print(f"  [ä¿¡æ¯] å·²æ•è·è¯·æ±‚ä½†æœªæ”¶åˆ°å“åº”ï¼Œç»§ç»­ç­‰å¾…...")
            
            if response_data:
                # ä¿å­˜å“åº”å†…å®¹ç”¨äºè°ƒè¯•
                debug_response = self.output_dir / "debug_api_response.html"
                try:
                    with open(debug_response, 'w', encoding='utf-8') as f:
                        f.write(response_data['body'][:5000])  # ä¿å­˜å‰5000å­—ç¬¦
                    print(f"  [è°ƒè¯•] å·²ä¿å­˜å“åº”å†…å®¹: {debug_response}")
                except:
                    pass
                return response_data
            else:
                print("  [è­¦å‘Š] æœªæ•è·åˆ°APIå“åº”")
                if captured_requests:
                    print(f"  [è­¦å‘Š] è¯·æ±‚å·²å‘å‡º ({len(captured_requests)}ä¸ª) ä½†æ²¡æœ‰æ•è·åˆ°å“åº”æ•°æ®")
                return None
                
        except Exception as e:
            print(f"  [é”™è¯¯] ç‚¹å‡»ä¸‹è½½æŒ‰é’®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    async def call_download_api_directly(self, page: Page, article_ids: List[str], p_param: str = "AWGLNB") -> Optional[Dict]:
        """
        ç›´æ¥è°ƒç”¨ nb-multidocs/get API
        
        è¿™æ˜¯æ›´å¯é çš„æ–¹å¼ï¼Œå¯ä»¥ç›´æ¥æ§åˆ¶payloadå¹¶è·å–å“åº”
        """
        print("\n[ç›´æ¥è°ƒç”¨API]")
        print("-" * 40)
        
        try:
            # ä½¿ç”¨_build_download_payloadæ–¹æ³•æ­£ç¡®æ„å»ºpayloadï¼ŒåŒ…å«article_ids
            payload = self._build_download_payload(
                page_num=1,
                article_ids=article_ids,
                p_param=p_param
            )
            
            # å°†payloadè½¬æ¢ä¸ºURLç¼–ç çš„form dataå­—ç¬¦ä¸²
            form_parts = []
            for key, value in payload.items():
                if isinstance(value, str):
                    form_parts.append(f"{key}={quote(value)}")
                else:
                    form_parts.append(f"{key}={value}")
            form_data_str = "&".join(form_parts)
            
            print(f"  [è¯·æ±‚] POST {self.api_endpoint}")
            print(f"  [æ–‡ç« IDæ•°é‡] {len(article_ids) if article_ids else 0}")
            print(f"  [å‚æ•°] {form_data_str[:150]}...")
            
            # ä½¿ç”¨Playwrightçš„APIä¸Šä¸‹æ–‡å‘é€POSTè¯·æ±‚ï¼ˆè‡ªåŠ¨æºå¸¦cookiesï¼‰
            try:
                # æ–¹æ³•1: ä½¿ç”¨page.context.request
                api_response = await page.context.request.post(
                    self.api_endpoint,
                    data=form_data_str,
                    headers={
                        'Content-Type': 'application/x-www-form-urlencoded',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                        'X-Requested-With': 'XMLHttpRequest',
                        'Referer': page.url
                    }
                )
                
                response_body = await api_response.text()
                response = {
                    "status": api_response.status,
                    "body": response_body,
                    "url": api_response.url
                }
            except Exception as api_err:
                print(f"  [è­¦å‘Š] APIä¸Šä¸‹æ–‡è¯·æ±‚å¤±è´¥: {api_err}")
                # æ–¹æ³•2: å›é€€åˆ°page.evaluate
                response = await page.evaluate("""async ({url, formDataStr}) => {
                    try {
                        const response = await fetch(url, {
                            method: 'POST',
                            headers: {
                                'Content-Type': 'application/x-www-form-urlencoded',
                                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                                'X-Requested-With': 'XMLHttpRequest'
                            },
                            body: formDataStr,
                            credentials: 'include'
                        });
                        
                        const text = await response.text();
                        return {
                            status: response.status,
                            body: text,
                            url: response.url,
                            ok: response.ok
                        };
                    } catch (e) {
                        return {status: 0, body: '', error: e.message};
                    }
                }""", {"url": self.api_endpoint, "formDataStr": form_data_str})
            
            if response and response.get('status') == 200:
                print(f"  [æˆåŠŸ] APIè°ƒç”¨æˆåŠŸ (çŠ¶æ€: {response['status']})")
                return {
                    "status": response['status'],
                    "body": response['body'],
                    "url": response.get('url', self.api_endpoint)
                }
            else:
                print(f"  [è­¦å‘Š] APIè°ƒç”¨å¤±è´¥ (çŠ¶æ€: {response.get('status') if response else 'None'})")
                return None
                
        except Exception as e:
            print(f"  [é”™è¯¯] ç›´æ¥è°ƒç”¨APIå¤±è´¥: {e}")
            return None
    
    async def _call_download_api_with_payload(self, page: Page, captured_payload: str, p_param: str = "AWGLNB") -> Optional[Dict]:
        """
        ä½¿ç”¨æ•è·çš„payloadç›´æ¥è°ƒç”¨ä¸‹è½½API
        
        è¿™æ˜¯æœ€å¯é çš„æ–¹å¼ï¼Œå› ä¸ºä½¿ç”¨çš„æ˜¯æµè§ˆå™¨å®é™…å‘é€çš„payload
        """
        print("\n[ä½¿ç”¨æ•è·çš„Payloadè°ƒç”¨API]")
        print("-" * 40)
        
        try:
            # æ„å»ºå®Œæ•´çš„è¯·æ±‚payload
            # æ•è·çš„payloadå¯èƒ½åªæ˜¯docs=éƒ¨åˆ†ï¼Œéœ€è¦è¡¥å……å…¶ä»–å‚æ•°
            full_payload = captured_payload
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ é¢å¤–å‚æ•°
            if 'instance_id' not in captured_payload:
                # å¯èƒ½éœ€è¦ä»é¡µé¢è·å–instance_id
                instance_id = await page.evaluate("""() => {
                    // å°è¯•ä»é¡µé¢è·å–instance_id
                    const el = document.querySelector('[data-instance-id]');
                    if (el) return el.dataset.instanceId;
                    // æˆ–è€…ä»URLå‚æ•°è·å–
                    const urlParams = new URLSearchParams(window.location.search);
                    return urlParams.get('instance_id') || '';
                }""")
                
                if instance_id:
                    full_payload = f"{captured_payload}&instance_id={instance_id}"
            
            print(f"  [è¯·æ±‚] POST {self.api_endpoint}")
            print(f"  [Payload] {full_payload[:200]}...")
            
            # ä½¿ç”¨Playwrightå‘é€è¯·æ±‚
            try:
                api_response = await page.context.request.post(
                    self.api_endpoint,
                    data=full_payload,
                    headers={
                        'Content-Type': 'application/x-www-form-urlencoded',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                        'X-Requested-With': 'XMLHttpRequest',
                        'Referer': page.url
                    }
                )
                
                response_body = await api_response.text()
                response = {
                    "status": api_response.status,
                    "body": response_body,
                    "url": api_response.url
                }
            except Exception as api_err:
                print(f"  [è­¦å‘Š] APIè¯·æ±‚å¤±è´¥: {api_err}")
                # å›é€€åˆ°fetchæ–¹å¼
                response = await page.evaluate("""async ({url, payload}) => {
                    try {
                        const response = await fetch(url, {
                            method: 'POST',
                            headers: {
                                'Content-Type': 'application/x-www-form-urlencoded',
                                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                                'X-Requested-With': 'XMLHttpRequest'
                            },
                            body: payload,
                            credentials: 'include'
                        });
                        
                        const text = await response.text();
                        return {
                            status: response.status,
                            body: text,
                            url: response.url,
                            ok: response.ok
                        };
                    } catch (e) {
                        return {status: 0, body: '', error: e.message};
                    }
                }""", {"url": self.api_endpoint, "payload": full_payload})
            
            if response and response.get('status') == 200:
                print(f"  [æˆåŠŸ] APIè°ƒç”¨æˆåŠŸ (çŠ¶æ€: {response['status']})")
                return {
                    "status": response['status'],
                    "body": response['body'],
                    "url": response.get('url', self.api_endpoint)
                }
            else:
                print(f"  [è­¦å‘Š] APIè°ƒç”¨å¤±è´¥ (çŠ¶æ€: {response.get('status') if response else 'None'})")
                return None
                
        except Exception as e:
            print(f"  [é”™è¯¯] è°ƒç”¨APIå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    async def _call_download_api_with_articles(self, page: Page, article_metadata: List[Dict], p_param: str = "AWGLNB") -> Optional[Dict]:
        """
        ä½¿ç”¨æ–‡ç« å…ƒæ•°æ®æ„å»ºpayloadå¹¶è°ƒç”¨ä¸‹è½½API
        
        article_metadata æ ¼å¼:
        [{"docref":"news/xxx","cache_type":"AWGLNB","size":xxx,"pbi":"xxx","title":"xxx","product":"AWGLNB"}, ...]
        """
        print("\n[ä½¿ç”¨æ–‡ç« å…ƒæ•°æ®è°ƒç”¨API]")
        print("-" * 40)
        
        try:
            # ===== æ­¥éª¤1: SET è°ƒç”¨ =====
            # å…ˆè°ƒç”¨ nb-cache-doc/js/set å‘Šè¯‰æœåŠ¡å™¨è¦ä¸‹è½½å“ªäº›æ–‡ç« 
            set_url = "https://infoweb-newsbank-com.ezproxy.sl.nsw.gov.au/apps/news/nb-cache-doc/js/set"
            
            # è¿‡æ»¤åªä¿ç•™ news/ å¼€å¤´çš„æ–‡ç« 
            valid_articles = [a for a in article_metadata if a.get('docref', '').startswith('news/')]
            
            if not valid_articles:
                print("[é”™è¯¯] æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ç« ")
                return None
            
            # æ„å»ºdocs JSONæ•°ç»„ï¼ˆåªåŒ…å« news/ å¼€å¤´çš„ï¼‰
            docs_json = json.dumps(valid_articles[:100])  # æœ€å¤š100ç¯‡
            
            # Set è¯·æ±‚çš„ payload
            set_payload = f"docs={quote(docs_json)}&p={p_param}"
            
            print(f"\n[SET] è®¾ç½®è¦ä¸‹è½½çš„æ–‡ç« ...")
            print(f"  [è¯·æ±‚] POST nb-cache-doc/js/set")
            print(f"  [æ–‡ç« æ•°] {len(valid_articles)}")
            
            try:
                set_response = await page.context.request.post(
                    set_url,
                    data=set_payload,
                    headers={
                        'Content-Type': 'application/x-www-form-urlencoded',
                        'Accept': 'application/json, text/javascript, */*; q=0.01',
                        'X-Requested-With': 'XMLHttpRequest',
                        'Referer': page.url
                    }
                )
                set_body = await set_response.text()
                print(f"  [SET] çŠ¶æ€: {set_response.status}")
            except Exception as set_err:
                print(f"  [SET] è¯·æ±‚å¤±è´¥: {set_err}")
                return None
            
            # ===== æ­¥éª¤2: GET è°ƒç”¨ =====
            # ç„¶åè°ƒç”¨ nb-multidocs/get ä¸‹è½½æ–‡ç« 
            print(f"\n[GET] ä¸‹è½½æ–‡ç« ...")
            print(f"  [è¯·æ±‚] POST {self.api_endpoint}")
            
            # æ„å»ºpdf_params
            pdf_params_parts = [
                "action=pdf",
                "format=html",
                "pdf_enabled=false",
                "load_pager=false",
                "maxresults=20"
            ]
            pdf_params = "&".join(pdf_params_parts)
            
            # Get è¯·æ±‚çš„ payload
            get_payload_parts = [
                f"p={p_param}",
                "action=download",
                "pdf_path=multidocs",
                "maxresults=20",
                f"pdf_params={quote(pdf_params)}",
                "zustat_category_override=co_sc_pdf_download"
            ]
            get_payload = "&".join(get_payload_parts)
            
            print(f"  [Payload] {get_payload[:200]}...")
            
            # å‘é€GETè¯·æ±‚
            try:
                api_response = await page.context.request.post(
                    self.api_endpoint,
                    data=get_payload,
                    headers={
                        'Content-Type': 'application/x-www-form-urlencoded',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                        'X-Requested-With': 'XMLHttpRequest',
                        'Referer': page.url,
                        'Cache-Control': 'no-cache',
                        'Pragma': 'no-cache'
                    }
                )
                
                response_body = await api_response.text()
                response = {
                    "status": api_response.status,
                    "body": response_body,
                    "url": api_response.url
                }
            except Exception as api_err:
                print(f"  [è­¦å‘Š] APIè¯·æ±‚å¤±è´¥: {api_err}")
                # å›é€€åˆ°fetch
                response = await page.evaluate("""async ({url, payload}) => {
                    try {
                        const response = await fetch(url, {
                            method: 'POST',
                            headers: {
                                'Content-Type': 'application/x-www-form-urlencoded',
                                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                                'X-Requested-With': 'XMLHttpRequest',
                                'Cache-Control': 'no-cache'
                            },
                            body: payload,
                            credentials: 'include'
                        });
                        
                        const text = await response.text();
                        return {
                            status: response.status,
                            body: text,
                            url: response.url,
                            ok: response.ok
                        };
                    } catch (e) {
                        return {status: 0, body: '', error: e.message};
                    }
                }""", {"url": self.api_endpoint, "payload": get_payload})
            
            if response and response.get('status') == 200:
                print(f"  [æˆåŠŸ] APIè°ƒç”¨æˆåŠŸ (çŠ¶æ€: {response['status']})")
                return {
                    "status": response['status'],
                    "body": response['body'],
                    "url": response.get('url', self.api_endpoint)
                }
            else:
                print(f"  [è­¦å‘Š] APIè°ƒç”¨å¤±è´¥ (çŠ¶æ€: {response.get('status') if response else 'None'})")
                return None
                
        except Exception as e:
            print(f"  [é”™è¯¯] è°ƒç”¨APIå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    async def remove_selection(self, page: Page, p_param: str = "AWGLNB") -> bool:
        """
        æ¸…é™¤å½“å‰çš„æ–‡ç« é€‰æ‹©
        
        è°ƒç”¨ nb-cache-doc/js/remove API æ¥æ¸…é™¤æ‰€æœ‰å·²é€‰æ‹©çš„æ–‡ç« 
        """
        print("\n[æ¸…é™¤é€‰æ‹©]")
        print("-" * 40)
        
        remove_url = "https://infoweb-newsbank-com.ezproxy.sl.nsw.gov.au/apps/news/nb-cache-doc/js/remove"
        
        # ç”¨æˆ·ç¡®è®¤çš„payloadæ ¼å¼
        payload = f"docrefs=ALL&p={p_param}"
        
        print(f"  [è¯·æ±‚] POST nb-cache-doc/js/remove")
        print(f"  [Payload] {payload}")
        
        try:
            # ä½¿ç”¨Playwrightå‘é€è¯·æ±‚
            api_response = await page.context.request.post(
                remove_url,
                data=payload,
                headers={
                    'Content-Type': 'application/x-www-form-urlencoded',
                    'Accept': 'application/json, text/javascript, */*; q=0.01',
                    'X-Requested-With': 'XMLHttpRequest',
                    'Referer': page.url
                }
            )
            
            response_body = await api_response.text()
            
            if api_response.status == 200:
                print(f"  [æˆåŠŸ] æ¸…é™¤é€‰æ‹©æˆåŠŸ")
                return True
            else:
                print(f"  [è­¦å‘Š] æ¸…é™¤é€‰æ‹©å¤±è´¥ (çŠ¶æ€: {api_response.status})")
                print(f"  [è°ƒè¯•] å“åº”: {response_body[:200]}...")
                return False
                
        except Exception as e:
            print(f"  [é”™è¯¯] æ¸…é™¤é€‰æ‹©è¯·æ±‚å¤±è´¥: {e}")
            return False
    
    async def fetch_articles_via_api(self, 
                                     page: Page, 
                                     base_url: str,
                                     page_num: int = 1) -> List[ArticleInfo]:
        """
        é€šè¿‡APIè·å–æ–‡ç« åˆ—è¡¨
        
        æµç¨‹ï¼š
        1. é€‰ä¸­æ‰€æœ‰æ–‡ç« 
        2. æ•è·å®é™…å‘é€çš„payloadï¼ˆä»ç½‘ç»œè¯·æ±‚ï¼‰
        3. å¦‚æœæ•è·å¤±è´¥ï¼Œä»é¡µé¢ç›´æ¥æ„å»ºpayload
        4. ä½¿ç”¨payloadè°ƒç”¨ä¸‹è½½API
        """
        print(f"\n[ç¬¬ {page_num} é¡µ] è·å–æ–‡ç« åˆ—è¡¨")
        print("-" * 40)
        print(f"  [è°ƒè¯•] å½“å‰URL: {page.url[:100]}...")
        
        articles = []
        p_param = "AWGLNB"  # é»˜è®¤å€¼
        
        try:
            # è·å–é¡µé¢HTMLä»¥æå–æ–‡ç« ID
            html_content = await page.content()
            
            article_ids = self._extract_article_ids_from_page(html_content)
            
            print(f"  [æ‰«æ] ä»é¡µé¢æå–åˆ° {len(article_ids)} ä¸ªæ–‡ç« ID")
            
            # ä»å½“å‰URLæå–å‚æ•°
            parsed_url = urlparse(page.url)
            query_params = parse_qs(parsed_url.query)
            p_param = query_params.get('p', ['AWGLNB'])[0]
            
            # æ­¥éª¤1: åœ¨é€‰ä¸­æ–‡ç« ä¹‹å‰ï¼Œå…ˆè®¾ç½®å¥½ç½‘ç»œè¯·æ±‚ç›‘å¬å™¨
            print(f"  [æ­¥éª¤1] è®¾ç½®payloadæ•è·ç›‘å¬å™¨...")
            captured_payloads = []
            
            async def handle_request(request):
                url = request.url
                if "nb-cache-doc" in url or "nb-multidocs" in url:
                    try:
                        post_data = request.post_data
                        if post_data:
                            if isinstance(post_data, bytes):
                                post_data = post_data.decode('utf-8')
                            if 'docs=' in post_data:
                                captured_payloads.append({
                                    'url': url,
                                    'post_data': post_data,
                                    'timestamp': time.time(),
                                    'length': len(post_data)
                                })
                                print(f"  [æ•è·] æ•è·åˆ°è¯·æ±‚: {request.method} {url.split('/')[-1]}")
                    except:
                        pass
            
            # è®¾ç½®ç›‘å¬å™¨ï¼ˆå…³é”®ï¼šåœ¨ç‚¹å‡»ä¹‹å‰å°±è®¾ç½®ï¼ï¼‰
            page.on("request", handle_request)
            
            # æ­¥éª¤2: é€‰ä¸­æ‰€æœ‰æ–‡ç« ï¼ˆç‚¹å‡»ä¼šè§¦å‘ç½‘ç»œè¯·æ±‚ï¼‰
            print(f"  [æ­¥éª¤2] é€‰ä¸­æ‰€æœ‰æ–‡ç« ...")
            select_success = await self.select_all_articles(page)
            if select_success:
                print(f"  [æˆåŠŸ] æ–‡ç« å·²é€‰ä¸­")
            else:
                print(f"  [è­¦å‘Š] é€‰æ‹©æ–‡ç« å¯èƒ½æœªæˆåŠŸï¼Œç»§ç»­å°è¯•...")
            
            # ç­‰å¾…æ•è·ç½‘ç»œè¯·æ±‚
            print(f"  [ç­‰å¾…] ç­‰å¾…æ•è·payload...")
            for i in range(5):
                await asyncio.sleep(0.5)
                if captured_payloads:
                    print(f"  [æˆåŠŸ] æ•è·åˆ° {len(captured_payloads)} ä¸ªè¯·æ±‚")
                    break
            
            # è·å–æ•è·çš„payload
            if captured_payloads:
                captured_payload = captured_payloads[-1]['post_data']
                print(f"  [æˆåŠŸ] æ•è·åˆ°payload ({len(captured_payload)} å­—ç¬¦)")
            else:
                captured_payload = None
                print(f"  [è­¦å‘Š] æœªæ•è·åˆ°payload")
            
            # ç”¨äºå­˜å‚¨æ–‡ç« å…ƒæ•°æ®
            article_metadata = None
            
            if captured_payload:
                # è§£ææ•è·çš„payloadè·å–æ–‡ç« å…ƒæ•°æ®
                article_metadata = self._parse_captured_payload(captured_payload)
            
            # å¦‚æœæ²¡æœ‰æ•è·åˆ°payloadï¼Œæˆ–è€…éœ€è¦å¼ºåˆ¶åˆ·æ–°ï¼Œä»é¡µé¢ç›´æ¥æ„å»º
            if not article_metadata:
                print(f"  [å¤‡é€‰] ä»é¡µé¢ç›´æ¥æå–æ–‡ç« å…ƒæ•°æ®...")
                article_metadata = await self._extract_selected_articles_metadata(page)
            
            if article_metadata and len(article_metadata) > 0:
                print(f"  [æˆåŠŸ] è·å–åˆ° {len(article_metadata)} ç¯‡æ–‡ç« å…ƒæ•°æ®")
                
                # ä½¿ç”¨æ–‡ç« å…ƒæ•°æ®æ„å»ºpayloadå¹¶è°ƒç”¨API
                print(f"  [æ­¥éª¤3] æ„å»ºpayloadå¹¶è°ƒç”¨ä¸‹è½½API...")
                response_data = await self._call_download_api_with_articles(page, article_metadata, p_param)
                
                if response_data and response_data.get('body'):
                    body = response_data['body']
                    print(f"  [è°ƒè¯•] APIå“åº”é•¿åº¦: {len(body)} bytes")
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯PDF
                    if body.startswith('%PDF'):
                        print("  [è­¦å‘Š] APIè¿”å›PDF")
                        articles = await self._parse_articles_from_page(page, page_num)
                    else:
                        articles = self._parse_api_response(body, page_num)
                        print(f"  [æˆåŠŸ] ä»APIå“åº”è§£æåˆ° {len(articles)} ç¯‡æ–‡ç« ")
                
                # æ¸…é™¤é€‰æ‹©ï¼Œé¿å…ç´¯ç§¯å¯¼è‡´payloadè¿‡é•¿
                await self.remove_selection(page, p_param)
                
                return articles
            
            # å¦‚æœå®Œå…¨æ²¡æœ‰è·å–åˆ°æ–‡ç« å…ƒæ•°æ®ï¼Œå›é€€åˆ°åŸæœ‰æ–¹æ³•
            print(f"  [å›é€€] æœªèƒ½è·å–æ–‡ç« å…ƒæ•°æ®ï¼Œä½¿ç”¨åŸæœ‰æ–¹æ³•...")
            print(f"  [æ­¥éª¤3] è°ƒç”¨APIè·å–æ–‡ç« æ•°æ®...")
            response_data = await self.call_download_api_directly(page, article_ids, p_param)
            
            if response_data and response_data.get('body'):
                body = response_data['body']
                print(f"  [è°ƒè¯•] APIå“åº”é•¿åº¦: {len(body)} bytes")
                print(f"  [è°ƒè¯•] å“åº”å‰500å­—ç¬¦: {body[:500]}")
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯PDFæˆ–äºŒè¿›åˆ¶æ•°æ®
                if body.startswith('%PDF'):
                    print("  [è­¦å‘Š] APIè¿”å›çš„æ˜¯PDFæ–‡ä»¶ï¼Œä¸æ˜¯HTML")
                    # ä¿å­˜PDF
                    pdf_path = self.output_dir / f"articles_page_{page_num}.pdf"
                    with open(pdf_path, 'wb') as f:
                        f.write(body.encode('utf-8', errors='replace'))
                    print(f"  [ä¿å­˜] PDFå·²ä¿å­˜: {pdf_path}")
                    # å›é€€åˆ°é¡µé¢è§£æ
                    articles = await self._parse_articles_from_page(page, page_num)
                else:
                    # è§£æHTMLå“åº”
                    articles = self._parse_api_response(body, page_num)
                    print(f"  [æˆåŠŸ] ä»APIå“åº”è§£æåˆ° {len(articles)} ç¯‡æ–‡ç« ")
            else:
                # å¤‡é€‰: ç›´æ¥è§£æé¡µé¢è·å–æ–‡ç« ä¿¡æ¯
                print("  [å¤‡é€‰] ç›´æ¥è§£æé¡µé¢å†…å®¹...")
                articles = await self._parse_articles_from_page(page, page_num)
            
            # æ¸…é™¤é€‰æ‹©ï¼Œé¿å…ç´¯ç§¯å¯¼è‡´payloadè¿‡é•¿
            await self.remove_selection(page, p_param)
            
            return articles
            
        except Exception as e:
            print(f"  [é”™è¯¯] APIè¯·æ±‚å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            # å¤±è´¥æ—¶ä¹Ÿå°è¯•æ¸…é™¤é€‰æ‹©
            try:
                await self.remove_selection(page, p_param)
            except:
                await self.remove_selection(page, "AWGLNB")  # ä½¿ç”¨é»˜è®¤å€¼
            # å¤±è´¥æ—¶å›é€€åˆ°é¡µé¢è§£æ
            return await self._parse_articles_from_page(page, page_num)
    
    async def fetch_page_metadata_only(self, page: Page, page_num: int = 1) -> Optional[List[Dict]]:
        """
        åªè·å–é¡µé¢å…ƒæ•°æ®ï¼Œä¸ä¸‹è½½æ–‡ç« å†…å®¹
        
        æ–°æµç¨‹æ­¥éª¤2ï¼šé€‰ä¸­â†’è·å–å…ƒæ•°æ®â†’è¿”å›ï¼ˆç¿»é¡µç”±è°ƒç”¨è€…å¤„ç†ï¼‰
        
        Returns:
            æ–‡ç« å…ƒæ•°æ®åˆ—è¡¨ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        print(f"\n[ç¬¬ {page_num} é¡µ] è·å–å…ƒæ•°æ®...")
        print("-" * 40)
        
        article_metadata = None
        
        # è®¾ç½®ç½‘ç»œè¯·æ±‚ç›‘å¬å™¨ï¼ˆåœ¨é€‰ä¸­ä¹‹å‰ï¼‰
        captured_payloads = []
        
        async def handle_request(request):
            url = request.url
            if "nb-cache-doc" in url:
                try:
                    post_data = request.post_data
                    if post_data:
                        if isinstance(post_data, bytes):
                            post_data = post_data.decode('utf-8')
                        if 'docs=' in post_data:
                            captured_payloads.append(post_data)
                except:
                    pass
        
        page.on("request", handle_request)
        
        try:
            # æ­¥éª¤1: é€‰ä¸­å½“å‰é¡µæ‰€æœ‰æ–‡ç« 
            print(f"  [é€‰ä¸­] ç‚¹å‡»å…¨é€‰å¤é€‰æ¡†...")
            select_success = await self.select_all_articles(page)
            if not select_success:
                print(f"  [è­¦å‘Š] é€‰æ‹©æ–‡ç« å¯èƒ½æœªæˆåŠŸï¼Œç»§ç»­...")
            
            # ç­‰å¾…ç½‘ç»œè¯·æ±‚å®Œæˆ
            print(f"  [ç­‰å¾…] ç­‰å¾…ç½‘ç»œè¯·æ±‚...")
            for i in range(5):
                await asyncio.sleep(0.5)
                if captured_payloads:
                    print(f"  [æˆåŠŸ] æ•è·åˆ° {len(captured_payloads)} ä¸ªè¯·æ±‚")
                    break
            
            # æ­¥éª¤2: è§£ææ•è·çš„payloadè·å–å…ƒæ•°æ®
            if captured_payloads:
                payload = captured_payloads[-1]  # ä½¿ç”¨æœ€åä¸€ä¸ª
                article_metadata = self._parse_captured_payload(payload)
                if article_metadata:
                    print(f"  [æˆåŠŸ] ä»æ•è·çš„payloadè§£æåˆ° {len(article_metadata)} ç¯‡æ–‡ç« ")
            
            # å¦‚æœæ²¡æœ‰æ•è·åˆ°ï¼Œä»é¡µé¢æå–
            if not article_metadata:
                print(f"  [æœªæ•è·] æœªæ•è·åˆ°payloadï¼Œä»é¡µé¢æå–...")
                article_metadata = await self._extract_selected_articles_metadata(page)
            
            # ä»HTMLä¸­æå–previewå¹¶è¡¥å……åˆ°å…ƒæ•°æ®ä¸­
            if article_metadata:
                print(f"  [è¡¥å……] ä»HTMLæå–preview...")
                html_content = await page.content()
                html_articles = self._extract_preview_from_html(html_content)
                
                # æ‰“å°payloadä¸­çš„docrefç”¨äºå¯¹æ¯”
                print(f"  [è°ƒè¯•] Payloadä¸­çš„docref (å‰5ä¸ª):")
                for i, art in enumerate(article_metadata[:5]):
                    print(f"    {i+1}: {art.get('docref', 'N/A')}")
                
                # æ‰“å°HTMLä¸­çš„docrefç”¨äºå¯¹æ¯”
                print(f"  [è°ƒè¯•] HTMLä¸­çš„docref (å‰5ä¸ª):")
                for i, ha in enumerate(html_articles[:5]):
                    print(f"    {i+1}: {ha.get('docref', 'N/A')}")
                
                if html_articles:
                    matched_count = 0
                    for art in article_metadata:
                        docref = art.get('docref', '')
                        for html_art in html_articles:
                            if html_art.get('docref') == docref:
                                # å¦‚æœHTMLä¸­æœ‰previewï¼Œåˆ™è¦†ç›–payloadä¸­çš„previewï¼ˆHTMLçš„æ›´å‡†ç¡®ï¼‰
                                html_preview = html_art.get('preview', '')
                                if html_preview:
                                    art['preview'] = html_preview
                                    matched_count += 1
                                break
                    print(f"  [è¡¥å……] ä¸º {matched_count} ç¯‡æ–‡ç« è¡¥å……äº†preview")
                else:
                    print(f"  [è­¦å‘Š] æœªä»HTMLæå–åˆ°preview")
            
            if not article_metadata:
                print(f"  [ç¬¬ {page_num}] æœªè·å–åˆ°å…ƒæ•°æ®")
                return None
            
            # æ­¥éª¤3: ç­›é€‰ docref ä»¥ "news/" å¼€å¤´çš„è®°å½•ï¼ˆè¿‡æ»¤æ‰ image/ ç­‰éæ–‡ç« è®°å½•ï¼‰
            filtered_by_docref = [
                art for art in article_metadata 
                if art.get('docref', '').startswith('news/')
            ]
            
            if len(filtered_by_docref) < len(article_metadata):
                removed_count = len(article_metadata) - len(filtered_by_docref)
                print(f"  [é¢„ç­›é€‰] è¿‡æ»¤æ‰ {removed_count} æ¡é news/ è®°å½•")
            
            print(f"  [å®Œæˆ] è·å–åˆ° {len(filtered_by_docref)} ç¯‡å…ƒæ•°æ®")
            
            # æ¸…é™¤é€‰æ‹©ï¼Œé¿å…ç´¯ç§¯
            try:
                await self.remove_selection(page, "AWGLNB")
            except:
                pass
            
            return filtered_by_docref
            
        except Exception as e:
            print(f"  [é”™è¯¯] è·å–å…ƒæ•°æ®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    async def _parse_articles_from_page(self, page: Page, page_num: int) -> List[ArticleInfo]:
        """ä»é¡µé¢HTMLä¸­è§£ææ–‡ç« åˆ—è¡¨"""
        articles = []
        
        try:
            # æŸ¥æ‰¾æ–‡ç« å…ƒç´ 
            article_elements = await page.query_selector_all('article.search-hits__hit')
            
            if not article_elements:
                print(f"  [è­¦å‘Š] æœªæ‰¾åˆ°æ–‡ç« å…ƒç´ ")
                return articles
            
            print(f"  [è§£æ] æ‰¾åˆ° {len(article_elements)} ç¯‡æ–‡ç« å…ƒç´ ")
            
            for i, elem in enumerate(article_elements, 1):
                try:
                    # æå–æ ‡é¢˜
                    title_elem = await elem.query_selector("h3.search-hits__hit__title a")
                    if not title_elem:
                        continue
                    
                    title = await title_elem.inner_text()
                    title = title.replace("Go to the document viewer for ", "").strip()
                    
                    # æå–URL
                    href = await title_elem.get_attribute("href") or ""
                    full_url = urljoin(page.url, href)
                    
                    # æå–æ—¥æœŸ
                    date = ""
                    date_elem = await elem.query_selector("li.search-hits__hit__meta__item--display-date")
                    if date_elem:
                        date = await date_elem.inner_text()
                    
                    # æå–æ¥æº
                    source = ""
                    source_elem = await elem.query_selector("li.search-hits__hit__meta__item--source")
                    if source_elem:
                        source = await source_elem.inner_text()
                    
                    # æå–ä½œè€…
                    author = ""
                    author_elem = await elem.query_selector("li.search-hits__hit__meta__item--author")
                    if author_elem:
                        author = await author_elem.inner_text()
                    
                    # æå–é¢„è§ˆ
                    preview = ""
                    preview_elem = await elem.query_selector("div.preview-first-paragraph")
                    if preview_elem:
                        preview = await preview_elem.inner_text()
                    
                    preview = preview.strip()
                    word_count = len(preview.split()) if preview else 0
                    
                    # æå–æ–‡ç« ID
                    article_id = None
                    id_match = re.search(r'doc=([^&]+)', href)
                    if id_match:
                        article_id = unquote(id_match.group(1))
                    
                    article = ArticleInfo(
                        title=title[:300],
                        date=date.strip()[:100],
                        source=source.strip()[:200],
                        author=author.strip()[:100],
                        preview=preview[:1000],
                        url=full_url[:500],
                        page_num=page_num,
                        article_id=article_id,
                        word_count=word_count
                    )
                    
                    articles.append(article)
                    
                    # æ˜¾ç¤ºå‰å‡ ç¯‡æ–‡ç« 
                    if i <= 3:
                        print(f"    [{i}] {title[:50]}... ({word_count}è¯)")
                
                except Exception as e:
                    print(f"    [é”™è¯¯] è§£ææ–‡ç« å¤±è´¥: {e}")
                    continue
            
            return articles
            
        except Exception as e:
            print(f"  [é”™è¯¯] è§£æé¡µé¢å¤±è´¥: {e}")
            return articles
    
    def _parse_api_response(self, response_body: str, page_num: int) -> List[ArticleInfo]:
        """
        è§£æAPIå“åº”å†…å®¹ - ä»DownloadæŒ‰é’®çš„å“åº”ä¸­æå–æ–‡ç« 
        
        APIå“åº”é€šå¸¸åŒ…å«HTMLæ ¼å¼çš„å¤šç¯‡æ–‡ç« ï¼Œæ¯ç¯‡éƒ½æœ‰å®Œæ•´æˆ–éƒ¨åˆ†æ–‡ç« å†…å®¹
        """
        articles = []
        
        try:
            # å°è¯•è§£æä¸ºJSON
            try:
                data = json.loads(response_body)
                print(f"  [è§£æ] å“åº”ä¸ºJSONæ ¼å¼")
                
                # æ ¹æ®å®é™…å“åº”ç»“æ„è°ƒæ•´
                if isinstance(data, dict):
                    # å¯èƒ½åŒ…å«æ–‡ç« åˆ—è¡¨çš„é”®
                    for key in ['articles', 'docs', 'results', 'data', 'items', 'documents']:
                        if key in data:
                            items = data[key]
                            if isinstance(items, list):
                                for item in items:
                                    article = self._convert_api_item_to_article(item, page_num)
                                    if article:
                                        articles.append(article)
                                print(f"  [æˆåŠŸ] ä»JSONè§£æåˆ° {len(articles)} ç¯‡æ–‡ç« ")
                                return articles
                
                # å¦‚æœæ‰¾ä¸åˆ°ç‰¹å®škeyï¼Œå°è¯•ç›´æ¥éå†
                for key, value in data.items():
                    if isinstance(value, list) and len(value) > 0:
                        for item in value:
                            article = self._convert_api_item_to_article(item, page_num)
                            if article:
                                articles.append(article)
                        if articles:
                            print(f"  [æˆåŠŸ] ä»JSON key '{key}' è§£æåˆ° {len(articles)} ç¯‡æ–‡ç« ")
                            return articles
                
            except json.JSONDecodeError:
                # ä¸æ˜¯JSONï¼Œæ˜¯HTML
                pass
            
            # è§£æHTMLå“åº” - Download APIé€šå¸¸è¿”å›åŒ…å«å¤šç¯‡æ–‡ç« çš„HTML
            print(f"  [è§£æ] å“åº”ä¸ºHTMLæ ¼å¼ï¼Œé•¿åº¦: {len(response_body)}")
            
            # ä¿å­˜å“åº”å†…å®¹ç”¨äºè°ƒè¯•åˆ†æ
            debug_file = self.output_dir / f"debug_response_page{page_num}.html"
            try:
                with open(debug_file, 'w', encoding='utf-8') as f:
                    f.write(response_body)
                print(f"  [è°ƒè¯•] å·²ä¿å­˜å“åº”HTML: {debug_file}")
            except Exception as e:
                print(f"  [è°ƒè¯•] ä¿å­˜å“åº”å¤±è´¥: {e}")
            
            # æŸ¥æ‰¾æ–‡ç« å®¹å™¨ - å¯èƒ½åŒ…å«å®Œæ•´æ–‡ç« å†…å®¹
            # NewsBank multidocä¸‹è½½å“åº”é€šå¸¸åŒ…å«å¤šä¸ªdocumentæˆ–articleåŒºå—
            article_blocks = []
            
            # å°è¯•å¤šç§æ–‡ç« åˆ†éš”æ¨¡å¼
            # APIå“åº”ä½¿ç”¨ multidocs_item ç±»ï¼ˆæ³¨æ„classåå¯èƒ½æœ‰å°¾éƒ¨ç©ºæ ¼ï¼‰
             
            # æ–¹æ³•1: ä½¿ç”¨å‰ç»æ–­è¨€æ­£ç¡®åˆ†å‰²ï¼ˆæ¨èï¼‰
            # åŒ¹é…ä» <div class="multidocs_item "> åˆ°ä¸‹ä¸€ä¸ª <div class="multidocs_item "> ä¹‹å‰
            # è¿™æ ·å¯ä»¥æ­£ç¡®å¤„ç†HTMLä¸­åµŒå¥—çš„å¤šå±‚div
            pattern1 = r'<div class="multidocs_item ">(.*?)(?=<div class="multidocs_item ">)'
            matches1 = re.findall(pattern1, response_body, re.DOTALL | re.IGNORECASE)
            if matches1 and len(matches1) >= 1:
                article_blocks = matches1
                print(f"  [åŒ¹é…] ä½¿ç”¨å‰ç»æ–­è¨€æ¨¡å¼æ‰¾åˆ° {len(matches1)} ä¸ªæ–‡ç« å—")
                
                # è·å–æœ€åä¸€ä¸ªå— - ä»æœ€åä¸€ä¸ª multidocs_item å¼€å§‹åˆ°æ–‡æ¡£æœ«å°¾
                # ä½¿ç”¨ last_index æ¥æ‰¾åˆ°æœ€åä¸€ä¸ª
                last_start = response_body.rfind('<div class="multidocs_item ">')
                if last_start > 0:
                    last_content = response_body[last_start + len('<div class="multidocs_item ">'):]
                    # æ‰¾åˆ°æœ€åä¸€ä¸ªå—çš„ç»“æŸä½ç½®
                    if last_content.strip():
                        article_blocks.append(last_content)
                        print(f"  [åŒ¹é…] æ·»åŠ æœ€åä¸€ä¸ªå—ï¼Œæ€»è®¡ {len(article_blocks)} ä¸ª")
            
            # æ–¹æ³•2: å¦‚æœæ–¹æ³•1å¤±è´¥ï¼Œå°è¯•åŒ¹é…åˆ°æœ€åä¸€ä¸ª </div></div>
            if not article_blocks:
                pattern2 = r'<div class="multidocs_item ">(.*?)</div>\s*</div>'
                matches2 = re.findall(pattern2, response_body, re.DOTALL | re.IGNORECASE)
                if matches2 and len(matches2) >= 1:
                    article_blocks = matches2
                    print(f"  [åŒ¹é…] ä½¿ç”¨è´ªå©ªæ¨¡å¼æ‰¾åˆ° {len(matches2)} ä¸ªæ–‡ç« å—")
            
            # æ–¹æ³•3: å›é€€åˆ°åŸæ¥çš„æ¨¡å¼ï¼ˆå¯èƒ½åŒ¹é…ä¸æ­£ç¡®ï¼‰
            if not article_blocks:
                separators = [
                    (r'<div[^>]*class="[^\"]*multidocs_item[^\"]*"\s*>', r'</div>'),
                    (r'<div[^>]*class="[^\"]*multidocs_item[^\"]*"[^>]*>', r'</div>'),
                ]
                for start_pattern, end_pattern in separators:
                    pattern = f'{start_pattern}(.*?){end_pattern}'
                    matches = re.findall(pattern, response_body, re.DOTALL | re.IGNORECASE)
                    if matches and len(matches) > 1:
                        article_blocks = matches
                        print(f"  [åŒ¹é…] ä½¿ç”¨å›é€€æ¨¡å¼æ‰¾åˆ° {len(matches)} ä¸ªæ–‡ç« å—")
                        break
            
            # å¦‚æœæ²¡æ‰¾åˆ°åˆ†éš”å—ï¼Œå°è¯•æ•´ä¸ªå“åº”ä½œä¸ºä¸€ä¸ªæ–‡ç« 
            if not article_blocks:
                print(f"  [è°ƒè¯•] æœªæ‰¾åˆ°æ–‡ç« åˆ†éš”å—ï¼Œå°è¯•æ•´ä¸ªå“åº”ä½œä¸ºå•ä¸ªæ–‡ç« ")
                article_blocks = [response_body]
            
            print(f"  [è°ƒè¯•] å¼€å§‹è§£æ {len(article_blocks)} ä¸ªæ–‡ç« å—...")
            
            # ã€ä¿®å¤ã€‘å¦‚æœåªæœ‰1ä¸ªå—ä¸”é•¿åº¦å¾ˆå°ï¼Œè¯´æ˜åˆ†å‰²å¤±è´¥ï¼Œç›´æ¥ç”¨å®Œæ•´response_body
            if len(article_blocks) == 1 and len(article_blocks[0]) < 500:
                print(f"  [è°ƒè¯•] æ–‡ç« å—å¤ªçŸ­({len(article_blocks[0])}å­—ç¬¦)ï¼Œå¯èƒ½æ˜¯åˆ†å‰²å¤±è´¥ï¼Œå°è¯•ç›´æ¥è§£æå®Œæ•´å“åº”")
                article_blocks = [response_body]
            
            # è§£ææ¯ä¸ªæ–‡ç« å—
            for i, html_snippet in enumerate(article_blocks):
                article = self._parse_full_article_from_html(html_snippet, page_num, i+1)
                
                # åªæ£€æŸ¥æ˜¯å¦æœ‰æ ‡é¢˜
                if article and article.title and len(article.title) > 3:
                    articles.append(article)
                    if len(articles) <= 3:  # åªæ‰“å°å‰3ç¯‡çš„è°ƒè¯•ä¿¡æ¯
                        print(f"    [è°ƒè¯•] æ–‡ç« {len(articles)}: {article.title[:50]}... (å­—æ•°: {article.word_count})")
                elif article and article.title:
                    if len(articles) < 3:
                        print(f"    [è¿‡æ»¤] æ ‡é¢˜å¤ªçŸ­: {article.title[:50]}...")
            
            if articles:
                print(f"  [æˆåŠŸ] ä»HTMLè§£æåˆ° {len(articles)} ç¯‡æ–‡ç« ")
            else:
                print(f"  [è­¦å‘Š] æœªèƒ½ä»HTMLè§£æåˆ°æ–‡ç« ï¼Œå°è¯•å¤‡ç”¨è§£æ...")
                # å¤‡ç”¨ï¼šåªæå–æ ‡é¢˜
                articles = self._extract_articles_fallback(response_body, page_num)
            
            return articles
            
        except Exception as e:
            print(f"  [é”™è¯¯] è§£æAPIå“åº”å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return articles
    
    def _parse_full_article_from_html(self, html_snippet: str, page_num: int, index: int) -> Optional[ArticleInfo]:
        """ä»HTMLç‰‡æ®µä¸­è§£æå®Œæ•´æ–‡ç« ä¿¡æ¯ï¼ˆåŒ…æ‹¬å…¨æ–‡ï¼‰- æ”¯æŒAPIå“åº”çš„multidocsæ ¼å¼"""
        try:
            # æå–æ ‡é¢˜ - APIå“åº”ä½¿ç”¨ h1.document-view__title
            title = ""
            
            # æ–¹æ³•1: APIå“åº”æ ¼å¼ h1.document-view__title
            title_match = re.search(r'<h1[^>]*class="[^"]*document-view__title[^"]*"[^>]*>(.*?)</h1>', 
                                   html_snippet, re.DOTALL | re.IGNORECASE)
            if title_match:
                title = re.sub(r'<[^>]+>', '', title_match.group(1)).strip()
            
            # æ–¹æ³•2: å›é€€åˆ°ä»»ä½• h1/h2/h3
            if not title:
                h_match = re.search(r'<h[123][^>]*>(.*?)</h[123]>', html_snippet, re.DOTALL | re.IGNORECASE)
                if h_match:
                    title = re.sub(r'<[^>]+>', '', h_match.group(1)).strip()
                    print(f"    [è§£æè°ƒè¯•] æ–¹æ³•2æ‰¾åˆ°æ ‡é¢˜: {title[:50]}")
            
            # è¿‡æ»¤æ— æ•ˆæ ‡é¢˜
            if not title or len(title) < 3 or title.startswith("<"):
                return None
            
            # æå–æ—¥æœŸ - APIæ ¼å¼ span.display-date
            date = ""
            date_match = re.search(r'<span[^>]*class="[^"]*display-date[^"]*"[^>]*>(.*?)</span>', 
                                  html_snippet, re.DOTALL | re.IGNORECASE)
            if date_match:
                date = re.sub(r'<[^>]+>', '', date_match.group(1)).strip()
            
            # æå–æ¥æº - APIæ ¼å¼ span.source
            source = ""
            source_match = re.search(r'<span[^>]*class="[^"]*source[^"]*"[^>]*>(.*?)</span>', 
                                    html_snippet, re.DOTALL | re.IGNORECASE)
            if source_match:
                source = re.sub(r'<[^>]+>', '', source_match.group(1)).strip()
            
            # æå–ä½œè€… - APIæ ¼å¼ span.author
            author = ""
            author_match = re.search(r'<span[^>]*class="[^"]*author[^"]*"[^>]*>(.*?)</span>', 
                                    html_snippet, re.DOTALL | re.IGNORECASE)
            if author_match:
                author_text = re.sub(r'<[^>]+>', '', author_match.group(1)).strip()
                # ç§»é™¤ "Author: " å‰ç¼€
                author = re.sub(r'^Author:\s*', '', author_text)
            
            # æå–æ–‡ç« ID - ä»OpenURLé“¾æ¥ä¸­æå–
            article_id = None
            openurl_match = re.search(r'rft_dat=document_id:([^"]+)', html_snippet)
            if openurl_match:
                article_id = unquote(openurl_match.group(1))
            
            # æå–URL
            url = ""
            if article_id:
                url = f"https://infoweb-newsbank-com.ezproxy.sl.nsw.gov.au/apps/news/document-view?p=AWGLNB&doc={article_id}"
            
            # æå–å…¨æ–‡ - APIæ ¼å¼ div.document-view__body
            full_text = ""
            body_match = re.search(r'<div[^>]*class="[^"]*document-view__body[^"]*"[^>]*>(.*?)</div>', 
                                  html_snippet, re.DOTALL | re.IGNORECASE)
            if body_match:
                # å°† <br/> è½¬æ¢ä¸ºæ¢è¡Œç¬¦
                body_html = body_match.group(1)
                body_html = re.sub(r'<br\s*/?>', '\n', body_html, flags=re.IGNORECASE)
                full_text = re.sub(r'<[^>]+>', '', body_html).strip()
            
            # ç¡®ä¿æœ‰é¢„è§ˆ
            preview = full_text[:500] if full_text else ""
            
            word_count = len(full_text.split()) if full_text else 0
            
            if title:  # åªæœ‰æœ‰æ ‡é¢˜æ‰è¿”å›
                return ArticleInfo(
                    title=title[:300],
                    date=date[:100],
                    source=source[:200],
                    author=author[:100],
                    preview=preview[:1000],
                    url=url[:500],
                    page_num=page_num,
                    article_id=article_id,
                    word_count=word_count,
                    full_text=full_text
                )
            
            return None
            
        except Exception as e:
            return None
    
    def _extract_articles_fallback(self, response_body: str, page_num: int) -> List[ArticleInfo]:
        """å¤‡ç”¨æ–¹æ³•ï¼šåªæå–æ ‡é¢˜å’ŒåŸºæœ¬ä¿¡æ¯"""
        articles = []
        
        # æå–æ‰€æœ‰æ ‡é¢˜
        title_pattern = r'<h[23][^>]*>(.*?)</h[23]>'
        titles = re.findall(title_pattern, response_body, re.DOTALL | re.IGNORECASE)
        
        for i, title_html in enumerate(titles, 1):
            title = re.sub(r'<[^>]+>', '', title_html).strip()
            if title and len(title) > 5:
                # å°è¯•åœ¨è¿™ä¸ªæ ‡é¢˜é™„è¿‘æ‰¾æ›´å¤šå†…å®¹
                # è·å–æ ‡é¢˜åçš„éƒ¨åˆ†å†…å®¹
                title_pos = response_body.find(title_html)
                nearby_content = response_body[title_pos:title_pos+2000] if title_pos >= 0 else ""
                
                # æå–æ–‡ç« ID
                article_id = None
                doc_match = re.search(r'doc=([^&"\s]+)', nearby_content)
                if doc_match:
                    article_id = unquote(doc_match.group(1))
                
                articles.append(ArticleInfo(
                    title=title[:300],
                    date="",
                    source="",
                    author="",
                    preview=nearby_content[:500],
                    url="",
                    page_num=page_num,
                    article_id=article_id,
                    word_count=0
                ))
        
        return articles
    
    def _convert_api_item_to_article(self, item: Dict, page_num: int) -> Optional[ArticleInfo]:
        """å°†APIè¿”å›çš„JSONé¡¹è½¬æ¢ä¸ºArticleInfo"""
        try:
            title = item.get('title', item.get('headline', item.get('name', '')))
            date = item.get('date', item.get('pubdate', item.get('published', '')))
            source = item.get('source', item.get('publication', ''))
            author = item.get('author', item.get('byline', ''))
            preview = item.get('preview', item.get('abstract', item.get('snippet', '')))
            url = item.get('url', item.get('link', ''))
            doc_id = item.get('doc', item.get('id', item.get('document_id', None)))
            
            word_count = len(preview.split()) if preview else 0
            
            return ArticleInfo(
                title=str(title)[:300],
                date=str(date)[:100],
                source=str(source)[:200],
                author=str(author)[:100],
                preview=str(preview)[:1000],
                url=str(url)[:500],
                page_num=page_num,
                article_id=str(doc_id) if doc_id else None,
                word_count=word_count
            )
        except Exception:
            return None
    
    def _parse_article_html(self, html_snippet: str, page_num: int) -> Optional[ArticleInfo]:
        """ä»HTMLç‰‡æ®µä¸­è§£ææ–‡ç« ä¿¡æ¯"""
        try:
            # æå–æ ‡é¢˜
            title_match = re.search(r'<h[123][^>]*>(.*?)</h[123]>', html_snippet, re.DOTALL | re.IGNORECASE)
            title = re.sub(r'<[^>]+>', '', title_match.group(1)).strip() if title_match else ""
            
            # æå–æ—¥æœŸ
            date_match = re.search(r'class="[^"]*date[^"]*"[^>]*>(.*?)</', html_snippet, re.DOTALL | re.IGNORECASE)
            date = re.sub(r'<[^>]+>', '', date_match.group(1)).strip() if date_match else ""
            
            # æå–æ¥æº
            source_match = re.search(r'class="[^"]*source[^"]*"[^>]*>(.*?)</', html_snippet, re.DOTALL | re.IGNORECASE)
            source = re.sub(r'<[^>]+>', '', source_match.group(1)).strip() if source_match else ""
            
            # æå–é¢„è§ˆ
            preview_match = re.search(r'class="[^"]*preview[^"]*"[^>]*>(.*?)</', html_snippet, re.DOTALL | re.IGNORECASE)
            preview = re.sub(r'<[^>]+>', '', preview_match.group(1)).strip() if preview_match else ""
            
            # æå–URL
            url_match = re.search(r'href="([^"]+)"', html_snippet)
            url = url_match.group(1) if url_match else ""
            
            # æå–æ–‡ç« ID
            doc_match = re.search(r'doc=([^&"\s]+)', html_snippet)
            doc_id = unquote(doc_match.group(1)) if doc_match else None
            
            word_count = len(preview.split()) if preview else 0
            
            if title:
                return ArticleInfo(
                    title=title[:300],
                    date=date[:100],
                    source=source[:200],
                    author="",
                    preview=preview[:1000],
                    url=url[:500],
                    page_num=page_num,
                    article_id=doc_id,
                    word_count=word_count
                )
            
            return None
            
        except Exception:
            return None
    
    async def scan_all_pages(self, page: Page, base_url: str, keyword: Optional[str] = None) -> List[ArticleInfo]:
        """
        æ‰«ææ‰€æœ‰é¡µé¢çš„æ–‡ç« 
        
        æ–°æµç¨‹ï¼š
        1. ç¿»å®Œæ‰€æœ‰é¡µï¼Œåªæ”¶é›†å…ƒæ•°æ®ï¼ˆä¸è°ƒç”¨ä¸‹è½½APIï¼‰
        2. ç­›é€‰ news/
        3. ä¿å­˜å…ƒæ•°æ®åˆ°JSONï¼ˆç”¨æˆ·å¯é€‰æ‹©ç¨åä¸‹è½½ï¼‰
        4. ç”¨æˆ·ç¡®è®¤
        5. è°ƒç”¨ä¸‹è½½API
        """
        print("\n" + "=" * 70)
        print("å¼€å§‹æ‰«ææ–‡ç« åˆ—è¡¨")
        print("=" * 70)
        
        # å¦‚æœæ²¡æœ‰æä¾›å…³é”®å­—ï¼Œå°è¯•ä»URLä¸­æå–
        if not keyword:
            try:
                parsed = urlparse(base_url)
                query_params = parse_qs(parsed.query)
                keyword = query_params.get('val-base-0', ['unknown'])[0]
                # URLè§£ç 
                keyword = unquote(keyword)
            except:
                keyword = "unknown"
        
        all_metadata = []  # æ”¶é›†æ‰€æœ‰é¡µçš„å…ƒæ•°æ®
        current_url = base_url
        
        for page_num in range(1, self.max_pages + 1):
            # è·å–å½“å‰é¡µé¢çš„å…ƒæ•°æ®ï¼ˆä¸è°ƒç”¨ä¸‹è½½APIï¼‰
            page_metadata = await self.fetch_page_metadata_only(page, page_num)
            
            if not page_metadata:
                print(f"\n[ç¬¬ {page_num} é¡µ] æœªè·å–åˆ°å…ƒæ•°æ®ï¼Œç»“æŸæ‰«æ")
                break
            
            all_metadata.extend(page_metadata)
            self.stats["total_pages"] += 1
            
            print(f"\n[ç¬¬ {page_num} é¡µ] è·å–åˆ° {len(page_metadata)} æ¡å…ƒæ•°æ®")
            print(f"  [ç´¯è®¡] ç›®å‰å…± {len(all_metadata)} æ¡å…ƒæ•°æ®")
            
            # ç¿»åˆ°ä¸‹ä¸€é¡µ
            if page_num < self.max_pages:
                # æ„å»ºä¸‹ä¸€é¡µURL
                next_url = self._build_page_url(current_url, page_num + 1)
                print(f"  [ç¿»é¡µ] å‰å¾€ç¬¬ {page_num + 1} é¡µ...")
                try:
                    await page.goto(next_url, wait_until="networkidle", timeout=30000)
                    await asyncio.sleep(2)
                    current_url = next_url
                except Exception as e:
                    print(f"  [è­¦å‘Š] ç¿»é¡µå¤±è´¥: {e}")
                    break
        
        # ========== ç¿»é¡µå®Œæˆï¼Œå¼€å§‹å¤„ç† ==========
        print("\n" + "=" * 70)
        print(f"æ‰«æå®Œæˆï¼Œå…±è·å– {len(all_metadata)} æ¡å…ƒæ•°æ®")
        print("=" * 70)
        
        # æ­¥éª¤1: ç­›é€‰ news/ å¼€å¤´çš„è®°å½•
        filtered_metadata = [
            art for art in all_metadata 
            if art.get('docref', '').startswith('news/')
        ]
        
        if len(filtered_metadata) < len(all_metadata):
            removed_count = len(all_metadata) - len(filtered_metadata)
            print(f"[ç­›é€‰] è¿‡æ»¤æ‰ {removed_count} æ¡é news/ è®°å½•")
            print(f"[ç­›é€‰] ä¿ç•™ {len(filtered_metadata)} æ¡è®°å½•")
        
        if not filtered_metadata:
            print("[é”™è¯¯] æ²¡æœ‰æ‰¾åˆ°ä»»ä½• news/ å¼€å¤´çš„è®°å½•")
            return []
        
        # æ­¥éª¤2: ä¿å­˜å…ƒæ•°æ®åˆ°JSONï¼ˆç”¨æˆ·å¯é€‰æ‹©ç¨åä¸‹è½½ï¼‰
        print("\n[ä¿å­˜] æ­£åœ¨ä¿å­˜å…ƒæ•°æ®åˆ°JSON...")
        json_path = await self._save_article_metadata_to_json(all_metadata, keyword)
        print(f"[ä¿å­˜] å…ƒæ•°æ®å·²ä¿å­˜åˆ°: {json_path}")
        print(f"[æç¤º] å¦‚éœ€ç¨åä¸‹è½½ï¼Œå¯ä½¿ç”¨: python newsbank_api_downloader.py --from-metadata \"{json_path}\"")
        
        # æ­¥éª¤3: æ˜¾ç¤ºæ–‡ç« åˆ—è¡¨ä¾›ç”¨æˆ·é€‰æ‹©
        print(f"\n[æ–‡ç« åˆ—è¡¨] å…± {len(filtered_metadata)} ç¯‡æ–‡ç« :")
        for i, art in enumerate(filtered_metadata[:20], 1):
            title = art.get('title', 'N/A')[:50]
            docref = art.get('docref', 'N/A')
            size = art.get('size', 0)
            print(f"  {i:3}. [{size:>6} bytes] {title}")
        
        if len(filtered_metadata) > 20:
            print(f"  ... è¿˜æœ‰ {len(filtered_metadata) - 20} ç¯‡æ–‡ç« ")
        
        # æ­¥éª¤4: è®©ç”¨æˆ·é€‰æ‹©
        print(f"\nè¯·é€‰æ‹©è¦ä¸‹è½½çš„æ–‡ç« :")
        print("  - è¾“å…¥ 'all' ä¸‹è½½å…¨éƒ¨")
        print("  - è¾“å…¥æ•°å­—é€‰æ‹©ï¼ˆä¾‹å¦‚: 1,5,10 æˆ– 1-20ï¼‰")
        print("  - è¾“å…¥ 'cancel' å–æ¶ˆ")
        
        user_input = input("\nè¯·è¾“å…¥é€‰æ‹©: ").strip().lower()
        
        if user_input == 'cancel':
            print("å·²å–æ¶ˆä¸‹è½½")
            return []
        
        # å¤„ç†ç”¨æˆ·é€‰æ‹©
        if user_input == 'all':
            selected_metadata = filtered_metadata
        elif '-' in user_input:
            try:
                parts = user_input.split('-')
                start = int(parts[0].strip())
                end = int(parts[1].strip())
                selected_metadata = filtered_metadata[start-1:end]
                print(f"å·²é€‰æ‹©ç¬¬ {start} åˆ° {end} ç¯‡")
            except:
                print("è¾“å…¥æ ¼å¼é”™è¯¯ï¼Œå°†ä¸‹è½½å…¨éƒ¨")
                selected_metadata = filtered_metadata
        else:
            # å¤„ç†æ•°å­—åˆ—è¡¨
            try:
                nums = [int(x) for x in user_input.replace(',', ' ').split() if x.isdigit()]
                selected_metadata = [filtered_metadata[n-1] for n in nums if n <= len(filtered_metadata)]
                print(f"å·²é€‰æ‹© {len(selected_metadata)} ç¯‡æ–‡ç« ")
            except:
                print("è¾“å…¥æ ¼å¼é”™è¯¯ï¼Œå°†ä¸‹è½½å…¨éƒ¨")
                selected_metadata = filtered_metadata
        
        if not selected_metadata:
            print("æ²¡æœ‰é€‰æ‹©ä»»ä½•æ–‡ç« ")
            return []
        
        print(f"\n[ç¡®è®¤] å³å°†ä¸‹è½½ {len(selected_metadata)} ç¯‡æ–‡ç« ")
        
        # æ­¥éª¤5: è°ƒç”¨ä¸‹è½½API
        print("\n[ä¸‹è½½] å¼€å§‹ä¸‹è½½æ–‡ç« ...")
        p_param = "AWGLNB"  # é»˜è®¤å€¼
        
        response_data = await self._call_download_api_with_articles(page, selected_metadata, p_param)
        
        if not response_data or not response_data.get('body'):
            print("[é”™è¯¯] ä¸‹è½½APIè°ƒç”¨å¤±è´¥")
            return []
        
        # æ­¥éª¤5: è§£æå“åº”
        body = response_data['body']
        print(f"  [è°ƒè¯•] APIå“åº”é•¿åº¦: {len(body)} bytes")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯PDF
        if body.startswith('%PDF'):
            print("  [è­¦å‘Š] APIè¿”å›PDF")
            return []
        
        # è§£ææ–‡ç« 
        articles = self._parse_api_response(body, page_num=1)
        print(f"  [æˆåŠŸ] ä»APIå“åº”è§£æåˆ° {len(articles)} ç¯‡æ–‡ç« ")
        
        self.stats["total_articles"] = len(articles)
        return articles
    
    def display_articles(self, articles: List[ArticleInfo]):
        """æ˜¾ç¤ºæ–‡ç« åˆ—è¡¨"""
        print("\n" + "=" * 70)
        print(f"æ–‡ç« åˆ—è¡¨ (å…± {len(articles)} ç¯‡)")
        print("=" * 70)
        
        for i, article in enumerate(articles[:30], 1):
            quality = "âœ“" if article.word_count >= 30 else "â—‹"
            print(f"\n[{i:3d}] {quality} {article.title[:60]}...")
            print(f"      æ—¥æœŸ: {article.date} | æ¥æº: {article.source[:30]}")
            print(f"      é¢„è§ˆ: {article.word_count}è¯")
            if article.article_id:
                print(f"      ID: {article.article_id[:40]}...")
        
        if len(articles) > 30:
            print(f"\n... è¿˜æœ‰ {len(articles) - 30} ç¯‡æ–‡ç«  ...")
        
        print("=" * 70)
    
    async def download_article_full_text(self, page: Page, article: ArticleInfo) -> str:
        """ä¸‹è½½æ–‡ç« å®Œæ•´å†…å®¹"""
        if not article.url:
            return ""
        
        try:
            await page.goto(article.url, wait_until="networkidle", timeout=30000)
            await asyncio.sleep(2)
            
            # å°è¯•æå–å…¨æ–‡
            full_text = ""
            selectors = [
                '.document-view__body',
                '.gnus-doc__body',
                '.document-text',
                'article'
            ]
            
            for selector in selectors:
                elem = await page.query_selector(selector)
                if elem:
                    full_text = await elem.inner_text()
                    if len(full_text.strip()) > 100:
                        break
            
            # å¤‡é€‰æ–¹æ¡ˆ
            if not full_text:
                paragraphs = await page.query_selector_all('p')
                texts = []
                for p in paragraphs:
                    text = await p.inner_text()
                    if len(text.strip()) > 20:
                        texts.append(text)
                full_text = '\n\n'.join(texts)
            
            return full_text
            
        except Exception as e:
            print(f"    [é”™è¯¯] ä¸‹è½½å…¨æ–‡å¤±è´¥: {e}")
            return ""
    
    async def batch_download_full_text(self, page: Page, articles: List[ArticleInfo]) -> None:
        """æ‰¹é‡ä¸‹è½½æ–‡ç« å…¨æ–‡ - ä½¿ç”¨æ›´é«˜æ•ˆçš„å¹¶å‘æ–¹å¼"""
        articles_need_download = [a for a in articles if not a.full_text or len(a.full_text) < 100]
        
        if not articles_need_download:
            return
        
        print(f"\n[æ‰¹é‡ä¸‹è½½] éœ€è¦ä¸‹è½½ {len(articles_need_download)} ç¯‡æ–‡ç« çš„å…¨æ–‡...")
        
        for i, article in enumerate(articles_need_download, 1):
            print(f"  [{i}/{len(articles_need_download)}] ä¸‹è½½: {article.title[:40]}...")
            try:
                full_text = await self.download_article_full_text(page, article)
                if full_text:
                    article.full_text = full_text
                    article.word_count = len(full_text.split())
                    print(f"    âœ“ æˆåŠŸ ({len(full_text)} å­—ç¬¦)")
                else:
                    print(f"    âœ— æ— å†…å®¹")
            except Exception as e:
                print(f"    âœ— å¤±è´¥: {e}")
            
            # å»¶è¿Ÿé¿å…è¢«å°
            if i < len(articles_need_download):
                await asyncio.sleep(1)
        
        print(f"[æ‰¹é‡ä¸‹è½½] å®Œæˆ")

    async def save_articles(self, 
                           page: Page, 
                           articles: List[ArticleInfo],
                           base_url: str,
                           download_all: bool = False):
        """ä¿å­˜æ–‡ç«  - ä¼˜å…ˆä½¿ç”¨APIå“åº”ä¸­å·²è·å–çš„å…¨æ–‡"""
        print("\n" + "=" * 70)
        print("å¼€å§‹ä¿å­˜æ–‡ç« ")
        print("=" * 70)
        
        # ç»Ÿè®¡æœ‰å¤šå°‘ç¯‡æ–‡ç« å·²ç»æœ‰å…¨æ–‡
        articles_with_fulltext = sum(1 for a in articles if a.full_text and len(a.full_text) > 100)
        articles_without_fulltext = len(articles) - articles_with_fulltext
        
        if articles_with_fulltext == len(articles):
            print(f"[è¯´æ˜] æ‰€æœ‰ {len(articles)} ç¯‡æ–‡ç« å·²ä»APIè·å–å…¨æ–‡ï¼Œç›´æ¥ä¿å­˜")
        elif articles_with_fulltext > 0:
            print(f"[è¯´æ˜] {articles_with_fulltext}/{len(articles)} ç¯‡æ–‡ç« æœ‰å…¨æ–‡ï¼Œå…¶ä½™ {articles_without_fulltext} ç¯‡éœ€è¦è®¿é—®é¡µé¢è·å–")
        else:
            print(f"[è¯´æ˜] APIå“åº”ä¸­æ— å…¨æ–‡ï¼Œå…¨éƒ¨ {len(articles)} ç¯‡æ–‡ç« éœ€è¦è®¿é—®é¡µé¢è·å–")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯APIæ¨¡å¼æˆåŠŸè·å–çš„æ–‡ç« ï¼ˆæœ‰å…¨æ–‡ï¼‰
        api_success = articles_with_fulltext > 0
        
        if api_success:
            # APIæˆåŠŸè·å–æ–‡ç« ï¼Œç›´æ¥ä¿å­˜æ‰€æœ‰æ–‡ç« ï¼Œæ— éœ€ç¡®è®¤
            print(f"\n[APIæ¨¡å¼] æ£€æµ‹åˆ° {articles_with_fulltext} ç¯‡æœ‰å…¨æ–‡ï¼Œç›´æ¥ä¿å­˜æ‰€æœ‰ {len(articles)} ç¯‡æ–‡ç« ")
            selected = articles
        else:
            # APIæœªæˆåŠŸï¼Œç›´æ¥é€€å‡º
            print(f"\n[é”™è¯¯] APIæœªæˆåŠŸè·å–æ–‡ç« å…¨æ–‡ï¼Œé€€å‡º")
            import sys
            sys.exit(1)
        
        if not selected:
            print("æœªé€‰æ‹©ä»»ä½•æ–‡ç« ")
            return
        
        print(f"\nå°†ä¿å­˜ {len(selected)} ç¯‡æ–‡ç« ...")
        
        downloaded = 0
        
        # ç§»é™¤ä¸‹è½½é™åˆ¶ï¼Œä¿å­˜æ‰€æœ‰æ–‡ç« 
        for i, article in enumerate(selected, 1):
            print(f"\n[{i}/{len(selected)}] {article.title[:50]}...")
            
            try:
                # ä½¿ç”¨APIå“åº”ä¸­å·²è·å–çš„å†…å®¹
                full_text = article.full_text
                
                # å¦‚æœAPIæ²¡æœ‰æä¾›å…¨æ–‡ï¼Œä½†æœ‰é¢„è§ˆï¼Œä½¿ç”¨é¢„è§ˆ
                if (not full_text or len(full_text.strip()) < 50) and article.preview:
                    full_text = article.preview
                    print(f"    [ä¿¡æ¯] APIæœªæä¾›å…¨æ–‡ï¼Œä½¿ç”¨é¢„è§ˆå†…å®¹ ({len(full_text)} å­—ç¬¦)")
                
                if not full_text or len(full_text.strip()) < 50:
                    print(f"    [è·³è¿‡] æ— æœ‰æ•ˆå†…å®¹")
                    self.stats["skipped"] += 1
                    continue
                
                print(f"    [ä¿¡æ¯] å†…å®¹é•¿åº¦: {len(full_text)} å­—ç¬¦")
                
                # ä¿å­˜æ–‡ä»¶
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                safe_title = "".join(c for c in article.title[:50] if c.isalnum() or c in (' ', '-', '_')).strip()
                filename = f"{i:03d}_{timestamp}_{safe_title}.txt"
                filepath = self.output_dir / filename
                
                content = f"""Title: {article.title}
Date: {article.date}
Source: {article.source}
Author: {article.author}
URL: {article.url}
Article ID: {article.article_id}
Original Search URL: {base_url}
Downloaded at: {datetime.now().isoformat()}
Page: {article.page_num}
Word Count: {len(full_text.split())}

Preview:
{article.preview}

Full Text:
{full_text}

{'='*70}
"""
                
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                downloaded += 1
                self.stats["downloaded"] += 1
                print(f"    [æˆåŠŸ] å·²ä¿å­˜ ({len(full_text)} å­—ç¬¦) -> {filename}")
                
            except Exception as e:
                print(f"    [é”™è¯¯] ä¿å­˜å¤±è´¥: {e}")
                self.stats["errors"].append(f"{article.title}: {str(e)}")
                continue
            
            # å»¶è¿Ÿï¼Œé¿å…æ“ä½œè¿‡å¿«
            await asyncio.sleep(0.5)
        
        print(f"\n[å®Œæˆ] æˆåŠŸä¿å­˜ {downloaded} ç¯‡æ–‡ç« ")
    
    async def download_from_url(self, url: str, download_all: bool = False):
        """ä»URLä¸‹è½½æ–‡ç« çš„ä¸»æ–¹æ³•"""
        print("=" * 80)
        print("NewsBank API ä¸‹è½½å™¨")
        print("=" * 80)
        print(f"\nç›®æ ‡URL: {url[:80]}...")
        print(f"æœ€å¤§é¡µæ•°: {self.max_pages}")
        
        # å¯åŠ¨æµè§ˆå™¨
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=self.headless,
                args=['--disable-blink-features=AutomationControlled']
            )
            
            context = await browser.new_context(
                storage_state=str(self.cookie_file) if self.cookie_file.exists() else None,
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                accept_downloads=False  # é˜»æ­¢PDFä¸‹è½½å¼¹çª—
            )
            
            page = await context.new_page()
            
            # ç›‘å¬å¹¶å–æ¶ˆä¸‹è½½äº‹ä»¶
            async def handle_download(download):
                print(f"  [ä¿¡æ¯] é˜»æ­¢äº†ä¸‹è½½: {download.suggested_filename}")
                await download.cancel()
            
            page.on("download", lambda d: asyncio.create_task(handle_download(d)))
            
            try:
                # æ£€æŸ¥ç™»å½•
                if not await self.check_login(context):
                    if self.headless:
                        print("[é”™è¯¯] æ— å¤´æ¨¡å¼ä¸‹æ— æ³•ç™»å½•")
                        return
                    
                    if not await self.do_login(page):
                        return
                    
                    await context.storage_state(path=str(self.cookie_file))
                
                # è®¿é—®URL
                print(f"\n[è®¿é—®é¡µé¢]")
                await page.goto(url, wait_until="networkidle", timeout=60000)
                await asyncio.sleep(2)
                
                print(f"é¡µé¢æ ‡é¢˜: {await page.title()}")
                
                # æå–æœç´¢å…³é”®å­—
                parsed_url = urlparse(url)
                query_params = parse_qs(parsed_url.query)
                keyword = query_params.get('val-base-0', ['unknown'])[0]
                keyword = unquote(keyword)
                
                # æ‰«ææ‰€æœ‰é¡µé¢
                self.articles = await self.scan_all_pages(page, url, keyword)
                
                if not self.articles:
                    print("\n[è­¦å‘Š] æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« ")
                    return
                
                # æ˜¾ç¤ºæ–‡ç« åˆ—è¡¨
                self.display_articles(self.articles)
                
                # ä¿å­˜æ–‡ç« åˆ—è¡¨åˆ°JSONï¼ˆå¸¦ç­›é€‰ï¼‰
                articles_to_save = self.articles
                
                # å°è¯•ä»æ–‡ç« ä¸­æå– docref ä¿¡æ¯è¿›è¡Œç­›é€‰
                # ArticleInfo æœ‰ article_id å­—æ®µï¼Œå¯ä»¥æ„é€  docref
                articles_with_docref = []
                for a in self.articles:
                    if a.article_id:
                        # æ„é€  docref
                        docref = f"news/{a.article_id}"
                        articles_with_docref.append((a, docref))
                
                if articles_with_docref:
                    # ç­›é€‰åªä¿ç•™ docref ä»¥ "news/" å¼€å¤´çš„
                    filtered_articles = [a for a, docref in articles_with_docref if docref.startswith('news/')]
                    removed_count = len(self.articles) - len(filtered_articles)
                    
                    if removed_count > 0:
                        print(f"\n[ç­›é€‰] è¿‡æ»¤æ‰ {removed_count} æ¡é news/ å¼€å¤´çš„è®°å½•")
                        print(f"[ç­›é€‰] ä¿ç•™ {len(filtered_articles)} æ¡è®°å½•")
                        articles_to_save = filtered_articles
                
                json_path = self.output_dir / f"article_list_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump([a.to_dict() for a in articles_to_save], f, indent=2, ensure_ascii=False)
                print(f"\næ–‡ç« åˆ—è¡¨å·²ä¿å­˜: {json_path}")
                
                # ä¸‹è½½æ–‡ç« 
                await self.save_articles(page, self.articles, url, download_all)
                
                # æœ€ç»ˆæŠ¥å‘Š
                print("\n" + "=" * 80)
                print("ä¸‹è½½å®ŒæˆæŠ¥å‘Š")
                print("=" * 80)
                print(f"æ‰«æé¡µæ•°: {self.stats['total_pages']}")
                print(f"å‘ç°æ–‡ç« : {self.stats['total_articles']}")
                print(f"æˆåŠŸä¸‹è½½: {self.stats['downloaded']}")
                print(f"è·³è¿‡/å¤±è´¥: {self.stats['skipped']}")
                print(f"è¾“å‡ºç›®å½•: {self.output_dir.absolute()}")
                
                if self.stats["errors"]:
                    print(f"\né”™è¯¯ ({len(self.stats['errors'])}):")
                    for error in self.stats["errors"][:5]:
                        print(f"  - {error}")
                
                # æ‰“å°æµé‡æŠ¥å‘Š
                self.traffic_logger.print_stats()
                
                # ä¿å­˜æµé‡æ—¥å¿—
                self.traffic_logger.save_log()
                
                print("=" * 80)
                
                if not self.headless:
                    print("\n[INFO] æµè§ˆå™¨å°†ä¿æŒæ‰“å¼€10ç§’...")
                    await asyncio.sleep(10)
                
            except Exception as e:
                print(f"\n[é”™è¯¯] {e}")
                import traceback
                traceback.print_exc()
            
            finally:
                await context.close()
                await browser.close()
    
    async def extract_metadata_only(self, url: str):
        """
        ä»…æå–æ–‡ç« å…ƒæ•°æ®å¹¶ä¿å­˜åˆ°JSONï¼Œä¸ä¸‹è½½æ–‡ç« å†…å®¹
        
        ä¸¤é˜¶æ®µæ¨¡å¼ç¬¬ä¸€æ­¥ï¼š
        1. è®¿é—®æœç´¢é¡µé¢
        2. é€‰ä¸­æ‰€æœ‰æ–‡ç« 
        3. æå–å…ƒæ•°æ®å¹¶ä¿å­˜åˆ°JSON
        4. è®©ç”¨æˆ·é€‰æ‹©è¦ä¸‹è½½çš„æ–‡ç« 
        """
        print("=" * 80)
        print("å…ƒæ•°æ®æå–æ¨¡å¼")
        print("=" * 80)
        print(f"\nç›®æ ‡URL: {url[:80]}...")
        print(f"æœ€å¤§é¡µæ•°: {self.max_pages}")
        
        # å¯åŠ¨æµè§ˆå™¨
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=self.headless,
                args=['--disable-blink-features=AutomationControlled']
            )
            
            context = await browser.new_context(
                storage_state=str(self.cookie_file) if self.cookie_file.exists() else None,
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            )
            
            page = await context.new_page()
            
            try:
                # æ£€æŸ¥ç™»å½•
                if not await self.check_login(context):
                    if self.headless:
                        print("[é”™è¯¯] æ— å¤´æ¨¡å¼ä¸‹æ— æ³•ç™»å½•")
                        return
                    
                    if not await self.do_login(page):
                        return
                    
                    await context.storage_state(path=str(self.cookie_file))
                
                # è®¿é—®URL
                print(f"\n[è®¿é—®é¡µé¢]")
                await page.goto(url, wait_until="networkidle", timeout=60000)
                await asyncio.sleep(2)
                
                print(f"é¡µé¢æ ‡é¢˜: {await page.title()}")
                
                # æå–æ€»ç»“æœæ•°ï¼ˆä»ç¬¬ä¸€é¡µHTMLï¼‰
                html_content = await page.content()
                total_results = self._extract_total_results(html_content)
                if total_results:
                    print(f"æ€»ç»“æœæ•°: {total_results}")
                
                # ä¿å­˜åŸºç¡€URLç”¨äºåç»­åˆ†é¡µ
                base_search_url = url
                
                # æå–æœç´¢å…³é”®å­—
                parsed_url = urlparse(url)
                query_params = parse_qs(parsed_url.query)
                keyword = query_params.get('val-base-0', ['unknown'])[0]
                
                all_metadata = []
                
                # åˆå§‹åŒ– LLM å®¢æˆ·ç«¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                # æ–°æµç¨‹ï¼šå…ˆæ”¶é›†æ‰€æœ‰é¡µå…ƒæ•°æ®ï¼Œæœ€åç»Ÿä¸€ç­›é€‰
                llm_client = None
                llm_model = None
                use_llm_filter = os.getenv("LLM_FILTER_ENABLED", "").lower() == "true"
                llm_threshold = float(os.getenv("LLM_FILTER_THRESHOLD", "0.5"))
                
                if use_llm_filter and OPENAI_AVAILABLE:
                    llm_result = self._init_llm_client()
                    if llm_result:
                        llm_client, llm_model = llm_result
                        print(f"[LLM] å·²å¯ç”¨ï¼Œå°†åœ¨æ”¶é›†å®Œæ‰€æœ‰é¡µåç»Ÿä¸€ç­›é€‰ï¼Œé˜ˆå€¼: {llm_threshold}")
                    else:
                        print("[è­¦å‘Š] LLM åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä¸ä½¿ç”¨ç­›é€‰")
                
                # ========== æ–°æµç¨‹ï¼šé€é¡µè·å–å…ƒæ•°æ® ==========
                for page_num in range(1, self.max_pages + 1):
                    # ä½¿ç”¨æ–°å‡½æ•°è·å–å…ƒæ•°æ®ï¼ˆä¸ä¸‹è½½å†…å®¹ï¼‰
                    article_metadata = await self.fetch_page_metadata_only(page, page_num)
                    
                    if not article_metadata:
                        print(f"  [ç¬¬ {page_num}] æ— æ³•è·å–å…ƒæ•°æ®ï¼Œåœæ­¢")
                        break
                    
                    # æ”¶é›†å…ƒæ•°æ®
                    all_metadata.extend(article_metadata)
                    print(f"  [ç´¯è®¡] å…±è·å– {len(all_metadata)} ç¯‡å…ƒæ•°æ®")
                    
                    # ä»URLè·å–på‚æ•°
                    parsed = urlparse(page.url)
                    query = parse_qs(parsed.query)
                    p_param = query.get('p', ['AWGLNB'])[0]
                    
                    # æ­¥éª¤3: remove_selection æ¸…é™¤é€‰æ‹©ï¼ˆç¿»é¡µå‰ï¼‰
                    print(f"  [æ¸…é™¤] ç¿»é¡µå‰æ¸…é™¤é€‰æ‹©...")
                    await self.remove_selection(page, p_param)
                    
                    # æ­¥éª¤4: ç¿»åˆ°ä¸‹ä¸€é¡µ
                    if page_num < self.max_pages:
                        next_button = await page.query_selector('a[data-testid="pager-next"], button[data-testid="pager-next"], a:has-text("Next"), a:has-text("â€º")')
                        if next_button:
                            next_url = self._build_page_url(base_search_url, page_num + 1)
                            print(f"  [ç¿»é¡µ] è®¿é—®ç¬¬ {page_num + 1} é¡µ...")
                            await page.goto(next_url, wait_until="networkidle", timeout=60000)
                            await asyncio.sleep(2)
                        else:
                            print(f"  [ä¿¡æ¯] æœªæ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œåœæ­¢")
                            break
                    
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°100ç¯‡ï¼ˆAPIé™åˆ¶ï¼‰
                    if len(all_metadata) >= 100:
                        print(f"  [ä¿¡æ¯] å·²è·å–100ç¯‡ï¼Œè¾¾åˆ°é™åˆ¶")
                        break
                
                # ========== æ”¶é›†å®Œæ‰€æœ‰é¡µåè¿›è¡Œå»é‡ ==========
                seen = set()
                unique_metadata = []
                for art in all_metadata:
                    docref = art.get('docref', '')
                    if docref not in seen:
                        seen.add(docref)
                        unique_metadata.append(art)
                
                print(f"\n[å®Œæˆ] å…±æå–åˆ° {len(unique_metadata)} ç¯‡å”¯ä¸€æ–‡ç« ")
                
                # ========== æ–°æµç¨‹ï¼šæ”¶é›†å®Œæ‰€æœ‰é¡µåï¼Œç»Ÿä¸€è¿›è¡ŒLLMç­›é€‰ ==========
                if use_llm_filter and llm_client and unique_metadata:
                    print(f"\n[LLM] å¼€å§‹ç»Ÿä¸€ç­›é€‰ {len(unique_metadata)} ç¯‡æ–‡ç« ...")
                    # æ‰¹é‡ç­›é€‰ï¼ˆæ¯æ‰¹20ç¯‡ï¼‰
                    batch_size = 20
                    filtered_metadata = []
                    total_batches = (len(unique_metadata) + batch_size - 1) // batch_size
                    
                    for batch_idx in range(total_batches):
                        start_idx = batch_idx * batch_size
                        end_idx = min(start_idx + batch_size, len(unique_metadata))
                        batch = unique_metadata[start_idx:end_idx]
                        
                        print(f"  [LLM] ç­›é€‰æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} ({start_idx+1}-{end_idx})...")
                        
                        filtered_batch = await self._filter_single_page_with_llm(
                            batch, keyword, llm_client, llm_model, llm_threshold
                        )
                        filtered_metadata.extend(filtered_batch)
                    
                    print(f"  [LLM] ç­›é€‰å®Œæˆ: {len(filtered_metadata)}/{len(unique_metadata)} ç¯‡ç›¸å…³æ–‡ç« ")
                    unique_metadata = filtered_metadata
                
                # ä¿å­˜åˆ°JSON
                json_path = await self._save_article_metadata_to_json(unique_metadata, keyword)
                
                # è®©ç”¨æˆ·é€‰æ‹©
                print("\n" + "=" * 80)
                confirm = input("æ˜¯å¦ç°åœ¨é€‰æ‹©è¦ä¸‹è½½çš„æ–‡ç« ? (y/n): ").strip().lower()
                
                if confirm == 'y':
                    selected_metadata = await self._prompt_user_to_select_articles(unique_metadata)
                    
                    if selected_metadata:
                        print(f"\n[å¼€å§‹] ä¸‹è½½é€‰ä¸­çš„ {len(selected_metadata)} ç¯‡æ–‡ç« ...")
                        # ä¸å…³é—­æµè§ˆå™¨ï¼Œç›´æ¥åœ¨åŒä¸€æµè§ˆå™¨ä¸­ä¸‹è½½
                        # è¿™æ ·å¯ä»¥ä¿æŒä¼šè¯çŠ¶æ€å’Œpayloadä¸€è‡´æ€§
                        await self.download_selected_articles_in_session(page, selected_metadata, self.output_dir, base_search_url)
                        return
                
                print(f"\n[å®Œæˆ] å…ƒæ•°æ®å·²ä¿å­˜åˆ°: {json_path}")
                print("å¦‚éœ€ä¸‹è½½ï¼Œè¯·è¿è¡Œ:")
                print(f"  python newsbank_api_downloader.py --from-metadata \"{json_path}\"")
                
            except Exception as e:
                print(f"\n[é”™è¯¯] {e}")
                import traceback
                traceback.print_exc()
            
            finally:
                await context.close()
                await browser.close()
    
    async def download_selected_articles(self, article_metadata: List[Dict[str, Any]], output_dir: str | Path):
        """
        ä¸‹è½½ç”¨æˆ·é€‰å®šçš„æ–‡ç« 
        
        Args:
            article_metadata: ç”¨æˆ·é€‰å®šçš„æ–‡ç« å…ƒæ•°æ®åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
        """
        print("\n" + "=" * 80)
        print("ä¸‹è½½é€‰å®šæ–‡ç« ")
        print("=" * 80)
        
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # å¯åŠ¨æµè§ˆå™¨
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=self.headless,
                args=['--disable-blink-features=AutomationControlled']
            )
            
            context = await browser.new_context(
                storage_state=str(self.cookie_file) if self.cookie_file.exists() else None,
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                accept_downloads=False
            )
            
            page = await context.new_page()
            
            # ç›‘å¬å¹¶å–æ¶ˆä¸‹è½½äº‹ä»¶
            async def handle_download(download):
                print(f"  [ä¿¡æ¯] é˜»æ­¢äº†ä¸‹è½½: {download.suggested_filename}")
                await download.cancel()
            
            page.on("download", lambda d: asyncio.create_task(handle_download(d)))
            
            try:
                # æ£€æŸ¥ç™»å½•
                if not await self.check_login(context):
                    if self.headless:
                        print("[é”™è¯¯] æ— å¤´æ¨¡å¼ä¸‹æ— æ³•ç™»å½•")
                        return
                    
                    if not await self.do_login(page):
                        return
                    
                    await context.storage_state(path=str(self.cookie_file))
                
                # è®¿é—®ä»»æ„NewsBanké¡µé¢ä»¥è·å–session
                await page.goto(
                    "https://infoweb-newsbank-com.ezproxy.sl.nsw.gov.au/apps/news/results?p=AWGLNB",
                    wait_until="networkidle", timeout=30000
                )
                
                # è·å–på‚æ•°
                p_param = "AWGLNB"
                
                # è°ƒç”¨APIä¸‹è½½é€‰å®šçš„æ–‡ç« 
                print(f"\n[è°ƒç”¨API] ä¸‹è½½ {len(article_metadata)} ç¯‡æ–‡ç« ...")
                response_data = await self._call_download_api_with_articles(page, article_metadata, p_param)
                
                if response_data and response_data.get('body'):
                    body = response_data['body']
                    print(f"  [è°ƒè¯•] APIå“åº”é•¿åº¦: {len(body)} bytes")
                    
                    # è§£ææ–‡ç« 
                    articles = self._parse_api_response(body, page_num=1)
                    print(f"  [æˆåŠŸ] è§£æåˆ° {len(articles)} ç¯‡æ–‡ç« ")
                    
                    # ä¿å­˜æ–‡ç« 
                    await self.save_articles(page, articles, "", download_all=True)
                else:
                    print("[é”™è¯¯] APIè°ƒç”¨å¤±è´¥")
                
            except Exception as e:
                print(f"\n[é”™è¯¯] {e}")
                import traceback
                traceback.print_exc()
            
            finally:
                await context.close()
                await browser.close()

    async def download_selected_articles_in_session(self, 
                                                    page: Page, 
                                                    article_metadata: List[Dict[str, Any]], 
                                                    output_dir: str | Path,
                                                    base_url: Optional[str] = None):
        """
        åœ¨ç°æœ‰æµè§ˆå™¨ä¼šè¯ä¸­ä¸‹è½½ç”¨æˆ·é€‰å®šçš„æ–‡ç« ï¼ˆä¸å…³é—­æµè§ˆå™¨ï¼‰
        
        è¿™æ ·å¯ä»¥ä¿æŒä¼šè¯çŠ¶æ€ï¼Œä½¿ç”¨æ­£ç¡®çš„payloadæ ¼å¼
        
        Args:
            page: ç°æœ‰çš„Playwrighté¡µé¢å¯¹è±¡
            article_metadata: ç”¨æˆ·é€‰å®šçš„æ–‡ç« å…ƒæ•°æ®åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            base_url: æœç´¢ç»“æœé¡µURLï¼ˆç”¨äºé‡æ–°è®¿é—®ï¼‰
        """
        print("\n" + "=" * 80)
        print("ä¸‹è½½é€‰å®šæ–‡ç« ï¼ˆä¼šè¯å†…ï¼‰")
        print("=" * 80)
        
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            # å¦‚æœæ²¡æœ‰æä¾›base_urlï¼Œå°è¯•ä»å½“å‰URLè·å–
            if not base_url:
                base_url = page.url
                
            # è·å–på‚æ•°
            parsed = urlparse(base_url)
            query = parse_qs(parsed.query)
            p_param = query.get('p', ['AWGLNB'])[0]
            
            print(f"\n[å‡†å¤‡] ä½¿ç”¨ç°æœ‰æµè§ˆå™¨ä¼šè¯ä¸‹è½½ {len(article_metadata)} ç¯‡æ–‡ç« ...")
            print(f"  [ä¼šè¯] ä¿æŒç™»å½•çŠ¶æ€")
            print(f"  [å‚æ•°] p={p_param}")
            
            # å¯¼èˆªåˆ°æœç´¢ç»“æœç¬¬ä¸€é¡µï¼ˆç¡®ä¿åœ¨æ­£ç¡®çš„é¡µé¢çŠ¶æ€ï¼‰
            first_page_url = self._build_page_url(base_url, 1)
            print(f"\n[å¯¼èˆª] å›åˆ°æœç´¢ç»“æœç¬¬ä¸€é¡µ...")
            await page.goto(first_page_url, wait_until="networkidle", timeout=60000)
            await asyncio.sleep(2)
            print(f"  [æˆåŠŸ] å½“å‰URL: {page.url[:60]}...")
            
            # æ–¹æ³•1: ç›´æ¥ä½¿ç”¨APIè°ƒç”¨ï¼ˆä¸fetch_articles_via_apiç›¸åŒçš„æ–¹å¼ï¼‰
            # è¿™æ˜¯æœ€å¯é çš„æ–¹å¼ï¼Œä½¿ç”¨ä¸é¡µé¢äº¤äº’æ—¶ç›¸åŒçš„payloadæ ¼å¼
            print(f"\n[è°ƒç”¨API] ä¸‹è½½ {len(article_metadata)} ç¯‡æ–‡ç« ...")
            response_data = await self._call_download_api_with_articles(page, article_metadata, p_param)
            
            if response_data and response_data.get('body'):
                body = response_data['body']
                print(f"  [è°ƒè¯•] APIå“åº”é•¿åº¦: {len(body)} bytes")
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯PDF
                if body.startswith('%PDF'):
                    print("  [è­¦å‘Š] APIè¿”å›PDFæ ¼å¼")
                
                # è§£ææ–‡ç« 
                articles = self._parse_api_response(body, page_num=1)
                print(f"  [æˆåŠŸ] è§£æåˆ° {len(articles)} ç¯‡æ–‡ç« ")
                
                # ä¿å­˜æ–‡ç« 
                await self.save_articles(page, articles, "", download_all=True)
            else:
                print("[é”™è¯¯] APIè°ƒç”¨å¤±è´¥ï¼Œå°è¯•å¤‡é€‰æ–¹æ¡ˆ...")
                # å¤‡é€‰æ–¹æ¡ˆï¼šé€ä¸ªè®¿é—®æ–‡ç« é¡µé¢è·å–å…¨æ–‡
                await self.download_articles_one_by_one(page, article_metadata)
            
        except Exception as e:
            print(f"\n[é”™è¯¯] {e}")
            import traceback
            traceback.print_exc()
    
    async def download_articles_one_by_one(self, page: Page, article_metadata: List[Dict[str, Any]]):
        """
        å¤‡é€‰æ–¹æ¡ˆï¼šé€ä¸ªè®¿é—®æ–‡ç« é¡µé¢è·å–å…¨æ–‡
        å½“APIè°ƒç”¨å¤±è´¥æ—¶ä½¿ç”¨æ­¤æ–¹æ³•
        """
        print("\n[å¤‡é€‰æ–¹æ¡ˆ] é€ä¸ªè®¿é—®æ–‡ç« é¡µé¢è·å–å…¨æ–‡...")
        
        articles = []
        for i, meta in enumerate(article_metadata, 1):
            try:
                docref = meta.get('docref', '')
                title = meta.get('title', 'Unknown')[:50]
                print(f"\n  [{i}/{len(article_metadata)}] è·å–: {title}...")
                
                # ä»docrefæ„é€ URL
                if docref.startswith('news/'):
                    article_id = docref.replace('news/', '')
                    article_url = f"https://infoweb-newsbank-com.ezproxy.sl.nsw.gov.au/apps/news/document-view?p=AWGLNB&doc={article_id}"
                else:
                    print(f"    [è·³è¿‡] æ— æ•ˆçš„docref: {docref}")
                    continue
                
                # è®¿é—®æ–‡ç« é¡µé¢
                await page.goto(article_url, wait_until="networkidle", timeout=30000)
                await asyncio.sleep(2)
                
                # æå–å…¨æ–‡
                full_text = ""
                selectors = [
                    '.document-view__body',
                    '.gnus-doc__body',
                    '.document-text',
                    'article'
                ]
                
                for selector in selectors:
                    elem = await page.query_selector(selector)
                    if elem:
                        full_text = await elem.inner_text()
                        if len(full_text.strip()) > 100:
                            break
                
                # å¤‡é€‰æ–¹æ¡ˆ
                if not full_text:
                    paragraphs = await page.query_selector_all('p')
                    texts = []
                    for p in paragraphs:
                        text = await p.inner_text()
                        if len(text.strip()) > 20:
                            texts.append(text)
                    full_text = '\n\n'.join(texts)
                
                if full_text:
                    article = ArticleInfo(
                        title=meta.get('title', 'Unknown')[:300],
                        date=meta.get('date', ''),
                        source=meta.get('source', ''),
                        author=meta.get('author', ''),
                        preview=full_text[:1000],
                        url=article_url,
                        page_num=1,
                        article_id=article_id if 'article_id' in dir() else None,
                        word_count=len(full_text.split()),
                        full_text=full_text
                    )
                    articles.append(article)
                    print(f"    [æˆåŠŸ] {len(full_text)} å­—ç¬¦")
                else:
                    print(f"    [å¤±è´¥] æ— å†…å®¹")
                
            except Exception as e:
                print(f"    [é”™è¯¯] {e}")
                continue
            
            # å»¶è¿Ÿ
            if i < len(article_metadata):
                await asyncio.sleep(1)
        
        if articles:
            print(f"\n[å®Œæˆ] è·å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
            await self.save_articles(page, articles, "", download_all=True)
        else:
            print("\n[é”™è¯¯] æœªèƒ½è·å–ä»»ä½•æ–‡ç« ")


def main():
    parser = argparse.ArgumentParser(
        description="NewsBank API ä¸‹è½½å™¨ - é€šè¿‡APIè·å–æ–‡ç« å†…å®¹",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨æ–¹æ³•ç¤ºä¾‹:

1. ä½¿ç”¨æœç´¢å…³é”®å­—:
   python newsbank_api_downloader.py "treasury wine penfolds" --max-results 200

2. æŒ‡å®šå¹´ä»½èŒƒå›´:
   python newsbank_api_downloader.py "treasury wine" --year-from 2014 --year-to 2020

3. æŒ‡å®šæ•°æ®æº:
   python newsbank_api_downloader.py "treasury wine" --source "Australian Financial Review Collection"

4. ä½¿ç”¨å®Œæ•´URL:
   python newsbank_api_downloader.py "https://infoweb-newsbank-com.ezproxy.sl.nsw.gov.au/apps/news/results?..."

5. é™åˆ¶é¡µæ•°å’Œä¸‹è½½æ•°:
   python newsbank_api_downloader.py "treasury wine" --max-pages 5 --max-results 100

6. æ— å¤´æ¨¡å¼:
   python newsbank_api_downloader.py "treasury wine" --headless

7. ä»…æå–å…ƒæ•°æ®ï¼ˆä¸¤é˜¶æ®µæ¨¡å¼ï¼‰:
   python newsbank_api_downloader.py "treasury wine" --metadata-only

8. ä»å…ƒæ•°æ®æ–‡ä»¶åŠ è½½å¹¶é€‰æ‹©ä¸‹è½½:
   python newsbank_api_downloader.py --from-metadata "article_metadata_xxx.json"

9. è®¾ç½®å•æ¬¡ä¸‹è½½æœ€å¤§æ•°é‡:
   python newsbank_api_downloader.py --from-metadata "xxx.json" --max-download 10

10. ä½¿ç”¨ LLM ç­›é€‰æ–‡ç« ç›¸å…³æ€§:
   python newsbank_api_downloader.py --filter-from "article_treasury_wine_xxx.json"

11. LLM ç­›é€‰å¹¶æŒ‡å®šé˜ˆå€¼:
   python newsbank_api_downloader.py --filter-from "xxx.json" --threshold 0.7

12. LLM ç­›é€‰å¹¶æŒ‡å®šæ¨¡å‹:
   python newsbank_api_downloader.py --filter-from "xxx.json" --llm-model "z-ai/glm4.7"
        """
    )
    
    parser.add_argument("keyword_or_url", nargs='?', default=None,
                       help="æœç´¢å…³é”®å­—æˆ–NewsBankæœç´¢URL (LLMç­›é€‰æ¨¡å¼æ—¶å¯é€‰)")
    
    parser.add_argument("--max-results", type=int, default=200,
                       help="æœ€å¤§ç»“æœæ•° (é»˜è®¤: 200)")
    
    parser.add_argument("--max-pages", type=int, default=10,
                       help="æœ€å¤§æ‰«æé¡µæ•° (é»˜è®¤: 10)")
    
    parser.add_argument("--year-from", type=int, default=None,
                       help="èµ·å§‹å¹´ä»½ (ä¾‹å¦‚: 2014)")
    
    parser.add_argument("--year-to", type=int, default=None,
                       help="ç»“æŸå¹´ä»½ (ä¾‹å¦‚: 2020)")
    
    parser.add_argument("--source", type=str, default="Australian Financial Review Collection",
                       help="æ•°æ®æºåç§° (é»˜è®¤: Australian Financial Review Collection)")
    
    parser.add_argument("--headless", action="store_true",
                       help="æ— å¤´æ¨¡å¼")
    
    parser.add_argument("--delay", type=float, default=3.0,
                       help="è¯·æ±‚é—´éš”ç§’æ•°ï¼Œé˜²æ­¢è¢«å° (é»˜è®¤: 3.0)")
    
    parser.add_argument("--output-dir", default="articles_api",
                       help="è¾“å‡ºç›®å½• (é»˜è®¤: articles_api)")
    
    # æ–°å¢ï¼šä¸¤é˜¶æ®µæ¨¡å¼å‚æ•°
    parser.add_argument("--metadata-only", action="store_true",
                       help="ä»…æå–æ–‡ç« å…ƒæ•°æ®å¹¶ä¿å­˜åˆ°JSONï¼Œä¸ä¸‹è½½æ–‡ç« å†…å®¹")
    
    parser.add_argument("--from-metadata", type=str, default=None,
                       help="ä»å·²ä¿å­˜çš„å…ƒæ•°æ®JSONæ–‡ä»¶åŠ è½½å¹¶è®©ç”¨æˆ·é€‰æ‹©ä¸‹è½½")
    
    parser.add_argument("--max-download", type=int, default=20,
                       help="å•æ¬¡ä¸‹è½½æœ€å¤§æ–‡ç« æ•° (é»˜è®¤: 20ï¼Œé˜²æ­¢æœåŠ¡å™¨é™åˆ¶)")
    
    # LLM ç­›é€‰å‚æ•°
    parser.add_argument("--filter-llm", action="store_true",
                       help="ä½¿ç”¨ LLM ç­›é€‰ç›¸å…³æ–‡ç« ")
    
    parser.add_argument("--filter-from", type=str, default=None,
                       help="ä»å·²ä¿å­˜çš„ article_*.json æ–‡ä»¶è¿›è¡Œ LLM ç­›é€‰")
    
    parser.add_argument("--api-key", type=str, default=None,
                       help="LLM API å¯†é’¥ (é»˜è®¤ä»ç¯å¢ƒå˜é‡ NVIDIA_API_KEY æˆ– OPENAI_API_KEY è¯»å–)")
    
    parser.add_argument("--llm-model", type=str, default=None,
                       help="LLM æ¨¡å‹åç§° (é»˜è®¤: z-ai/glm4.7 for NVIDIA, gpt-3.5-turbo for OpenAI)")
    
    parser.add_argument("--threshold", type=float, default=0.5,
                       help="LLM ç›¸å…³æ€§é˜ˆå€¼ (0-1, é»˜è®¤: 0.5)")
    
    parser.add_argument("--batch-size", type=int, default=10,
                       help="LLM æ¯æ‰¹æ¬¡å¤„ç†çš„æ–‡ç« æ•° (é»˜è®¤: 10)")
    
    args = parser.parse_args()
    
    print(f"[æ³¨æ„] è¯·æ±‚é—´éš”: {args.delay} ç§’ (é˜²æ­¢è¢«å°)")
    
    # åˆ›å»ºä¸‹è½½å™¨å®ä¾‹
    downloader = NewsBankAPIDownloader(
        headless=args.headless,
        max_pages=args.max_pages,
        output_dir=args.output_dir,
        request_delay=args.delay
    )
    
    # æ¨¡å¼2: ä»å…ƒæ•°æ®æ–‡ä»¶åŠ è½½å¹¶ä¸‹è½½
    if args.from_metadata:
        json_path = Path(args.from_metadata)
        if not json_path.exists():
            print(f"[é”™è¯¯] æ–‡ä»¶ä¸å­˜åœ¨: {json_path}")
            return
        
        # åŠ è½½å…ƒæ•°æ®
        article_metadata = downloader._load_article_metadata_from_json(json_path)
        if not article_metadata:
            print("[é”™è¯¯] åŠ è½½å…ƒæ•°æ®å¤±è´¥")
            return
        
        # è®©ç”¨æˆ·é€‰æ‹©
        selected_metadata = asyncio.run(downloader._prompt_user_to_select_articles(
            article_metadata, max_download=args.max_download
        ))
        
        if not selected_metadata:
            print("å·²å–æ¶ˆä¸‹è½½")
            return
        
        # ä¸‹è½½é€‰ä¸­çš„æ–‡ç« 
        print(f"\n[å¼€å§‹] ä¸‹è½½é€‰ä¸­çš„ {len(selected_metadata)} ç¯‡æ–‡ç« ...")
        asyncio.run(downloader.download_selected_articles(selected_metadata, args.output_dir))
        return
    
    # LLM ç­›é€‰æ¨¡å¼
    if args.filter_from:
        print("\n[æ¨¡å¼] LLM æ™ºèƒ½ç­›é€‰æ¨¡å¼")
        print("=" * 50)
        
        json_path = Path(args.filter_from)
        if not json_path.exists():
            print(f"[é”™è¯¯] æ–‡ä»¶ä¸å­˜åœ¨: {json_path}")
            return
        
        # è¿è¡Œ LLM ç­›é€‰
        result = asyncio.run(downloader._filter_articles_by_llm(
            json_file=json_path,
            api_key=args.api_key,
            model=args.llm_model,
            threshold=args.threshold,
            batch_size=args.batch_size
        ))
        
        if result:
            print(f"\n[å®Œæˆ] LLM ç­›é€‰å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°: {result}")
            print("\nå¯ä»¥ä½¿ç”¨ --from-metadata å‚æ•°ä¸‹è½½ç­›é€‰åçš„æ–‡ç« :")
            print(f"  python newsbank_api_downloader.py --from-metadata \"{result}\"")
        else:
            print("[é”™è¯¯] LLM ç­›é€‰å¤±è´¥")
        return
    
    # åˆ¤æ–­è¾“å…¥æ˜¯å…³é”®å­—è¿˜æ˜¯URL
    if downloader._is_search_keyword(args.keyword_or_url):
        # æ˜¯å…³é”®å­—ï¼Œæ„å»ºæœç´¢URL
        print(f"\n[ä¿¡æ¯] æ£€æµ‹ä¸ºæœç´¢å…³é”®å­—: {args.keyword_or_url}")
        search_url = downloader._build_search_url(
            keyword=args.keyword_or_url,
            maxresults=args.max_results,
            source=args.source,
            year_from=args.year_from,
            year_to=args.year_to
        )
        print(f"[ä¿¡æ¯] ç”Ÿæˆçš„æœç´¢URL:")
        print(f"  {search_url}")
        url = search_url
    else:
        # æ˜¯URLï¼Œç›´æ¥ä½¿ç”¨
        url = args.keyword_or_url
        print(f"\n[ä¿¡æ¯] ä½¿ç”¨æä¾›çš„URL: {url}")
    
    # æ¨¡å¼1: ä»…æå–å…ƒæ•°æ®
    if args.metadata_only:
        print("\n[æ¨¡å¼] å…ƒæ•°æ®æå–æ¨¡å¼")
        print("=" * 50)
        asyncio.run(downloader.extract_metadata_only(url))
        return
    
    # æ­£å¸¸ä¸‹è½½æ¨¡å¼
    asyncio.run(downloader.download_from_url(url, download_all=True))


if __name__ == "__main__":
    exit(main())
