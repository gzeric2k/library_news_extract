# Playwright ç½‘ç«™ç™»å½•å’Œæ–‡ç« æŠ“å–ä»£ç ç¤ºä¾‹æ±‡æ€»

## 1. Cookie/Session ä¿å­˜å’ŒåŠ è½½

### 1.1 ä¿å­˜ Cookie åˆ°æ–‡ä»¶
```python
# æ¥æº: dreammis/social-auto-upload
import asyncio
from pathlib import Path
from playwright.async_api import async_playwright

async def save_cookies():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        context = await browser.new_context()
        page = await context.new_page()
        
        # æ‰‹åŠ¨ç™»å½•ï¼ˆæš‚åœé¡µé¢ç­‰å¾…ç”¨æˆ·æ“ä½œï¼‰
        await page.goto("https://creator.douyin.com/")
        await page.pause()  # è°ƒè¯•å™¨æš‚åœï¼Œç­‰å¾…æ‰‹åŠ¨ç™»å½•
        
        # ä¿å­˜ cookie åˆ°æ–‡ä»¶
        cookies_dir = Path("cookiesFile")
        cookies_dir.mkdir(exist_ok=True)
        await context.storage_state(path=cookies_dir / "douyin_cookies.json")
        print("âœ… Cookie å·²ä¿å­˜")
        
        await context.close()
        await browser.close()
```

### 1.2 ä»æ–‡ä»¶åŠ è½½ Cookie å¹¶ä½¿ç”¨
```python
# æ¥æº: dreammis/social-auto-upload
async def use_saved_cookies(account_file):
    async with async_playwright() as playwright:
        browser = await playwright.chromium.launch(headless=True)
        
        # ä½¿ç”¨ä¿å­˜çš„ storage_stateï¼ˆåŒ…å« cookies å’Œ localStorageï¼‰
        context = await browser.new_context(storage_state=account_file)
        page = await context.new_page()
        
        # ç›´æ¥è®¿é—®éœ€è¦ç™»å½•çš„é¡µé¢
        await page.goto("https://creator.douyin.com/creator-micro/content/upload")
        
        # éªŒè¯ç™»å½•çŠ¶æ€
        try:
            await page.wait_for_url(
                "https://creator.douyin.com/creator-micro/content/upload", 
                timeout=5000
            )
            print("âœ… Cookie æœ‰æ•ˆï¼Œå·²ç™»å½•")
        except:
            print("âŒ Cookie å¤±æ•ˆï¼Œéœ€è¦é‡æ–°ç™»å½•")
        
        await context.close()
        await browser.close()
```

### 1.3 æ›´æ–°å’Œä¿å­˜ Cookie
```python
# æ¥æº: dreammis/social-auto-upload
async def update_cookies_after_action(context, account_file):
    """åœ¨æ‰§è¡Œæ“ä½œåæ›´æ–° cookie"""
    # æ‰§è¡ŒæŸäº›æ“ä½œ...
    await page.click("button.publish")
    await page.wait_for_load_state("networkidle")
    
    # æ›´æ–°ä¿å­˜çš„ cookie
    await context.storage_state(path=account_file)
    print("âœ… Cookie å·²æ›´æ–°")
```

---

## 2. é¡µé¢å¯¼èˆªå’Œç­‰å¾…ç­–ç•¥

### 2.1 åŸºç¡€å¯¼èˆª
```python
# æ¥æº: NanmiCoder/CrawlerTutorial
async def navigate_page():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        
        # åŸºæœ¬å¯¼èˆª
        await page.goto("https://example.com")
        print(f"é¡µé¢æ ‡é¢˜: {await page.title()}")
        print(f"å½“å‰ URL: {page.url}")
        
        await browser.close()
```

### 2.2 ç­‰å¾…ç­–ç•¥è¯¦è§£
```python
# æ¥æº: NanmiCoder/CrawlerTutorial
async def wait_strategies():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        
        # 1. ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆï¼ˆæ¨èç”¨äºæ–°é—»ç½‘ç«™ï¼‰
        await page.goto(
            "https://example.com",
            wait_until="networkidle"  # ç­‰å¾…ç½‘ç»œç©ºé—²
        )
        
        # 2. ç­‰å¾… DOM å†…å®¹åŠ è½½
        await page.goto(
            "https://example.com",
            wait_until="domcontentloaded"
        )
        
        # 3. ç­‰å¾…ç‰¹å®šå…ƒç´ å‡ºç°
        await page.wait_for_selector("div.article", timeout=10000)
        
        # 4. ç­‰å¾…é¡µé¢åŠ è½½çŠ¶æ€
        await page.wait_for_load_state("load", timeout=15000)
        await page.wait_for_load_state("networkidle", timeout=15000)
        
        # 5. ç­‰å¾… URL å˜åŒ–ï¼ˆç™»å½•åè·³è½¬ï¼‰
        await page.wait_for_url(
            "https://example.com/dashboard", 
            timeout=5000
        )
        
        await browser.close()
```

### 2.3 è‡ªåŠ¨ç­‰å¾…æœºåˆ¶
```python
# æ¥æº: NanmiCoder/CrawlerTutorial
async def auto_waiting():
    """Playwright çš„æ“ä½œä¼šè‡ªåŠ¨ç­‰å¾…å…ƒç´ å¯æ“ä½œ"""
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        
        await page.goto("https://quotes.toscrape.com/login")
        
        # ä»¥ä¸‹æ“ä½œä¼šè‡ªåŠ¨ç­‰å¾…ï¼š
        # - å…ƒç´ å­˜åœ¨äº DOM
        # - å…ƒç´ å¯è§
        # - å…ƒç´ ç¨³å®šï¼ˆä¸åœ¨åŠ¨ç”»ä¸­ï¼‰
        # - å…ƒç´ å¯æ¥æ”¶äº‹ä»¶
        # - å…ƒç´ æ²¡æœ‰è¢«å…¶ä»–å…ƒç´ é®æŒ¡
        
        await page.fill("input#username", "test")  # è‡ªåŠ¨ç­‰å¾…è¾“å…¥æ¡†å¯ç”¨
        await page.click("input[type='submit']")   # è‡ªåŠ¨ç­‰å¾…æŒ‰é’®å¯ç‚¹å‡»
        
        await browser.close()
```

---

## 3. è¡¨å•å¡«å……å’Œç™»å½•

### 3.1 ç™»å½•è¡¨å•å¡«å……
```python
# æ¥æº: NanmiCoder/CrawlerTutorial
async def login_form():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        
        await page.goto("https://quotes.toscrape.com/login")
        
        # å¡«å……ç”¨æˆ·å
        await page.fill("input#username", "test")
        
        # å¡«å……å¯†ç 
        await page.fill("input#password", "test")
        
        # ç‚¹å‡»ç™»å½•æŒ‰é’®
        await page.click("input[type='submit']")
        
        # ç­‰å¾…ç™»å½•å®Œæˆ
        await page.wait_for_load_state("networkidle")
        
        await browser.close()
```

### 3.2 å¤æ‚ç™»å½•æµç¨‹ï¼ˆå¸¦éªŒè¯ï¼‰
```python
# æ¥æº: s3rgeym/hh-applicant-tool
async def complex_login():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        page = await browser.new_page()
        
        await page.goto("https://example.com/login")
        
        # é€‰æ‹©ç™»å½•æ–¹å¼ï¼ˆé‚®ç®±æˆ–ç”µè¯ï¼‰
        username = "user@example.com"
        if "@" in username:
            # é€‰æ‹©é‚®ç®±ç™»å½•
            await page.check('input[value="email"]', force=True)
            await page.fill('input[name="email"]', username)
        else:
            # é€‰æ‹©ç”µè¯ç™»å½•
            await page.fill('input[name="phone"]', username)
        
        # ç‚¹å‡»å±•å¼€å¯†ç è¾“å…¥
        await page.click('button.expand-password')
        
        # ç­‰å¾…å¯†ç è¾“å…¥æ¡†å‡ºç°
        await page.wait_for_selector('input[name="password"]')
        
        # å¡«å……å¯†ç 
        await page.fill('input[name="password"]', "password123")
        
        # æäº¤è¡¨å•
        await page.click('button[type="submit"]')
        
        # ç­‰å¾…ç™»å½•å®Œæˆ
        await page.wait_for_load_state("networkidle", timeout=10000)
        
        await browser.close()
```

---

## 4. æ–‡ç« æŠ“å–ï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼‰

### 4.1 åŸºç¡€æ–‡ç« æŠ“å–
```python
# æ¥æº: ed-donner/agents
async def scrape_articles():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        
        await page.goto("https://news.example.com")
        await page.wait_for_load_state("networkidle")
        
        # å¤šä¸ªé€‰æ‹©å™¨å°è¯•ï¼ˆæé«˜å…¼å®¹æ€§ï¼‰
        selectors = [
            "div.article",
            "article",
            "div[role='article']",
            "div.news-item"
        ]
        
        articles = []
        for selector in selectors:
            elements = await page.query_selector_all(selector)
            if len(elements) > 5:  # æ‰¾åˆ°è¶³å¤Ÿçš„ç»“æœ
                for element in elements[:10]:
                    try:
                        title = await element.query_selector("h2, h3, .title")
                        if title:
                            text = await title.inner_text()
                            if text and len(text) > 10:
                                articles.append(text)
                    except:
                        continue
                break
        
        print(f"âœ… æŠ“å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
        for article in articles:
            print(f"  - {article[:50]}...")
        
        await browser.close()
        return articles
```

### 4.2 è¯¦ç»†æ–‡ç« ä¿¡æ¯æŠ“å–
```python
# æ¥æº: ç»¼åˆç¤ºä¾‹
async def scrape_article_details():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        
        await page.goto("https://news.example.com")
        await page.wait_for_load_state("networkidle")
        
        # è·å–æ‰€æœ‰æ–‡ç« å…ƒç´ 
        articles = await page.query_selector_all("div.article-item")
        
        article_list = []
        for article in articles:
            try:
                # æå–æ ‡é¢˜
                title_elem = await article.query_selector("h2.title")
                title = await title_elem.inner_text() if title_elem else "N/A"
                
                # æå–æ‘˜è¦
                summary_elem = await article.query_selector("p.summary")
                summary = await summary_elem.inner_text() if summary_elem else "N/A"
                
                # æå–å‘å¸ƒæ—¶é—´
                time_elem = await article.query_selector("span.publish-time")
                publish_time = await time_elem.inner_text() if time_elem else "N/A"
                
                # æå–é“¾æ¥
                link_elem = await article.query_selector("a.article-link")
                link = await link_elem.get_attribute("href") if link_elem else "N/A"
                
                article_list.append({
                    "title": title.strip(),
                    "summary": summary.strip(),
                    "publish_time": publish_time.strip(),
                    "link": link
                })
            except Exception as e:
                print(f"âŒ æŠ“å–æ–‡ç« å¤±è´¥: {e}")
                continue
        
        print(f"âœ… æŠ“å–åˆ° {len(article_list)} ç¯‡å®Œæ•´æ–‡ç« ")
        return article_list
```

---

## 5. ç¿»é¡µå¤„ç†

### 5.1 ç‚¹å‡»ä¸‹ä¸€é¡µæŒ‰é’®
```python
# æ¥æº: xnl-h4ck3r/xnldorker
async def pagination_click_next():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        
        await page.goto("https://news.example.com")
        
        all_articles = []
        page_num = 1
        
        while page_num <= 5:  # æœ€å¤šæŠ“å– 5 é¡µ
            print(f"ğŸ“„ æ­£åœ¨æŠ“å–ç¬¬ {page_num} é¡µ...")
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            await page.wait_for_load_state("networkidle")
            
            # æŠ“å–å½“å‰é¡µçš„æ–‡ç« 
            articles = await page.query_selector_all("div.article")
            for article in articles:
                title = await article.query_selector("h2")
                if title:
                    text = await title.inner_text()
                    all_articles.append(text)
            
            # æŸ¥æ‰¾å¹¶ç‚¹å‡»ä¸‹ä¸€é¡µæŒ‰é’®
            next_button = await page.query_selector("a.next-page, button.next")
            if not next_button:
                print("âœ… å·²åˆ°æœ€åä¸€é¡µ")
                break
            
            # æ£€æŸ¥ä¸‹ä¸€é¡µæŒ‰é’®æ˜¯å¦å¯ç”¨
            is_disabled = await next_button.get_attribute("disabled")
            if is_disabled:
                print("âœ… ä¸‹ä¸€é¡µæŒ‰é’®å·²ç¦ç”¨ï¼ŒæŠ“å–å®Œæˆ")
                break
            
            # ç‚¹å‡»ä¸‹ä¸€é¡µ
            await next_button.click()
            await page.wait_for_load_state("networkidle", timeout=10000)
            page_num += 1
        
        print(f"âœ… æ€»å…±æŠ“å– {len(all_articles)} ç¯‡æ–‡ç« ")
        await browser.close()
        return all_articles
```

### 5.2 æ»šåŠ¨åŠ è½½æ›´å¤šï¼ˆæ— é™æ»šåŠ¨ï¼‰
```python
# æ¥æº: ç»¼åˆç¤ºä¾‹
async def pagination_scroll_load():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        
        await page.goto("https://news.example.com")
        
        all_articles = []
        previous_height = 0
        scroll_count = 0
        max_scrolls = 10
        
        while scroll_count < max_scrolls:
            # è·å–å½“å‰é¡µé¢é«˜åº¦
            current_height = await page.evaluate("document.body.scrollHeight")
            
            if current_height == previous_height:
                print("âœ… å·²åˆ°åº•éƒ¨ï¼Œæ— æ›´å¤šå†…å®¹")
                break
            
            # æ»šåŠ¨åˆ°åº•éƒ¨
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            
            # ç­‰å¾…æ–°å†…å®¹åŠ è½½
            await page.wait_for_load_state("networkidle", timeout=5000)
            
            # æŠ“å–æ–‡ç« 
            articles = await page.query_selector_all("div.article")
            all_articles = []
            for article in articles:
                title = await article.query_selector("h2")
                if title:
                    text = await title.inner_text()
                    all_articles.append(text)
            
            previous_height = current_height
            scroll_count += 1
            print(f"ğŸ“„ æ»šåŠ¨ {scroll_count} æ¬¡ï¼Œå·²åŠ è½½ {len(all_articles)} ç¯‡æ–‡ç« ")
        
        print(f"âœ… æ€»å…±æŠ“å– {len(all_articles)} ç¯‡æ–‡ç« ")
        await browser.close()
        return all_articles
```

### 5.3 URL å‚æ•°ç¿»é¡µ
```python
# æ¥æº: ç»¼åˆç¤ºä¾‹
async def pagination_url_params():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        
        all_articles = []
        base_url = "https://news.example.com/list"
        
        for page_num in range(1, 6):  # æŠ“å– 1-5 é¡µ
            url = f"{base_url}?page={page_num}"
            print(f"ğŸ“„ æ­£åœ¨æŠ“å–: {url}")
            
            await page.goto(url)
            await page.wait_for_load_state("networkidle")
            
            # æŠ“å–æ–‡ç« 
            articles = await page.query_selector_all("div.article")
            if not articles:
                print("âœ… æ— æ›´å¤šæ–‡ç« ï¼ŒæŠ“å–å®Œæˆ")
                break
            
            for article in articles:
                title = await article.query_selector("h2")
                if title:
                    text = await title.inner_text()
                    all_articles.append(text)
            
            print(f"âœ… ç¬¬ {page_num} é¡µæŠ“å– {len(articles)} ç¯‡æ–‡ç« ")
        
        print(f"âœ… æ€»å…±æŠ“å– {len(all_articles)} ç¯‡æ–‡ç« ")
        await browser.close()
        return all_articles
```

---

## 6. åŠ¨æ€å†…å®¹åŠ è½½ç­‰å¾…

### 6.1 ç­‰å¾…ç‰¹å®šå…ƒç´ åŠ è½½
```python
# æ¥æº: agiresearch/AIOS
async def wait_for_dynamic_content():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        
        await page.goto("https://news.example.com")
        
        # ç­‰å¾…æ–‡ç« åˆ—è¡¨åŠ è½½
        try:
            await page.wait_for_selector(
                "div.article-list",
                state="attached",  # å…ƒç´ é™„åŠ åˆ° DOM
                timeout=10000
            )
            print("âœ… æ–‡ç« åˆ—è¡¨å·²åŠ è½½")
        except:
            print("âŒ æ–‡ç« åˆ—è¡¨åŠ è½½è¶…æ—¶")
        
        # ç­‰å¾…å…ƒç´ å¯è§
        try:
            await page.wait_for_selector(
                "div.article-item",
                state="visible",  # å…ƒç´ å¯è§
                timeout=10000
            )
            print("âœ… æ–‡ç« é¡¹å·²å¯è§")
        except:
            print("âŒ æ–‡ç« é¡¹åŠ è½½è¶…æ—¶")
        
        await browser.close()
```

### 6.2 ç­‰å¾…å›¾ç‰‡åŠ è½½
```python
# æ¥æº: unclecode/crawl4ai
async def wait_for_images():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        
        await page.goto("https://news.example.com")
        
        # ç­‰å¾… DOM å†…å®¹åŠ è½½
        try:
            await page.wait_for_load_state("domcontentloaded", timeout=5000)
        except:
            pass
        
        # ç­‰å¾…ç½‘ç»œç©ºé—²ï¼ˆæ‰€æœ‰å›¾ç‰‡åŠ è½½å®Œæˆï¼‰
        await page.wait_for_load_state("networkidle", timeout=15000)
        
        # æ£€æŸ¥å›¾ç‰‡æ˜¯å¦åŠ è½½
        images = await page.query_selector_all("img")
        print(f"âœ… é¡µé¢åŒ…å« {len(images)} å¼ å›¾ç‰‡")
        
        await browser.close()
```

### 6.3 ç­‰å¾… JavaScript æ‰§è¡Œå®Œæˆ
```python
# æ¥æº: ç»¼åˆç¤ºä¾‹
async def wait_for_javascript():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        
        await page.goto("https://news.example.com")
        
        # ç­‰å¾…ç‰¹å®šçš„ JavaScript å˜é‡å‡ºç°
        await page.wait_for_function(
            "() => window.articlesLoaded === true",
            timeout=10000
        )
        print("âœ… JavaScript æ‰§è¡Œå®Œæˆ")
        
        # æˆ–è€…ç­‰å¾…ç‰¹å®šå…ƒç´ çš„å±æ€§å˜åŒ–
        await page.wait_for_function(
            "() => document.querySelectorAll('.article').length > 0",
            timeout=10000
        )
        print("âœ… æ–‡ç« å·²åŠ è½½")
        
        await browser.close()
```

---

## 7. é”™è¯¯å¤„ç†å’Œé‡è¯•

### 7.1 åŸºç¡€é”™è¯¯å¤„ç†
```python
# æ¥æº: ç»¼åˆç¤ºä¾‹
async def error_handling():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        
        try:
            await page.goto("https://news.example.com", timeout=30000)
        except Exception as e:
            print(f"âŒ é¡µé¢åŠ è½½å¤±è´¥: {e}")
            await browser.close()
            return None
        
        try:
            await page.wait_for_load_state("networkidle", timeout=15000)
        except Exception as e:
            print(f"âš ï¸ ç½‘ç»œç­‰å¾…è¶…æ—¶: {e}")
            # ç»§ç»­æ‰§è¡Œï¼Œä¸ä¸­æ–­
        
        try:
            articles = await page.query_selector_all("div.article")
            print(f"âœ… æŠ“å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
        except Exception as e:
            print(f"âŒ æŠ“å–å¤±è´¥: {e}")
        finally:
            await browser.close()
```

### 7.2 é‡è¯•æœºåˆ¶
```python
# æ¥æº: ç»¼åˆç¤ºä¾‹
async def retry_mechanism():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        
        max_retries = 3
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                await page.goto("https://news.example.com", timeout=30000)
                await page.wait_for_load_state("networkidle", timeout=15000)
                print("âœ… é¡µé¢åŠ è½½æˆåŠŸ")
                break
            except Exception as e:
                retry_count += 1
                print(f"âš ï¸ åŠ è½½å¤±è´¥ (å°è¯• {retry_count}/{max_retries}): {e}")
                
                if retry_count < max_retries:
                    await page.reload()  # é‡æ–°åŠ è½½é¡µé¢
                    await asyncio.sleep(2)  # ç­‰å¾… 2 ç§’åé‡è¯•
                else:
                    print("âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒ")
                    await browser.close()
                    return None
        
        await browser.close()
```

---

## 8. å®Œæ•´ç¤ºä¾‹ï¼šNewsBank ç½‘ç«™æŠ“å–

```python
# -*- coding: utf-8 -*-
import asyncio
import json
from pathlib import Path
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout

class NewsExtractor:
    def __init__(self, cookies_file="cookies.json"):
        self.cookies_file = cookies_file
    
    async def login_and_save_cookies(self):
        """æ‰‹åŠ¨ç™»å½•å¹¶ä¿å­˜ cookies"""
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=False)
            context = await browser.new_context()
            page = await context.new_page()
            
            await page.goto("https://newsbank.example.com/login")
            print("â¸ï¸ è¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨ç™»å½•...")
            await page.pause()  # ç­‰å¾…æ‰‹åŠ¨ç™»å½•
            
            # ä¿å­˜ cookies
            await context.storage_state(path=self.cookies_file)
            print(f"âœ… Cookies å·²ä¿å­˜åˆ° {self.cookies_file}")
            
            await context.close()
            await browser.close()
    
    async def scrape_articles(self, url, max_pages=5):
        """æŠ“å–æ–‡ç« """
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(storage_state=self.cookies_file)
            page = await context.new_page()
            
            all_articles = []
            
            for page_num in range(1, max_pages + 1):
                try:
                    # æ„å»º URL
                    page_url = f"{url}?page={page_num}"
                    print(f"ğŸ“„ æ­£åœ¨æŠ“å–ç¬¬ {page_num} é¡µ: {page_url}")
                    
                    # å¯¼èˆªåˆ°é¡µé¢
                    await page.goto(page_url, timeout=30000)
                    await page.wait_for_load_state("networkidle", timeout=15000)
                    
                    # ç­‰å¾…æ–‡ç« åˆ—è¡¨åŠ è½½
                    await page.wait_for_selector("div.article-item", timeout=10000)
                    
                    # æŠ“å–æ–‡ç« 
                    articles = await page.query_selector_all("div.article-item")
                    
                    if not articles:
                        print("âœ… æ— æ›´å¤šæ–‡ç« ï¼ŒæŠ“å–å®Œæˆ")
                        break
                    
                    for article in articles:
                        try:
                            # æå–æ ‡é¢˜
                            title_elem = await article.query_selector("h2.title")
                            title = await title_elem.inner_text() if title_elem else "N/A"
                            
                            # æå–æ‘˜è¦
                            summary_elem = await article.query_selector("p.summary")
                            summary = await summary_elem.inner_text() if summary_elem else "N/A"
                            
                            # æå–å‘å¸ƒæ—¶é—´
                            time_elem = await article.query_selector("span.publish-time")
                            publish_time = await time_elem.inner_text() if time_elem else "N/A"
                            
                            # æå–é“¾æ¥
                            link_elem = await article.query_selector("a.article-link")
                            link = await link_elem.get_attribute("href") if link_elem else "N/A"
                            
                            all_articles.append({
                                "title": title.strip(),
                                "summary": summary.strip(),
                                "publish_time": publish_time.strip(),
                                "link": link,
                                "page": page_num
                            })
                        except Exception as e:
                            print(f"âš ï¸ æŠ“å–å•ç¯‡æ–‡ç« å¤±è´¥: {e}")
                            continue
                    
                    print(f"âœ… ç¬¬ {page_num} é¡µæŠ“å– {len(articles)} ç¯‡æ–‡ç« ")
                    
                except PlaywrightTimeout:
                    print(f"âš ï¸ ç¬¬ {page_num} é¡µåŠ è½½è¶…æ—¶")
                    continue
                except Exception as e:
                    print(f"âŒ ç¬¬ {page_num} é¡µæŠ“å–å¤±è´¥: {e}")
                    continue
            
            print(f"\nâœ… æ€»å…±æŠ“å– {len(all_articles)} ç¯‡æ–‡ç« ")
            
            await context.close()
            await browser.close()
            
            return all_articles
    
    async def save_articles(self, articles, output_file="articles.json"):
        """ä¿å­˜æ–‡ç« åˆ° JSON æ–‡ä»¶"""
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(articles, f, ensure_ascii=False, indent=2)
        print(f"âœ… æ–‡ç« å·²ä¿å­˜åˆ° {output_file}")

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    extractor = NewsExtractor(cookies_file="newsbank_cookies.json")
    
    # ç¬¬ä¸€æ¬¡è¿è¡Œï¼šç™»å½•å¹¶ä¿å­˜ cookies
    # await extractor.login_and_save_cookies()
    
    # æŠ“å–æ–‡ç« 
    articles = await extractor.scrape_articles(
        url="https://newsbank.example.com/search",
        max_pages=5
    )
    
    # ä¿å­˜ç»“æœ
    await extractor.save_articles(articles)

if __name__ == "__main__":
    asyncio.run(main())
```

---

## 9. å…³é”®è¦ç‚¹æ€»ç»“

### Cookie ç®¡ç†
- âœ… ä½¿ç”¨ `context.storage_state(path=file)` ä¿å­˜ cookies
- âœ… ä½¿ç”¨ `browser.new_context(storage_state=file)` åŠ è½½ cookies
- âœ… åœ¨æ‰§è¡Œæ“ä½œåæ›´æ–° cookies ä»¥ä¿æŒä¼šè¯æœ‰æ•ˆ

### ç­‰å¾…ç­–ç•¥
- âœ… `wait_until="networkidle"` - æœ€å¯é ï¼Œç­‰å¾…ç½‘ç»œç©ºé—²
- âœ… `wait_for_load_state("networkidle")` - ç­‰å¾…ç½‘ç»œç©ºé—²
- âœ… `wait_for_selector()` - ç­‰å¾…ç‰¹å®šå…ƒç´ å‡ºç°
- âœ… è‡ªåŠ¨ç­‰å¾… - Playwright æ“ä½œè‡ªåŠ¨ç­‰å¾…å…ƒç´ å¯æ“ä½œ

### æ–‡ç« æŠ“å–
- âœ… ä½¿ç”¨å¤šä¸ªé€‰æ‹©å™¨æé«˜å…¼å®¹æ€§
- âœ… ä½¿ç”¨ `query_selector_all()` è·å–æ‰€æœ‰å…ƒç´ 
- âœ… ä½¿ç”¨ `inner_text()` è·å–æ–‡æœ¬å†…å®¹
- âœ… ä½¿ç”¨ `get_attribute()` è·å–å±æ€§å€¼

### ç¿»é¡µå¤„ç†
- âœ… ç‚¹å‡»ä¸‹ä¸€é¡µæŒ‰é’® - é€‚åˆä¼ ç»Ÿåˆ†é¡µ
- âœ… æ»šåŠ¨åŠ è½½ - é€‚åˆæ— é™æ»šåŠ¨
- âœ… URL å‚æ•° - é€‚åˆ RESTful API

### é”™è¯¯å¤„ç†
- âœ… ä½¿ç”¨ try-except æ•è·å¼‚å¸¸
- âœ… å®ç°é‡è¯•æœºåˆ¶
- âœ… è®¾ç½®åˆç†çš„è¶…æ—¶æ—¶é—´
- âœ… è®°å½•è¯¦ç»†çš„é”™è¯¯æ—¥å¿—

---

## 10. å‚è€ƒèµ„æº

- **Playwright å®˜æ–¹æ–‡æ¡£**: https://playwright.dev/python/
- **GitHub ç¤ºä¾‹é¡¹ç›®**:
  - dreammis/social-auto-upload - ç¤¾äº¤åª’ä½“è‡ªåŠ¨ä¸Šä¼ 
  - NanmiCoder/CrawlerTutorial - çˆ¬è™«æ•™ç¨‹
  - unclecode/crawl4ai - AI çˆ¬è™«æ¡†æ¶
  - ed-donner/agents - æ™ºèƒ½ä»£ç†ç¤ºä¾‹
