# -*- coding: utf-8 -*-
"""
CLI Proxy é›†æˆæµ‹è¯•è„šæœ¬
æµ‹è¯• cli_proxy_client.py æ˜¯å¦èƒ½æ­£ç¡®è°ƒç”¨ cli-proxy-api.exe

ä½¿ç”¨æ–¹æ³•:
    1. ç¡®ä¿ cli-proxy-api.exe æ­£åœ¨è¿è¡Œ (å¦‚ http://localhost:8080)
    2. åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® CLI_PROXY_ENABLED=true
    3. è¿è¡Œ: python test_cli_proxy.py

ä½œè€…: AI Assistant
æ—¥æœŸ: 2026-02-15
"""

import os
import sys

# åŠ è½½ .env æ–‡ä»¶
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# å¯¼å…¥ CLI Proxy Client
try:
    from cli_proxy_client import CLIProxyClient, is_proxy_available
    CLI_PROXY_AVAILABLE = True
except ImportError as e:
    print(f"[é”™è¯¯] æ— æ³•å¯¼å…¥ cli_proxy_client: {e}")
    CLI_PROXY_AVAILABLE = False
    sys.exit(1)


def test_cli_proxy_connection():
    """æµ‹è¯• CLI Proxy è¿æ¥"""
    print("=" * 70)
    print("CLI Proxy è¿æ¥æµ‹è¯•")
    print("=" * 70)

    # è·å–é…ç½®
    proxy_url = os.getenv("CLI_PROXY_URL", "http://localhost:8080/v1")
    proxy_model = os.getenv("CLI_PROXY_MODEL", "local-model")
    proxy_enabled = os.getenv("CLI_PROXY_ENABLED", "false").lower() == "true"

    print(f"\n[é…ç½®ä¿¡æ¯]")
    print(f"  CLI_PROXY_ENABLED: {proxy_enabled}")
    print(f"  CLI_PROXY_URL: {proxy_url}")
    print(f"  CLI_PROXY_MODEL: {proxy_model}")

    # å¿«é€Ÿæ£€æµ‹
    print(f"\n[å¿«é€Ÿæ£€æµ‹]")
    if is_proxy_available(proxy_url):
        print(f"  âœ“ CLI Proxy æœåŠ¡åœ¨çº¿ ({proxy_url})")
    else:
        print(f"  âœ— CLI Proxy æœåŠ¡æœªæ£€æµ‹åˆ°")
        print(f"\nè¯·ç¡®ä¿ cli-proxy-api.exe æ­£åœ¨è¿è¡Œå¹¶ç›‘å¬ {proxy_url}")
        return False

    # åˆ›å»ºå®¢æˆ·ç«¯
    print(f"\n[åˆ›å»ºå®¢æˆ·ç«¯]")
    try:
        client = CLIProxyClient(base_url=proxy_url)
        print(f"  âœ“ å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"  âœ— å®¢æˆ·ç«¯åˆ›å»ºå¤±è´¥: {e}")
        return False

    # æ£€æµ‹è¿æ¥
    print(f"\n[è¿æ¥æ£€æµ‹]")
    is_online, message = client.check_connection()
    if is_online:
        print(f"  âœ“ {message}")
    else:
        print(f"  âœ— {message}")
        return False

    # è·å–æ¨¡å‹åˆ—è¡¨
    print(f"\n[è·å–æ¨¡å‹åˆ—è¡¨]")
    try:
        models = client.list_models()
        if models:
            print(f"  âœ“ å‘ç° {len(models)} ä¸ªæ¨¡å‹:")
            for model in models[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                model_id = model.get('id', 'unknown')
                print(f"    - {model_id}")
            if len(models) > 5:
                print(f"    ... è¿˜æœ‰ {len(models) - 5} ä¸ªæ¨¡å‹")
        else:
            print(f"  ! æœªè·å–åˆ°æ¨¡å‹åˆ—è¡¨ï¼ˆå¯èƒ½ API ä¸æ”¯æŒï¼‰")
    except Exception as e:
        print(f"  ! è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")

    return True


def test_chat_completion():
    """æµ‹è¯•èŠå¤©å®ŒæˆåŠŸèƒ½"""
    print("\n" + "=" * 70)
    print("èŠå¤©å®Œæˆæµ‹è¯•")
    print("=" * 70)

    # è·å–é…ç½®
    proxy_url = os.getenv("CLI_PROXY_URL", "http://localhost:8080/v1")
    proxy_model = os.getenv("CLI_PROXY_MODEL", "local-model")

    # åˆ›å»ºå®¢æˆ·ç«¯
    client = CLIProxyClient(base_url=proxy_url)

    # æµ‹è¯•ç®€å•å¯¹è¯
    print(f"\n[æµ‹è¯• 1: ç®€å•å¯¹è¯]")
    try:
        response = client.chat.completions.create(
            model=proxy_model,
            messages=[
                {"role": "user", "content": "Say 'Hello from CLI Proxy' and nothing else."}
            ],
            temperature=0.1,
            max_tokens=20
        )

        content = response.choices[0].message.content
        print(f"  è¯·æ±‚: Say 'Hello from CLI Proxy' and nothing else.")
        print(f"  å“åº”: {content}")
        print(f"  âœ“ æµ‹è¯•æˆåŠŸ")

    except Exception as e:
        print(f"  âœ— æµ‹è¯•å¤±è´¥: {e}")
        return False

    # æµ‹è¯•ç³»ç»Ÿæ¶ˆæ¯
    print(f"\n[æµ‹è¯• 2: ç³»ç»Ÿæ¶ˆæ¯]")
    try:
        response = client.chat.completions.create(
            model=proxy_model,
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": "What is 2+2? Answer with just the number."}
            ],
            temperature=0.1,
            max_tokens=10
        )

        content = response.choices[0].message.content
        print(f"  è¯·æ±‚: What is 2+2?")
        print(f"  å“åº”: {content}")
        print(f"  âœ“ æµ‹è¯•æˆåŠŸ")

    except Exception as e:
        print(f"  âœ— æµ‹è¯•å¤±è´¥: {e}")
        return False

    # æµ‹è¯•ä¸­æ–‡
    print(f"\n[æµ‹è¯• 3: ä¸­æ–‡å¯¹è¯]")
    try:
        response = client.chat.completions.create(
            model=proxy_model,
            messages=[
                {"role": "user", "content": "ä½ å¥½ï¼Œè¯·å›å¤'ä½ å¥½ï¼Œä¸–ç•Œ'"}
            ],
            temperature=0.1,
            max_tokens=20
        )

        content = response.choices[0].message.content
        print(f"  è¯·æ±‚: ä½ å¥½ï¼Œè¯·å›å¤'ä½ å¥½ï¼Œä¸–ç•Œ'")
        print(f"  å“åº”: {content}")
        print(f"  âœ“ æµ‹è¯•æˆåŠŸ")

    except Exception as e:
        print(f"  âœ— æµ‹è¯•å¤±è´¥: {e}")
        return False

    return True


def test_with_llm_filter():
    """æµ‹è¯•ä¸ LLMArticleFilter çš„é›†æˆ"""
    print("\n" + "=" * 70)
    print("LLMArticleFilter é›†æˆæµ‹è¯•")
    print("=" * 70)

    try:
        from newsbank_ai_downloader import LLMArticleFilter
    except ImportError as e:
        print(f"[é”™è¯¯] æ— æ³•å¯¼å…¥ LLMArticleFilter: {e}")
        return False

    # è·å–é…ç½®
    proxy_url = os.getenv("CLI_PROXY_URL", "http://localhost:8080/v1")
    proxy_model = os.getenv("CLI_PROXY_MODEL", "local-model")

    print(f"\n[åˆ›å»º LLMArticleFilter å®ä¾‹]")
    print(f"  Provider: cli-proxy")
    print(f"  URL: {proxy_url}")
    print(f"  Model: {proxy_model}")

    try:
        llm_filter = LLMArticleFilter(
            api_key="dummy-key",
            model=proxy_model,
            base_url=proxy_url,
            provider="cli-proxy",
            relevance_threshold=0.4
        )
        print(f"  âœ“ åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"  âœ— åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

    # æ£€æµ‹ API è¿æ¥
    print(f"\n[æ£€æµ‹ API è¿æ¥]")
    is_online, message = llm_filter.check_api_connection()
    if is_online:
        print(f"  âœ“ {message}")
    else:
        print(f"  âœ— {message}")
        return False

    return True


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 70)
    print("CLI Proxy é›†æˆæµ‹è¯•å¥—ä»¶")
    print("=" * 70)
    print("\næ­¤æµ‹è¯•è„šæœ¬å°†éªŒè¯ cli_proxy_client æ˜¯å¦èƒ½æ­£ç¡®è°ƒç”¨")
    print("cli-proxy-api.exe æä¾›çš„æœ¬åœ° AI æ¨¡å‹æœåŠ¡ã€‚")
    print("\nè¯·ç¡®ä¿:")
    print("  1. cli-proxy-api.exe æ­£åœ¨è¿è¡Œ")
    print("  2. æœåŠ¡ç›‘å¬åœ¨é…ç½®çš„åœ°å€ (é»˜è®¤: http://localhost:8080)")

    # è¿è¡Œæµ‹è¯•
    results = []

    # æµ‹è¯• 1: è¿æ¥æµ‹è¯•
    results.append(("è¿æ¥æµ‹è¯•", test_cli_proxy_connection()))

    # æµ‹è¯• 2: èŠå¤©å®Œæˆ
    results.append(("èŠå¤©å®Œæˆ", test_chat_completion()))

    # æµ‹è¯• 3: LLMArticleFilter é›†æˆ
    results.append(("LLMArticleFilter é›†æˆ", test_with_llm_filter()))

    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 70)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 70)

    for name, result in results:
        status = "âœ“ é€šè¿‡" if result else "âœ— å¤±è´¥"
        print(f"  {name}: {status}")

    all_passed = all(r for _, r in results)

    if all_passed:
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        print(f"\næ‚¨ç°åœ¨å¯ä»¥åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®:")
        print(f"  CLI_PROXY_ENABLED=true")
        print(f"\nç„¶åè¿è¡Œ newsbank_ai_downloader.py æ¥ä½¿ç”¨æœ¬åœ° AI æ¨¡å‹ã€‚")
    else:
        print(f"\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
        print(f"\nè¯·æ£€æŸ¥:")
        print(f"  1. cli-proxy-api.exe æ˜¯å¦æ­£åœ¨è¿è¡Œ")
        print(f"  2. é…ç½®æ˜¯å¦æ­£ç¡® (URLã€ç«¯å£)")
        print(f"  3. API æ˜¯å¦å…¼å®¹ OpenAI æ ¼å¼")

    return 0 if all_passed else 1


if __name__ == "__main__":
    sys.exit(main())
