# -*- coding: utf-8 -*-
"""
NewsBank AIæ™ºèƒ½ä¸‹è½½å™¨ï¼ˆLLMå¢å¼ºç‰ˆï¼‰
ä½¿ç”¨AIè‡ªåŠ¨ç­›é€‰å’Œä¸‹è½½ä¸ä¸»é¢˜ç›¸å…³çš„æ–‡ç« 

åŠŸèƒ½ï¼š
1. æ™ºèƒ½è§£æURLæœç´¢æ„å›¾
2. AIè‡ªåŠ¨è¯„ä¼°æ¯ç¯‡æ–‡ç« ç›¸å…³æ€§ï¼ˆé»˜è®¤ä½¿ç”¨LLMï¼‰
3. åªä¸‹è½½é«˜ç›¸å…³åº¦æ–‡ç« 
4. æ”¯æŒå¤šç§AIç­›é€‰ç­–ç•¥

ä½¿ç”¨æ–¹æ³•ï¼š
    # LLMå¢å¼ºç­›é€‰ï¼ˆé»˜è®¤ï¼Œéœ€è¦NVIDIA_API_KEYï¼‰
    python newsbank_ai_downloader.py
    
    # ç¦ç”¨LLMï¼Œä»…ä½¿ç”¨å…³é”®è¯åŒ¹é…
    python newsbank_ai_downloader.py --no-llm
    
    # BERT+LLMåŒé‡ç­›é€‰
    python newsbank_ai_downloader.py --use-bert

ç¯å¢ƒå˜é‡é…ç½® (.envæ–‡ä»¶):
    NVIDIA_API_KEY=nvapi-your-key-here  ï¼ˆé»˜è®¤ä½¿ç”¨ï¼‰
    OPENAI_API_KEY=sk-your-key-here     ï¼ˆå¤‡é€‰ï¼‰
    LLM_PROVIDER=auto
    LLM_MODEL=z-ai/glm4.7
    RELEVANCE_THRESHOLD=0.4

ä½œè€…: AI Assistant
æ—¥æœŸ: 2026-02-15
"""

import os
import sys
import asyncio
import argparse
import json
import random
import time
import re
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Dict, Any, Tuple
from urllib.parse import urlparse, parse_qs, urljoin, unquote, urlencode, parse_qsl
from dataclasses import dataclass, asdict

from playwright.async_api import async_playwright

# åŠ è½½ .env æ–‡ä»¶
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# å°è¯•å¯¼å…¥OpenAI
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

# NVIDIA APIé…ç½®
NVIDIA_BASE_URL = "https://integrate.api.nvidia.com/v1"


@dataclass
class ArticleInfo:
    """æ–‡ç« ä¿¡æ¯æ•°æ®ç±»"""
    title: str
    date: str
    source: str
    author: str
    preview: str
    url: str
    page_num: int
    article_id: Optional[str] = None
    word_count: int = 0
    relevance_score: float = 0.0  # AIç›¸å…³æ€§åˆ†æ•°
    relevance_reason: str = ""     # AIåˆ¤æ–­ç†ç”±
    
    def to_dict(self) -> dict:
        return asdict(self)


@dataclass
class URLAnalysis:
    """URLåˆ†æç»“æœ"""
    original_url: str
    base_params: Dict[str, str]
    search_conditions: List[Dict[str, str]]
    total_conditions: int
    source_filter: Optional[str]
    sort_method: Optional[str]
    max_results: int
    extracted_keywords: List[str]  # ä»URLæå–çš„å…³é”®è¯
    
    def to_display_string(self) -> str:
        """æ ¼å¼åŒ–ä¸ºæ˜¾ç¤ºå­—ç¬¦ä¸²"""
        lines = [
            "",
            "=" * 70,
            "URL Analysis Results",
            "=" * 70,
            f"Original URL: {self.original_url[:80]}...",
            "",
            "Base Parameters:",
        ]
        
        for key, value in self.base_params.items():
            lines.append(f"   {key}: {value}")
        
        lines.extend([
            "",
            f"Search Conditions ({self.total_conditions} total):",
        ])
        
        for i, condition in enumerate(self.search_conditions[:5], 1):
            field = condition.get('field', 'unknown')
            value = condition.get('value', '')
            boolean = condition.get('boolean', 'AND')
            lines.append(f"   [{i}] {boolean} {field}: {value[:50]}")
        
        if self.total_conditions > 5:
            lines.append(f"   ... and {self.total_conditions - 5} more")
        
        lines.extend([
            "",
            f"Source Filter: {self.source_filter or 'None'}",
            f"Sort Method: {self.sort_method or 'Default'}",
            f"Max Results per Page: {self.max_results}",
            "",
            "Extracted Keywords for AI Filtering:",
        ])
        
        for kw in self.extracted_keywords:
            lines.append(f"   *{kw}")
        
        lines.append("=" * 70)
        
        return "\n".join(lines)


class URLParser:
    """NewsBank URLè§£æå™¨"""
    
    @staticmethod
    def parse_url(url: str) -> URLAnalysis:
        """
        è§£æNewsBank URLå‚æ•°å¹¶æå–å…³é”®è¯
        
        Args:
            url: NewsBankæœç´¢URL
        
        Returns:
            URLAnalysiså¯¹è±¡
        """
        parsed = urlparse(url)
        query_params = parse_qs(parsed.query)
        
        # è§£æåŸºç¡€å‚æ•°
        base_params = {}
        for key, values in query_params.items():
            if len(values) == 1:
                base_params[key] = values[0]
            else:
                base_params[key] = values
        
        # è§£ææœç´¢æ¡ä»¶
        search_conditions = []
        condition_index = 0
        extracted_keywords = []
        
        while True:
            value_key = f'val-base-{condition_index}'
            field_key = f'fld-base-{condition_index}'
            boolean_key = f'bln-base-{condition_index}'
            
            if value_key not in base_params:
                break
            
            value = unquote(base_params.get(value_key, ''))
            field = base_params.get(field_key, 'unknown')
            boolean = base_params.get(boolean_key, 'AND' if condition_index > 0 else None)
            
            condition = {
                'index': condition_index,
                'value': value,
                'field': field,
                'boolean': boolean
            }
            search_conditions.append(condition)
            
            # æå–å…³é”®è¯ï¼ˆç”¨äºAIç­›é€‰ï¼‰
            if value and field in ['all', 'headline', 'Title', 'alltext', 'body']:
                # æ¸…ç†å…³é”®è¯
                clean_kw = value.strip().lower()
                # ç§»é™¤å¼•å·
                clean_kw = clean_kw.strip('"').strip("'")
                # åˆ†å‰²å¤šä¸ªå…³é”®è¯
                for kw in re.split(r'[;,|]', clean_kw):
                    kw = kw.strip()
                    if kw and len(kw) > 1:
                        extracted_keywords.append(kw)
            
            condition_index += 1
        
        # æå–å…¶ä»–ä¿¡æ¯
        source_filter = None
        year_filter = None
        if 't' in base_params:
            t_param = base_params['t']
            if 'favorite:' in str(t_param):
                match = re.search(r'favorite:([^!]+)', str(t_param))
                if match:
                    source_filter = unquote(match.group(1))
            # æå–å¹´ä»½
            year_match = re.search(r'year:(\d{4})!(\d{4})', str(t_param))
            if year_match:
                year_filter = f"{year_match.group(1)}-{year_match.group(2)}"
        
        sort_method = base_params.get('sort', 'Default')
        if sort_method == 'YMD_date:D':
            sort_method = 'Date (Newest First)'
        elif sort_method == 'YMD_date:A':
            sort_method = 'Date (Oldest First)'
        elif sort_method == 'relevance':
            sort_method = 'Relevance'
        
        max_results = int(base_params.get('maxresults', 60))
        
        # å¦‚æœæ²¡æœ‰æå–åˆ°å…³é”®è¯ï¼Œå°è¯•ä»URLå…¶ä»–éƒ¨åˆ†æå–
        if not extracted_keywords:
            # å°è¯•ä»æ•´ä¸ªURLä¸­æå–å¯èƒ½çš„æœç´¢è¯
            url_text = unquote(url).lower()
            # ç§»é™¤URLå‚æ•°åï¼Œä¿ç•™å€¼
            for pattern in [r'val-base-\d+[=]([^&]+)', r'q[=]([^&]+)', r'query[=]([^&]+)']:
                matches = re.findall(pattern, url_text)
                for match in matches:
                    clean = match.strip('"').strip("'").strip()
                    if clean and len(clean) > 1:
                        extracted_keywords.append(clean)
        
        # å»é‡å¹¶ä¿æŒé¡ºåº
        seen = set()
        unique_keywords = []
        for kw in extracted_keywords:
            if kw not in seen:
                seen.add(kw)
                unique_keywords.append(kw)
        extracted_keywords = unique_keywords
        
        # å¦‚æœæœ‰å¹´ä»½ä¿¡æ¯ï¼Œæ·»åŠ åˆ°æ¥æºè¿‡æ»¤å™¨
        if year_filter and source_filter:
            source_filter = f"{source_filter} ({year_filter})"
        
        return URLAnalysis(
            original_url=url,
            base_params=base_params,
            search_conditions=search_conditions,
            total_conditions=len(search_conditions),
            source_filter=source_filter,
            sort_method=sort_method,
            max_results=max_results,
            extracted_keywords=extracted_keywords
        )
    
    @staticmethod
    def validate_url(url: str) -> Tuple[bool, str]:
        """
        éªŒè¯URLæ˜¯å¦æœ‰æ•ˆ
        
        Returns:
            (æ˜¯å¦æœ‰æ•ˆ, é”™è¯¯ä¿¡æ¯)
        """
        if not url or not url.strip():
            return False, "URLä¸èƒ½ä¸ºç©º"
        
        url = url.strip()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«NewsBankåŸŸå
        if 'infoweb-newsbank-com' not in url and 'newsbank.com' not in url:
            return False, "URLä¸æ˜¯NewsBankçš„æœç´¢URL"
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœç´¢ç»“æœé¡µ
        if '/apps/news/results' not in url:
            return False, "URLä¸æ˜¯æœç´¢ç»“æœé¡µé¢"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœç´¢å‚æ•°
        parsed = urlparse(url)
        if not parsed.query:
            return False, "URLæ²¡æœ‰æœç´¢å‚æ•°"
        
        return True, "Valid"


class LLMArticleFilter:
    """LLMæ™ºèƒ½æ–‡ç« ç­›é€‰å™¨"""
    
    def __init__(self, 
                 api_key: Optional[str] = None, 
                 model: str = "gpt-3.5-turbo",
                 base_url: Optional[str] = None, 
                 provider: str = "auto",
                 relevance_threshold: float = 0.4):
        """
        åˆå§‹åŒ–LLMç­›é€‰å™¨
        
        Args:
            api_key: API Key
            model: æ¨¡å‹åç§°
            base_url: APIåŸºç¡€URL
            provider: æä¾›å•† ("nvidia", "openai", "auto")
            relevance_threshold: ç›¸å…³æ€§é˜ˆå€¼
        """
        if not OPENAI_AVAILABLE:
            raise ImportError("openaiæœªå®‰è£…ï¼Œè¿è¡Œ: pip install openai")
        
        self.provider = self._detect_provider(api_key, base_url, provider)
        self.client = self._initialize_client(api_key, base_url)
        self.model = self._get_model_name(model)
        self.relevance_threshold = relevance_threshold
        self.target_keywords: List[str] = []
        
        print(f"[LLM] ä½¿ç”¨{self.provider.upper()} API, æ¨¡å‹: {self.model}")
        print(f"[LLM] ç›¸å…³æ€§é˜ˆå€¼: {relevance_threshold}")
    
    def _detect_provider(self, api_key: Optional[str], base_url: Optional[str], 
                        provider: str) -> str:
        """è‡ªåŠ¨æ£€æµ‹APIæä¾›å•†"""
        if provider != "auto":
            return provider
        
        if base_url and "nvidia" in base_url.lower():
            return "nvidia"
        
        if api_key and api_key.startswith("nvapi-"):
            return "nvidia"
        
        return "openai"
    
    def _initialize_client(self, api_key: Optional[str], base_url: Optional[str]):
        """åˆå§‹åŒ–APIå®¢æˆ·ç«¯"""
        if not OPENAI_AVAILABLE:
            raise ImportError("openai åŒ…æœªå®‰è£…ï¼Œæ— æ³•åˆå§‹åŒ–å®¢æˆ·ç«¯")
        
        # ä½¿ç”¨ç±»å‹å¿½ç•¥æ¥é¿å… LSP é”™è¯¯
        import openai as oai  # type: ignore
        
        if self.provider == "nvidia":
            return oai.OpenAI(
                api_key=api_key,
                base_url=base_url or NVIDIA_BASE_URL
            )
        else:
            return oai.OpenAI(api_key=api_key)
    
    def _get_model_name(self, model: str) -> str:
        """è·å–æ­£ç¡®çš„æ¨¡å‹åç§°"""
        if self.provider == "nvidia":
            # NVIDIAæ¨¡å‹åç§°æ˜ å°„
            nvidia_models = {
                "gpt-3.5-turbo": "meta/llama-3.1-405b-instruct",
                "gpt-4": "meta/llama-3.1-405b-instruct",
                "llama-3.1-405b": "meta/llama-3.1-405b-instruct",
                "llama-3.1-70b": "meta/llama-3.1-70b-instruct",
                "llama-3.1-8b": "meta/llama-3.1-8b-instruct",
            }
            return nvidia_models.get(model, model)
        return model
    
    def set_keywords(self, keywords: List[str]):
        """è®¾ç½®ç›®æ ‡å…³é”®è¯"""
        self.target_keywords = keywords
        print(f"[LLM] ç›®æ ‡å…³é”®è¯: {', '.join(keywords)}")
    
    def check_api_connection(self) -> Tuple[bool, str]:
        """
        æ£€æµ‹ LLM API æ˜¯å¦åœ¨çº¿å¯ç”¨
        
        Returns:
            (æ˜¯å¦åœ¨çº¿, çŠ¶æ€ä¿¡æ¯)
        """
        if not OPENAI_AVAILABLE:
            return False, "openai åŒ…æœªå®‰è£…"
        
        try:
            # å‘é€ä¸€ä¸ªç®€å•çš„æµ‹è¯•è¯·æ±‚
            from typing import Any
            test_messages: List[Any] = [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": "Say 'API is working' and nothing else."}
            ]
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=test_messages,
                temperature=0.1,
                max_tokens=20
            )
            
            if response and response.choices and len(response.choices) > 0:
                content = response.choices[0].message.content
                if content and len(content) > 0:
                    return True, f"API åœ¨çº¿æ­£å¸¸ (å“åº”: {content[:30]}...)"
                else:
                    return True, "API åœ¨çº¿ä½†å“åº”å†…å®¹ä¸ºç©º"
            else:
                return False, "API å“åº”æ ¼å¼å¼‚å¸¸"
                
        except Exception as e:
            error_msg = str(e).lower()
            if "authentication" in error_msg or "api key" in error_msg:
                return False, f"API Key è®¤è¯å¤±è´¥: {str(e)[:50]}"
            elif "rate limit" in error_msg or "too many requests" in error_msg:
                return False, f"API é€Ÿç‡é™åˆ¶: {str(e)[:50]}"
            elif "connection" in error_msg or "timeout" in error_msg:
                return False, f"API è¿æ¥å¤±è´¥: {str(e)[:50]}"
            else:
                return False, f"API æ£€æµ‹å¤±è´¥: {str(e)[:50]}"
    
    async def filter_articles_batch(self, articles: List[ArticleInfo], 
                                    batch_size: int = 10) -> List[ArticleInfo]:
        """
        æ‰¹é‡ä½¿ç”¨LLMç­›é€‰æ–‡ç« 
        
        Args:
            articles: æ–‡ç« åˆ—è¡¨
            batch_size: æ¯æ‰¹å¤„ç†çš„æ–‡ç« æ•°
        
        Returns:
            ç­›é€‰åçš„æ–‡ç« åˆ—è¡¨ï¼ˆæ·»åŠ äº†relevance_scoreå’Œrelevance_reasonï¼‰
        """
        if not self.target_keywords:
            print("[è­¦å‘Š] æœªè®¾ç½®ç›®æ ‡å…³é”®è¯ï¼Œè·³è¿‡LLMç­›é€‰")
            return articles
        
        print(f"\n[LLMç­›é€‰] æ­£åœ¨è¯„ä¼° {len(articles)} ç¯‡æ–‡ç« ...")
        print("-" * 60)
        
        filtered_articles = []
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(articles), batch_size):
            batch = articles[i:i+batch_size]
            print(f"  å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(articles)-1)//batch_size + 1} ({len(batch)} ç¯‡)")
            
            batch_results = await self._evaluate_batch(batch)
            filtered_articles.extend(batch_results)
            
            # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
            if i + batch_size < len(articles):
                await asyncio.sleep(0.5)
        
        # æŒ‰ç›¸å…³æ€§åˆ†æ•°æ’åº
        filtered_articles.sort(key=lambda x: x.relevance_score, reverse=True)
        
        # ç»Ÿè®¡
        relevant_count = sum(1 for a in filtered_articles if a.relevance_score >= self.relevance_threshold)
        print(f"\n[LLMç­›é€‰å®Œæˆ]")
        print(f"  æ€»æ–‡ç« æ•°: {len(filtered_articles)}")
        print(f"  ç›¸å…³æ–‡ç« : {relevant_count} (â‰¥{self.relevance_threshold})")
        print(f"  ç­›é€‰æ¯”ä¾‹: {relevant_count/len(filtered_articles)*100:.1f}%")
        
        return filtered_articles
    
    async def _evaluate_batch(self, articles: List[ArticleInfo]) -> List[ArticleInfo]:
        """è¯„ä¼°ä¸€æ‰¹æ–‡ç« """
        # æ„å»ºæç¤º - å‘é€å®Œæ•´çš„æ ‡é¢˜å’Œé¢„è§ˆå†…å®¹
        articles_text = "\n\n".join([
            f"[{i+1}] æ ‡é¢˜: {a.title}\n    é¢„è§ˆ: {a.preview}"
            for i, a in enumerate(articles)
        ])
        
        keywords_text = ", ".join(self.target_keywords)
        
        prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹æ–‡ç« ä¸æœç´¢ä¸»é¢˜çš„ç›¸å…³æ€§ã€‚

æœç´¢ä¸»é¢˜å…³é”®è¯: {keywords_text}

æ–‡ç« åˆ—è¡¨:
{articles_text}

è¯·ä¸ºæ¯ç¯‡æ–‡ç« ç»™å‡º:
1. ç›¸å…³æ€§åˆ†æ•° (0-100): 0=å®Œå…¨æ— å…³, 100=é«˜åº¦ç›¸å…³
2. åˆ¤æ–­ç†ç”±: ç®€è¦è¯´æ˜ä¸ºä»€ä¹ˆç›¸å…³æˆ–ä¸ç›¸å…³

å›å¤æ ¼å¼ï¼ˆä¸¥æ ¼æŒ‰æ­¤æ ¼å¼ï¼‰:
[1] åˆ†æ•°: XX, ç†ç”±: XXXXXX
[2] åˆ†æ•°: XX, ç†ç”±: XXXXXX
..."""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a professional article relevance assessment assistant. Analyze news articles and provide relevance scores based on the given keywords. Be objective and consistent."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=800
            )
            
            content = response.choices[0].message.content or ""
            
            # è§£æç»“æœ
            results = self._parse_llm_response(content, len(articles))
            
            # æ›´æ–°æ–‡ç« å¯¹è±¡
            for i, article in enumerate(articles):
                if i < len(results):
                    article.relevance_score = results[i]['score']
                    article.relevance_reason = results[i]['reason']
                else:
                    article.relevance_score = 0.0
                    article.relevance_reason = "è§£æå¤±è´¥"
            
            return articles
            
        except Exception as e:
            print(f"  [é”™è¯¯] LLM APIè°ƒç”¨å¤±è´¥: {e}")
            # å¦‚æœAPIå¤±è´¥ï¼Œç»™æ‰€æœ‰æ–‡ç« é»˜è®¤åˆ†æ•°
            for article in articles:
                article.relevance_score = 0.5
                article.relevance_reason = f"APIé”™è¯¯: {str(e)[:50]}"
            return articles
    
    def _parse_llm_response(self, content: str, expected_count: int) -> List[Dict]:
        """è§£æLLMå›å¤"""
        results = []
        
        # å°è¯•åŒ¹é…æ ¼å¼: [N] åˆ†æ•°: XX, ç†ç”±: XXXXXX
        pattern = r'\[(\d+)\]\s*åˆ†æ•°[:ï¼š]\s*(\d+),?\s*ç†ç”±[:ï¼š]\s*(.+?)(?=\[\d+\]|$)'
        matches = re.findall(pattern, content, re.DOTALL)
        
        if matches:
            for match in matches:
                idx = int(match[0]) - 1
                score = int(match[1]) / 100.0
                reason = match[2].strip()
                results.append({'index': idx, 'score': score, 'reason': reason})
        else:
            # å¤‡é€‰è§£æï¼šæŸ¥æ‰¾æ‰€æœ‰æ•°å­—ä½œä¸ºåˆ†æ•°
            scores = re.findall(r'(\d+)', content)
            reasons = content.split('\n')
            
            for i in range(min(len(scores), expected_count)):
                score = min(100, max(0, int(scores[i]))) / 100.0
                reason = reasons[i] if i < len(reasons) else "æœªæä¾›ç†ç”±"
                results.append({'index': i, 'score': score, 'reason': reason[:100]})
        
        # ç¡®ä¿æœ‰æ‰€æœ‰æ–‡ç« çš„ç»“æœ
        while len(results) < expected_count:
            results.append({'index': len(results), 'score': 0.5, 'reason': 'æœªè§£æåˆ°ç»“æœ'})
        
        return results[:expected_count]


class NewsBankAIDownloader:
    """NewsBank AIæ™ºèƒ½ä¸‹è½½å™¨"""
    
    def __init__(self,
                 headless: bool = False,
                 max_pages: int = 10,
                 download_limit: int = 50,
                 min_preview_words: int = 30,
                 use_llm: bool = True,
                 use_bert: bool = False,
                 relevance_threshold: float = 0.4,
                 output_dir: str = "articles_ai"):
        self.headless = headless
        self.max_pages = max_pages
        self.download_limit = download_limit
        self.min_preview_words = min_preview_words
        self.use_llm = use_llm
        self.use_bert = use_bert
        self.relevance_threshold = relevance_threshold
        
        self.cookie_file = Path("cookies/newsbank_auth.json")
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.cookie_file.parent.mkdir(parents=True, exist_ok=True)
        
        # åæ£€æµ‹
        self.min_delay = 2
        self.max_delay = 5
        self.last_request_time = 0
        
        # ç»Ÿè®¡
        self.stats = {
            "total_pages": 0,
            "total_articles": 0,
            "ai_filtered": 0,
            "user_selected": 0,
            "downloaded": 0,
            "skipped": 0,
            "errors": []
        }
        
        self.articles: List[ArticleInfo] = []
        self.url_analysis: Optional[URLAnalysis] = None
        self.llm_filter: Optional[LLMArticleFilter] = None
        
        # åˆå§‹åŒ–LLMç­›é€‰å™¨
        if use_llm:
            self._init_llm_filter()
    
    def _init_llm_filter(self):
        """åˆå§‹åŒ–LLMç­›é€‰å™¨"""
        if not OPENAI_AVAILABLE:
            print("[è­¦å‘Š] openaiæœªå®‰è£…ï¼Œç¦ç”¨LLMç­›é€‰")
            self.use_llm = False
            return
        
        # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
        api_key = os.getenv("NVIDIA_API_KEY") or os.getenv("OPENAI_API_KEY")
        provider = os.getenv("LLM_PROVIDER", "auto")
        model = os.getenv("LLM_MODEL", "gpt-3.5-turbo")
        threshold = float(os.getenv("RELEVANCE_THRESHOLD", "0.4"))
        
        if not api_key:
            print("[è­¦å‘Š] æœªæ‰¾åˆ°API Key (NVIDIA_API_KEY æˆ– OPENAI_API_KEY)ï¼Œç¦ç”¨LLMç­›é€‰")
            self.use_llm = False
            return
        
        try:
            self.llm_filter = LLMArticleFilter(
                api_key=api_key,
                model=model,
                provider=provider,
                relevance_threshold=threshold
            )
            
            # æ£€æµ‹ API æ˜¯å¦åœ¨çº¿
            print("[AI] æ­£åœ¨æ£€æµ‹ LLM API è¿æ¥çŠ¶æ€...")
            is_online, status_msg = self.llm_filter.check_api_connection()
            
            if is_online:
                print(f"[AI] âœ“ {status_msg}")
                print(f"[AI] LLMæ™ºèƒ½ç­›é€‰å·²å¯ç”¨ (é˜ˆå€¼: {threshold})")
            else:
                print(f"[è­¦å‘Š] âœ— {status_msg}")
                print("[è­¦å‘Š] LLM API ä¸åœ¨çº¿ï¼Œå°†ç¦ç”¨AIç­›é€‰åŠŸèƒ½")
                self.use_llm = False
                self.llm_filter = None
                
        except Exception as e:
            print(f"[é”™è¯¯] LLMç­›é€‰å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.use_llm = False
    
    async def human_like_delay(self, min_sec: float = 0, max_sec: float = 0):
        """æ·»åŠ éšæœºå»¶è¿Ÿ"""
        min_seconds = min_sec if min_sec > 0 else self.min_delay
        max_seconds = max_sec if max_sec > 0 else self.max_delay
        delay = random.uniform(min_seconds, max_seconds)
        
        time_since_last = time.time() - self.last_request_time
        if time_since_last < min_seconds:
            delay = max(delay, min_seconds - time_since_last)
        
        await asyncio.sleep(delay)
        self.last_request_time = time.time()
    
    async def check_login(self, context) -> bool:
        """æ£€æŸ¥ç™»å½•çŠ¶æ€"""
        print("\n[æ£€æŸ¥ç™»å½•çŠ¶æ€]")
        print("-" * 40)
        
        if not self.cookie_file.exists():
            print("[ä¿¡æ¯] æœªæ‰¾åˆ°ç™»å½•Cookie")
            return False
        
        try:
            test_page = await context.new_page()
            await test_page.goto(
                "https://infoweb-newsbank-com.ezproxy.sl.nsw.gov.au/apps/news/browse-multi?p=AWGLNB",
                wait_until="networkidle", timeout=30000
            )
            
            current_url = test_page.url
            await test_page.close()
            
            if "infoweb-newsbank" in current_url and "login" not in current_url:
                print("[æˆåŠŸ] Cookieæœ‰æ•ˆï¼Œå·²ç™»å½•")
                return True
            else:
                print("[ä¿¡æ¯] Cookieå·²è¿‡æœŸï¼Œéœ€è¦é‡æ–°ç™»å½•")
                return False
                
        except Exception as e:
            print(f"[è­¦å‘Š] æ£€æŸ¥ç™»å½•çŠ¶æ€æ—¶å‡ºé”™: {e}")
            return False
    
    async def do_login(self, page) -> bool:
        """æ‰§è¡Œç™»å½•"""
        print("\n[ç™»å½•]")
        print("-" * 40)
        print("è¯·åœ¨æµè§ˆå™¨çª—å£ä¸­å®Œæˆç™»å½•...")
        print("ç™»å½•æˆåŠŸåå°†è‡ªåŠ¨ç»§ç»­")
        
        try:
            await page.goto(
                "https://eresources.sl.nsw.gov.au/newsbank-including-access-australia",
                wait_until="networkidle", timeout=60000
            )
            
            start_time = asyncio.get_event_loop().time()
            while (asyncio.get_event_loop().time() - start_time) < 180:
                if "infoweb-newsbank-com.ezproxy" in page.url and "login" not in page.url:
                    print("[æˆåŠŸ] ç™»å½•æˆåŠŸï¼")
                    return True
                await asyncio.sleep(2)
            
            print("[é”™è¯¯] ç™»å½•è¶…æ—¶ï¼ˆ3åˆ†é’Ÿï¼‰")
            return False
            
        except Exception as e:
            print(f"[é”™è¯¯] ç™»å½•å¤±è´¥: {e}")
            return False
    
    def _build_page_url(self, base_url: str, page_num: int, max_results: int = 20) -> str:
        """æ„å»ºåˆ†é¡µURL
        
        Args:
            base_url: åŸºç¡€URL
            page_num: é¡µç ï¼ˆä»1å¼€å§‹ï¼‰
            max_results: æ¯é¡µç»“æœæ•°
        
        Returns:
            å¸¦åˆ†é¡µå‚æ•°çš„URL
        """
        parsed = urlparse(base_url)
        query_params = dict(parse_qsl(parsed.query))
        
        # è®¡ç®—offset (ç¬¬ä¸€é¡µoffset=0, ç¬¬äºŒé¡µoffset=20æˆ–maxresults)
        offset = (page_num - 1) * max_results
        
        # æ›´æ–°åˆ†é¡µå‚æ•°
        query_params['offset'] = str(offset)
        query_params['maxresults'] = str(max_results)
        query_params['page'] = str(page_num - 1)  # pageå‚æ•°ä»0å¼€å§‹
        
        # é‡æ–°æ„å»ºURL
        new_query = urlencode(query_params, doseq=True)
        new_url = parsed._replace(query=new_query).geturl()
        
        return new_url
    
    async def scan_articles(self, page, url: str) -> List[ArticleInfo]:
        """æ‰«ææ–‡ç« åˆ—è¡¨"""
        print("\n" + "=" * 70)
        print("æ‰«ææ–‡ç« åˆ—è¡¨")
        print("=" * 70)
        
        articles = []
        current_url = url
        
        # ä»URLè§£æmaxresultsï¼Œé»˜è®¤20
        parsed = urlparse(url)
        query_params = dict(parse_qsl(parsed.query))
        max_results = int(query_params.get('maxresults', 20))
        
        for page_num in range(1, self.max_pages + 1):
            print(f"\n[Page] ç¬¬ {page_num} é¡µ")
            
            if page_num > 1:
                # æ„å»ºä¸‹ä¸€é¡µçš„URL
                current_url = self._build_page_url(url, page_num, max_results)
                print(f"  è®¿é—®: {current_url[:100]}...")
                
                # ç›´æ¥è®¿é—®ä¸‹ä¸€é¡µURL
                await page.goto(current_url, wait_until="networkidle", timeout=60000)
                await asyncio.sleep(2)
            
            # æå–æ–‡ç« 
            article_elements = await page.query_selector_all('article.search-hits__hit')
            
            if not article_elements:
                print("  æœªæ‰¾åˆ°æ–‡ç« ")
                break
            
            self.stats["total_pages"] += 1
            print(f"  æ‰¾åˆ° {len(article_elements)} ç¯‡æ–‡ç« ")
            
            page_articles = []
            for i, elem in enumerate(article_elements, 1):
                try:
                    # æå–æ ‡é¢˜
                    title_elem = await elem.query_selector("h3.search-hits__hit__title a")
                    if not title_elem:
                        continue
                    
                    title = await title_elem.inner_text()
                    title = title.replace("Go to the document viewer for ", "").strip()
                    
                    # æå–URL
                    href = await title_elem.get_attribute("href") or ""
                    full_url = urljoin(page.url, href)
                    
                    # æå–æ—¥æœŸ
                    date = ""
                    date_elem = await elem.query_selector("li.search-hits__hit__meta__item--display-date")
                    if date_elem:
                        date = await date_elem.inner_text()
                    
                    # æå–æ¥æº
                    source = ""
                    source_elem = await elem.query_selector("li.search-hits__hit__meta__item--source")
                    if source_elem:
                        source = await source_elem.inner_text()
                    
                    # æå–ä½œè€…
                    author = ""
                    author_elem = await elem.query_selector("li.search-hits__hit__meta__item--author")
                    if author_elem:
                        author = await author_elem.inner_text()
                    
                    # æå–é¢„è§ˆ
                    preview = ""
                    preview_elem = await elem.query_selector("div.preview-first-paragraph")
                    if preview_elem:
                        preview = await preview_elem.inner_text()
                    
                    preview = preview.strip()
                    word_count = len(preview.split()) if preview else 0
                    
                    # æå–æ–‡ç« ID
                    article_id = None
                    id_match = re.search(r'doc=([^&]+)', href)
                    if id_match:
                        article_id = id_match.group(1)
                    
                    article = ArticleInfo(
                        title=title[:300],
                        date=date.strip()[:100],
                        source=source.strip()[:200],
                        author=author.strip()[:100],
                        preview=preview[:1000],
                        url=full_url[:500],
                        page_num=page_num,
                        article_id=article_id,
                        word_count=word_count
                    )
                    
                    page_articles.append(article)
                    
                    # æ˜¾ç¤ºå‰å‡ ç¯‡æ–‡ç« 
                    if i <= 3:
                        print(f"  [{i}] {title[:60]}... ({word_count}è¯)")
                
                except Exception as e:
                    print(f"  [é”™è¯¯] æå–æ–‡ç« å¤±è´¥: {e}")
                    continue
            
            articles.extend(page_articles)
            self.stats["total_articles"] += len(page_articles)
            
            print(f"  æœ¬é¡µæˆåŠŸæå–: {len(page_articles)} ç¯‡")
        
        return articles
    
    def display_article_list(self, articles: List[ArticleInfo], show_scores: bool = True):
        """æ˜¾ç¤ºæ–‡ç« åˆ—è¡¨"""
        print("\n" + "=" * 70)
        print(f"[æ–‡ç« åˆ—è¡¨ (å…± {len(articles)} ç¯‡)")
        print("=" * 70)
        
        for i, article in enumerate(articles[:30], 1):
            quality_mark = "[OK]" if article.word_count >= self.min_preview_words else "[NO]"
            score_info = ""
            if show_scores and article.relevance_score > 0:
                score_emoji = "ğŸŸ¢" if article.relevance_score >= self.relevance_threshold else "ğŸ”´"
                score_info = f" [{score_emoji} {article.relevance_score:.0%}]"
            
            print(f"\n[{i:3d}] {quality_mark}{score_info} {article.title[:60]}...")
            print(f"      Date: {article.date} | Source: {article.source[:30]}")
            print(f"      Words: {article.word_count}è¯")
            if show_scores and article.relevance_reason:
                print(f"      [Reason] {article.relevance_reason[:60]}...")
        
        if len(articles) > 30:
            print(f"\n... è¿˜æœ‰ {len(articles) - 30} ç¯‡æ–‡ç«  ...")
        
        print("=" * 70)
    
    async def interactive_select(self, articles: List[ArticleInfo]) -> List[ArticleInfo]:
        """äº¤äº’å¼é€‰æ‹©æ–‡ç« """
        print("\n" + "=" * 70)
        print("Keywords: äº¤äº’å¼é€‰æ‹©")
        print("=" * 70)
        print("è¾“å…¥è¦ä¸‹è½½çš„æ–‡ç« ç¼–å·ï¼Œç”¨é€—å·åˆ†éš”")
        print("ä¾‹å¦‚: 1,3,5,7-10")
        print("è¾“å…¥ 'all' ä¸‹è½½æ‰€æœ‰æ–‡ç« ")
        print("è¾“å…¥ 'high' ä¸‹è½½é«˜ç›¸å…³åº¦æ–‡ç« ï¼ˆAIè¯„åˆ† â‰¥ é˜ˆå€¼ï¼‰")
        print("è¾“å…¥ 'quality' ä¸‹è½½æ‰€æœ‰ä¼˜è´¨æ–‡ç« ï¼ˆé¢„è§ˆ>30è¯ï¼‰")
        print("è¾“å…¥ 'q' é€€å‡º")
        print("-" * 70)
        
        while True:
            try:
                user_input = input("\nè¯·è¾“å…¥é€‰æ‹©: ").strip().lower()
                
                if user_input == 'q':
                    return []
                
                if user_input == 'all':
                    return articles
                
                if user_input == 'high':
                    high_relevance = [a for a in articles if a.relevance_score >= self.relevance_threshold]
                    print(f"å·²é€‰æ‹© {len(high_relevance)} ç¯‡é«˜ç›¸å…³åº¦æ–‡ç« ")
                    return high_relevance
                
                if user_input == 'quality':
                    quality_articles = [a for a in articles if a.word_count >= self.min_preview_words]
                    print(f"å·²é€‰æ‹© {len(quality_articles)} ç¯‡ä¼˜è´¨æ–‡ç« ")
                    return quality_articles
                
                # è§£æé€‰æ‹©
                selected_indices = set()
                for part in user_input.split(','):
                    part = part.strip()
                    if '-' in part:
                        start, end = part.split('-')
                        selected_indices.update(range(int(start)-1, int(end)))
                    else:
                        selected_indices.add(int(part) - 1)
                
                selected = [articles[i] for i in selected_indices if 0 <= i < len(articles)]
                print(f"å·²é€‰æ‹© {len(selected)} ç¯‡æ–‡ç« ")
                return selected
                
            except (ValueError, IndexError) as e:
                print(f"è¾“å…¥æ ¼å¼é”™è¯¯: {e}")
                print("è¯·é‡æ–°è¾“å…¥")
    
    async def download_articles(self, page, articles: List[ArticleInfo], url: str) -> int:
        """ä¸‹è½½é€‰ä¸­çš„æ–‡ç« """
        print("\n" + "=" * 70)
        print(f"[Download] å¼€å§‹ä¸‹è½½æ–‡ç«  (å…± {len(articles)} ç¯‡)")
        print("=" * 70)
        
        downloaded = 0
        
        for i, article in enumerate(articles[:self.download_limit], 1):
            print(f"\n[{i}/{len(articles)}] ä¸‹è½½: {article.title[:50]}...")
            
            try:
                await self.human_like_delay(3, 7)
                
                # è®¿é—®æ–‡ç« é¡µé¢
                await page.goto(article.url, wait_until="networkidle", timeout=30000)
                await asyncio.sleep(2)
                
                # æå–å…¨æ–‡
                full_text = ""
                selectors = [
                    '.document-view__body',
                    '.gnus-doc__body',
                    '.document-text',
                    'article'
                ]
                
                for selector in selectors:
                    elem = await page.query_selector(selector)
                    if elem:
                        full_text = await elem.inner_text()
                        if len(full_text.strip()) > 100:
                            break
                
                if not full_text:
                    # å¤‡é€‰æ–¹æ¡ˆï¼šæå–æ‰€æœ‰æ®µè½
                    paragraphs = await page.query_selector_all('p')
                    texts = []
                    for p in paragraphs:
                        text = await p.inner_text()
                        if len(text.strip()) > 20:
                            texts.append(text)
                    full_text = '\n\n'.join(texts)
                
                if len(full_text.strip()) < 50:
                    print(f"  [è­¦å‘Š] æ–‡ç« æ— æœ‰æ•ˆå…¨æ–‡")
                    self.stats["skipped"] += 1
                    continue
                
                # ä¿å­˜æ–‡ç« 
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                safe_title = "".join(c for c in article.title[:50] if c.isalnum() or c in (' ', '-', '_')).strip()
                filename = f"{i:03d}_{timestamp}_{safe_title}.txt"
                filepath = self.output_dir / filename
                
                content = f"""Title: {article.title}
Date: {article.date}
Source: {article.source}
Author: {article.author}
URL: {article.url}
Original Search URL: {url}
Downloaded at: {datetime.now().isoformat()}
Page: {article.page_num}
AI Relevance Score: {article.relevance_score:.0%}
AI Relevance Reason: {article.relevance_reason}

Full Text:
{full_text}

{'='*70}
"""
                
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                downloaded += 1
                self.stats["downloaded"] += 1
                print(f"  [Success] å·²ä¿å­˜ ({len(full_text)} å­—ç¬¦) -> {filename}")
                
            except Exception as e:
                print(f"  [Error] ä¸‹è½½å¤±è´¥: {e}")
                self.stats["errors"].append(f"{article.title}: {str(e)}")
                continue
        
        return downloaded
    
    async def download_from_url(self, url: str):
        """ä»URLä¸‹è½½æ–‡ç« çš„ä¸»æ–¹æ³•"""
        # éªŒè¯URL
        is_valid, message = URLParser.validate_url(url)
        if not is_valid:
            print(f"[é”™è¯¯] {message}")
            return
        
        print("=" * 80)
        print("NewsBank AIæ™ºèƒ½ä¸‹è½½å™¨")
        print("=" * 80)
        
        # è§£æURL
        self.url_analysis = URLParser.parse_url(url)
        print(self.url_analysis.to_display_string())
        
        # è®©ç”¨æˆ·ç¡®è®¤URLå‚æ•°
        print("\n" + "-" * 70)
        confirm = input("[OK] æ˜¯å¦ç»§ç»­ï¼Ÿ (y/n): ").strip().lower()
        if confirm != 'y':
            print("å·²å–æ¶ˆ")
            return
        
        # è®¾ç½®LLMå…³é”®è¯
        if self.llm_filter and self.url_analysis.extracted_keywords:
            self.llm_filter.set_keywords(self.url_analysis.extracted_keywords)
        
        # å¯åŠ¨æµè§ˆå™¨
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=self.headless,
                args=['--disable-blink-features=AutomationControlled']
            )
            
            context = await browser.new_context(
                storage_state=str(self.cookie_file) if self.cookie_file.exists() else None,
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            )
            
            page = await context.new_page()
            
            try:
                # æ£€æŸ¥/æ‰§è¡Œç™»å½•
                if not await self.check_login(context):
                    if self.headless:
                        print("[é”™è¯¯] æ— å¤´æ¨¡å¼ä¸‹æ— æ³•ç™»å½•")
                        return
                    
                    if not await self.do_login(page):
                        return
                    
                    # ä¿å­˜Cookie
                    await context.storage_state(path=str(self.cookie_file))
                
                # è®¿é—®æœç´¢URL
                print(f"\n[è®¿é—®URL]")
                print(f"æ­£åœ¨æ‰“å¼€æœç´¢é¡µé¢...")
                
                await page.goto(url, wait_until="networkidle", timeout=60000)
                await asyncio.sleep(2)
                
                print(f"é¡µé¢æ ‡é¢˜: {await page.title()}")
                
                # æ‰«ææ–‡ç« 
                self.articles = await self.scan_articles(page, url)
                
                if not self.articles:
                    print("\n[è­¦å‘Š] æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« ")
                    return
                
                # AIç­›é€‰
                if self.use_llm and self.llm_filter:
                    print("\n" + "=" * 70)
                    print("ğŸ§  AIæ­£åœ¨åˆ†ææ–‡ç« ç›¸å…³æ€§...")
                    print("=" * 70)
                    self.articles = await self.llm_filter.filter_articles_batch(self.articles)
                    self.stats["ai_filtered"] = sum(1 for a in self.articles if a.relevance_score >= self.relevance_threshold)
                
                # æ˜¾ç¤ºæ–‡ç« åˆ—è¡¨
                self.display_article_list(self.articles, show_scores=self.use_llm)
                
                # ä¿å­˜æ–‡ç« åˆ—è¡¨åˆ°JSON
                json_path = self.output_dir / f"article_list_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump([a.to_dict() for a in self.articles], f, indent=2, ensure_ascii=False)
                print(f"\n[Save] æ–‡ç« åˆ—è¡¨å·²ä¿å­˜: {json_path}")
                
                # äº¤äº’å¼é€‰æ‹©æ–‡ç« 
                selected = await self.interactive_select(self.articles)
                
                if not selected:
                    print("\n[ä¿¡æ¯] æ²¡æœ‰é€‰æ‹©ä»»ä½•æ–‡ç« ")
                    return
                
                self.stats["user_selected"] = len(selected)
                
                # ç¡®è®¤ä¸‹è½½
                print("\n" + "-" * 70)
                final_confirm = input(f"ç¡®è®¤ä¸‹è½½ {len(selected)} ç¯‡æ–‡ç« ? (y/n): ").strip().lower()
                if final_confirm != 'y':
                    print("å·²å–æ¶ˆä¸‹è½½")
                    return
                
                # ä¸‹è½½æ–‡ç« 
                downloaded = await self.download_articles(page, selected, url)
                
                # æœ€ç»ˆæŠ¥å‘Š
                print("\n" + "=" * 80)
                print("[Success] ä¸‹è½½å®ŒæˆæŠ¥å‘Š")
                print("=" * 80)
                print(f"[Page] æ‰«æé¡µæ•°: {self.stats['total_pages']}")
                print(f"Source: å‘ç°æ–‡ç« : {self.stats['total_articles']}")
                if self.use_llm:
                    print(f"[AI] AIç­›é€‰å‡ºç›¸å…³æ–‡ç« : {self.stats['ai_filtered']}")
                print(f"Keywords: ç”¨æˆ·é€‰æ‹©: {self.stats['user_selected']}")
                print(f"[Download] æˆåŠŸä¸‹è½½: {self.stats['downloaded']}")
                print(f"[Skip] è·³è¿‡/å¤±è´¥: {self.stats['skipped']}")
                print(f"Output: è¾“å‡ºç›®å½•: {self.output_dir.absolute()}")
                
                if self.stats["errors"]:
                    print(f"\n[Warning] é”™è¯¯ ({len(self.stats['errors'])}):")
                    for error in self.stats["errors"][:5]:
                        print(f"  - {error}")
                
                print("=" * 80)
                
                if not self.headless:
                    print("\n[INFO] æµè§ˆå™¨å°†ä¿æŒæ‰“å¼€10ç§’...")
                    await asyncio.sleep(10)
            
            except Exception as e:
                print(f"\n[é”™è¯¯] {e}")
                import traceback
                traceback.print_exc()
            
            finally:
                await context.close()
                await browser.close()


def get_user_url() -> str:
    """è·å–ç”¨æˆ·è¾“å…¥çš„URL"""
    print("=" * 80)
    print("[AI] NewsBank AIæ™ºèƒ½ä¸‹è½½å™¨")
    print("=" * 80)
    print("\nè¯·è¾“å…¥NewsBankæœç´¢URL")
    print("æç¤º: åœ¨æµè§ˆå™¨ä¸­å®Œæˆæœç´¢åï¼Œå¤åˆ¶åœ°å€æ çš„URL")
    print("ç¤ºä¾‹: https://infoweb-newsbank-com.ezproxy.sl.nsw.gov.au/apps/news/results?...&val-base-0=Treasury&fld-base-0=Title...")
    print("-" * 80)
    
    while True:
        url = input("\nURL: URL: ").strip()
        
        if not url:
            print("[Error] URLä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥")
            continue
        
        # éªŒè¯URL
        is_valid, message = URLParser.validate_url(url)
        if not is_valid:
            print(f"[Error] {message}")
            retry = input("æ˜¯å¦é‡æ–°è¾“å…¥? (y/n): ").strip().lower()
            if retry != 'y':
                return ""
            continue
        
        return url


def main():
    parser = argparse.ArgumentParser(
        description="NewsBank AIæ™ºèƒ½ä¸‹è½½å™¨ - ä½¿ç”¨LLMæ™ºèƒ½ç­›é€‰æ–‡ç« ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨æ–¹æ³•ç¤ºä¾‹:

1. äº¤äº’å¼æ¨¡å¼ï¼ˆæ¨èï¼‰:
   python newsbank_ai_downloader.py

2. ç›´æ¥æŒ‡å®šURL:
   python newsbank_ai_downloader.py "https://infoweb-newsbank-com.ezproxy.sl.nsw.gov.au/apps/news/results?..."

3. ç¦ç”¨LLMï¼Œä»…ä½¿ç”¨å…³é”®è¯ç­›é€‰:
   python newsbank_ai_downloader.py --no-llm

4. è°ƒæ•´ç›¸å…³æ€§é˜ˆå€¼:
   python newsbank_ai_downloader.py --threshold 0.6

5. æ— å¤´æ¨¡å¼:
   python newsbank_ai_downloader.py --headless "URL"

ç¯å¢ƒå˜é‡ (.envæ–‡ä»¶):
    NVIDIA_API_KEY=nvapi-xxx     # NVIDIA API Key (æ¨è)
    OPENAI_API_KEY=sk-xxx        # OpenAI API Key (å¤‡é€‰)
    LLM_PROVIDER=auto            # è‡ªåŠ¨æ£€æµ‹æä¾›å•†
    LLM_MODEL=z-ai/glm4.7        # æ¨¡å‹é€‰æ‹©
    RELEVANCE_THRESHOLD=0.4      # ç›¸å…³æ€§é˜ˆå€¼

æµç¨‹:
    1. è¾“å…¥NewsBankæœç´¢URL
    2. ç³»ç»Ÿè‡ªåŠ¨è§£æURLå‚æ•°å¹¶æ˜¾ç¤º
    3. ç”¨æˆ·ç¡®è®¤å‚æ•°
    4. ç³»ç»Ÿè‡ªåŠ¨ç™»å½•ï¼ˆå¦‚éœ€è¦ï¼‰
    5. æ‰«ææ–‡ç« åˆ—è¡¨
    6. AIè¯„ä¼°æ¯ç¯‡æ–‡ç« ç›¸å…³æ€§
    7. æ˜¾ç¤ºæ–‡ç« åˆ—è¡¨ï¼ˆå¸¦AIè¯„åˆ†ï¼‰
    8. ç”¨æˆ·é€‰æ‹©è¦ä¸‹è½½çš„æ–‡ç« 
    9. ä¸‹è½½é€‰ä¸­çš„æ–‡ç« 
        """
    )
    
    parser.add_argument("url", nargs="?", default=None,
                       help="NewsBankæœç´¢URLï¼ˆå¯é€‰ï¼Œä¸æä¾›åˆ™äº¤äº’å¼è¾“å…¥ï¼‰")
    
    parser.add_argument("--max-pages", type=int, default=10,
                       help="æœ€å¤§æ‰«æé¡µæ•° (é»˜è®¤: 10)")
    
    parser.add_argument("--download-limit", type=int, default=50,
                       help="æœ€å¤§ä¸‹è½½æ–‡ç« æ•° (é»˜è®¤: 50)")
    
    parser.add_argument("--min-preview-words", type=int, default=30,
                       help="ä¼˜è´¨æ–‡ç« æœ€å°é¢„è§ˆè¯æ•° (é»˜è®¤: 30)")
    
    parser.add_argument("--no-llm", action="store_true",
                       help="ç¦ç”¨LLMç­›é€‰ï¼Œä»…ä½¿ç”¨åŸºç¡€å…³é”®è¯åŒ¹é…")
    
    parser.add_argument("--threshold", type=float, default=None,
                       help="AIç›¸å…³æ€§é˜ˆå€¼ (0.0-1.0ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–)")
    
    parser.add_argument("--headless", action="store_true",
                       help="æ— å¤´æ¨¡å¼")
    
    parser.add_argument("--output-dir", default="articles_ai",
                       help="è¾“å‡ºç›®å½• (é»˜è®¤: articles_ai)")
    
    args = parser.parse_args()
    
    # è·å–URL
    url = args.url
    if not url:
        url = get_user_url()
        if not url:
            print("æœªæä¾›æœ‰æ•ˆURLï¼Œé€€å‡º")
            return 1
    
    # ç¡®å®šæ˜¯å¦ä½¿ç”¨LLM
    use_llm = not args.no_llm
    
    # ç¡®å®šé˜ˆå€¼
    threshold = args.threshold
    if threshold is None:
        threshold = float(os.getenv("RELEVANCE_THRESHOLD", "0.4"))
    
    # åˆ›å»ºä¸‹è½½å™¨
    downloader = NewsBankAIDownloader(
        headless=args.headless,
        max_pages=args.max_pages,
        download_limit=args.download_limit,
        min_preview_words=args.min_preview_words,
        use_llm=use_llm,
        relevance_threshold=threshold,
        output_dir=args.output_dir
    )
    
    # æ‰§è¡Œä¸‹è½½
    asyncio.run(downloader.download_from_url(url))
    
    return 0


if __name__ == "__main__":
    exit(main())
